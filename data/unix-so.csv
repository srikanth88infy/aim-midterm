title,question,answer,url
linux - Running Selenium WebDriver Python bindings in Chrome,"I ran into a problem while working with Selenium. For my project, I have to use Chrome. However, I can't connect to that browser after launching it with Selenium.","
You need to make sure the standalone ChromeDriver binary (which is different than the Chrome browser binary) is either in your path or available in the webdriver.chrome.driver environment variable.
see http://code.google.com/p/selenium/wiki/ChromeDriver for full information on how wire things up.
Edit:
Right, seems to be a bug in the Python bindings wrt reading the chromedriver binary from the path or the environment variable. Seems if chromedriver is not in your path you have to pass it in as an argument to the constructor.
import os
from selenium import webdriver

chromedriver = ""/Users/adam/Downloads/chromedriver""
os.environ[""webdriver.chrome.driver""] = chromedriver
driver = webdriver.Chrome(chromedriver)
driver.get(""http://stackoverflow.com"")
driver.quit()

",https://stackoverflow.com//questions/8255929/running-selenium-webdriver-python-bindings-in-chrome
windows - Cross-platform space remaining on volume using python,"I need a way to determine the space remaining on a disk volume using python on linux, Windows and OS X. I'm currently parsing the output of the various system calls (df, dir) to accomplish this - is there a better way?","
import ctypes
import os
import platform
import sys

def get_free_space_mb(dirname):
    """"""Return folder/drive free space (in megabytes).""""""
    if platform.system() == 'Windows':
        free_bytes = ctypes.c_ulonglong(0)
        ctypes.windll.kernel32.GetDiskFreeSpaceExW(ctypes.c_wchar_p(dirname), None, None, ctypes.pointer(free_bytes))
        return free_bytes.value / 1024 / 1024
    else:
        st = os.statvfs(dirname)
        return st.f_bavail * st.f_frsize / 1024 / 1024

Note that you must pass a directory name for GetDiskFreeSpaceEx() to work
(statvfs() works on both files and directories). You can get a directory name
from a file with os.path.dirname().
Also see the documentation for os.statvfs() and GetDiskFreeSpaceEx.
",https://stackoverflow.com//questions/51658/cross-platform-space-remaining-on-volume-using-python
linux - Run Python script at startup in Ubuntu,I have a short Python script that needs to run at startup - Ubuntu 13.10. I have tried everything I can think of but can't get it to run. The script:,"
Instructions

Copy the python file to /bin:
sudo cp -i /path/to/your_script.py /bin
Add A New Cron Job:
sudo crontab -e
Scroll to the bottom and add the following line (after all the #'s):
@reboot python /bin/your_script.py &
The “&” at the end of the line means the command is run in the background and it won’t stop the system booting up.
Test it:
sudo reboot

Practical example:

Add this file to your Desktop: test_code.py (run it to check that it works for you)
from os.path import expanduser
import datetime

file = open(expanduser(""~"") + '/Desktop/HERE.txt', 'w')
file.write(""It worked!\n"" + str(datetime.datetime.now()))
file.close()

Run the following commands:
sudo cp -i ~/Desktop/test_code.py /bin
sudo crontab -e
Add the following line and save it:
@reboot python /bin/test_code.py &
Now reboot your computer and you should find a new file on your Desktop: HERE.txt

",https://stackoverflow.com//questions/24518522/run-python-script-at-startup-in-ubuntu
Is there a way to figure out what is using a Linux kernel module?,"If I load a kernel module and list the loaded modules with lsmod, I can get the ""use count"" of the module (number of other modules with a reference to the module). Is there a way to figure out what is using a module, though?","
Actually, there seems to be a way to list processes that claim a module/driver - however, I haven't seen it advertised (outside of Linux kernel documentation), so I'll jot down my notes here:
First of all, many thanks for @haggai_e's answer; the pointer to the functions try_module_get and try_module_put as those responsible for managing the use count (refcount) was the key that allowed me to track down the procedure. 
Looking further for this online, I somehow stumbled upon the post Linux-Kernel Archive: [PATCH 1/2] tracing: Reduce overhead of module tracepoints; which finally pointed to a facility present in the kernel, known as (I guess) ""tracing""; the documentation for this is in the directory Documentation/trace - Linux kernel source tree. In particular, two files explain the tracing facility, events.txt and ftrace.txt.
But, there is also a short ""tracing mini-HOWTO"" on a running Linux system in /sys/kernel/debug/tracing/README (see also I'm really really tired of people saying that there's no documentation…); note that in the kernel source tree, this file is actually generated by the file kernel/trace/trace.c. I've tested this on Ubuntu natty, and note that since /sys is owned by root, you have to use sudo to read this file, as in sudo cat or 
sudo less /sys/kernel/debug/tracing/README

... and that goes for pretty much all other operations under /sys which will be described here. 

First of all, here is a simple minimal module/driver code (which I put together from the referred resources), which simply creates a /proc/testmod-sample file node, which returns the string ""This is testmod."" when it is being read; this is testmod.c:
/*
https://github.com/spotify/linux/blob/master/samples/tracepoints/tracepoint-sample.c
https://www.linux.com/learn/linux-training/37985-the-kernel-newbie-corner-kernel-debugging-using-proc-qsequenceq-files-part-1
*/

#include <linux/module.h>
#include <linux/sched.h>
#include <linux/proc_fs.h>
#include <linux/seq_file.h> // for sequence files

struct proc_dir_entry *pentry_sample;

char *defaultOutput = ""This is testmod."";


static int my_show(struct seq_file *m, void *v)
{
  seq_printf(m, ""%s\n"", defaultOutput);
  return 0;
}

static int my_open(struct inode *inode, struct file *file)
{
  return single_open(file, my_show, NULL);
}

static const struct file_operations mark_ops = {
  .owner    = THIS_MODULE,
  .open = my_open,
  .read = seq_read,
  .llseek   = seq_lseek,
  .release  = single_release,
};


static int __init sample_init(void)
{
  printk(KERN_ALERT ""sample init\n"");
  pentry_sample = proc_create(
    ""testmod-sample"", 0444, NULL, &mark_ops);
  if (!pentry_sample)
    return -EPERM;
  return 0;
}

static void __exit sample_exit(void)
{
    printk(KERN_ALERT ""sample exit\n"");
    remove_proc_entry(""testmod-sample"", NULL);
}

module_init(sample_init);
module_exit(sample_exit);

MODULE_LICENSE(""GPL"");
MODULE_AUTHOR(""Mathieu Desnoyers et al."");
MODULE_DESCRIPTION(""based on Tracepoint sample"");

This module can be built with the following Makefile (just have it placed in the same directory as testmod.c, and then run make in that same directory):
CONFIG_MODULE_FORCE_UNLOAD=y
# for oprofile
DEBUG_INFO=y
EXTRA_CFLAGS=-g -O0

obj-m += testmod.o

# mind the tab characters needed at start here:
all:
    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules

clean:
    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean

When this module/driver is built, the output is a kernel object file, testmod.ko. 

At this point, we can prepare the event tracing related to try_module_get and try_module_put; those are in /sys/kernel/debug/tracing/events/module:
$ sudo ls /sys/kernel/debug/tracing/events/module
enable  filter  module_free  module_get  module_load  module_put  module_request

Note that on my system, tracing is by default enabled:
$ sudo cat /sys/kernel/debug/tracing/tracing_enabled
1

... however, the module tracing (specifically) is not: 
$ sudo cat /sys/kernel/debug/tracing/events/module/enable
0

Now, we should first make a filter, that will react on the module_get, module_put etc events, but only for the testmod module. To do that, we should first check the format of the event: 
$ sudo cat /sys/kernel/debug/tracing/events/module/module_put/format
name: module_put
ID: 312
format:
...
    field:__data_loc char[] name;   offset:20;  size:4; signed:1;

print fmt: ""%s call_site=%pf refcnt=%d"", __get_str(name), (void *)REC->ip, REC->refcnt

Here we can see that there is a field called name, which holds the driver name, which we can filter against. To create a filter, we simply echo the filter string into the corresponding file: 
sudo bash -c ""echo name == testmod > /sys/kernel/debug/tracing/events/module/filter""

Here, first note that since we have to call sudo, we have to wrap the whole echo redirection as an argument command of a sudo-ed bash. Second, note that since we wrote to the ""parent"" module/filter, not the specific events (which would be module/module_put/filter etc), this filter will be applied to all events listed as ""children"" of module directory. 
Finally, we enable tracing for module: 
sudo bash -c ""echo 1 > /sys/kernel/debug/tracing/events/module/enable""

From this point on, we can read the trace log file; for me, reading the blocking, 
""piped"" version of the trace file worked - like this: 
sudo cat /sys/kernel/debug/tracing/trace_pipe | tee tracelog.txt

At this point, we will not see anything in the log - so it is time to load (and utilize, and remove) the driver (in a different terminal from where trace_pipe is being read):
$ sudo insmod ./testmod.ko
$ cat /proc/testmod-sample 
This is testmod.
$ sudo rmmod testmod

If we go back to the terminal where trace_pipe is being read, we should see something like:
# tracer: nop
#
#           TASK-PID    CPU#    TIMESTAMP  FUNCTION
#              | |       |          |         |
          insmod-21137 [001] 28038.101509: module_load: testmod
          insmod-21137 [001] 28038.103904: module_put: testmod call_site=sys_init_module refcnt=2
           rmmod-21354 [000] 28080.244448: module_free: testmod

That is pretty much all we will obtain for our testmod driver - the refcount changes only when the driver is loaded (insmod) or unloaded (rmmod), not when we do a read through cat. So we can simply interrupt the read from trace_pipe with CTRL+C in that terminal; and to stop the tracing altogether:
sudo bash -c ""echo 0 > /sys/kernel/debug/tracing/tracing_enabled""

Here, note that most examples refer to reading the file /sys/kernel/debug/tracing/trace instead of trace_pipe as here. However, one problem is that this file is not meant to be ""piped"" (so you shouldn't run a tail -f on this trace file); but instead you should re-read the trace after each operation. After the first insmod, we would obtain the same output from cat-ing both trace and trace_pipe; however, after the rmmod, reading the trace file would give:
   <...>-21137 [001] 28038.101509: module_load: testmod
   <...>-21137 [001] 28038.103904: module_put: testmod call_site=sys_init_module refcnt=2
   rmmod-21354 [000] 28080.244448: module_free: testmod

... that is: at this point, the insmod had already been exited for long, and so it doesn't exist anymore in the process list - and therefore cannot be found via the recorded process ID (PID) at the time - thus we get a blank <...> as process name. Therefore, it is better to log (via tee) a running output from trace_pipe in this case. Also, note that in order to clear/reset/erase the trace file, one simply writes a 0 to it: 
sudo bash -c ""echo 0 > /sys/kernel/debug/tracing/trace""

If this seems counterintuitive, note that trace is a special file, and will always report a file size of zero anyways: 
$ sudo ls -la /sys/kernel/debug/tracing/trace
-rw-r--r-- 1 root root 0 2013-03-19 06:39 /sys/kernel/debug/tracing/trace

... even if it is ""full"". 
Finally, note that if we didn't implement a filter, we would have obtained a log of all module calls on the running system - which would log any call (also background) to grep and such, as those use the binfmt_misc module: 
...
  tr-6232  [001] 25149.815373: module_put: binfmt_misc call_site=search_binary_handler refcnt=133194
..
  grep-6231  [001] 25149.816923: module_put: binfmt_misc call_site=search_binary_handler refcnt=133196
..
  cut-6233  [000] 25149.817842: module_put: binfmt_misc call_site=search_binary_handler refcnt=129669
..
  sudo-6234  [001] 25150.289519: module_put: binfmt_misc call_site=search_binary_handler refcnt=133198
..
  tail-6235  [000] 25150.316002: module_put: binfmt_misc call_site=search_binary_handler refcnt=129671

... which adds quite a bit of overhead (in both log data ammount, and processing time required to generate it).

While looking this up, I stumbled upon Debugging Linux Kernel by Ftrace PDF, which refers to a tool trace-cmd, which pretty much does the similar as above - but through an easier command line interface. There is also a ""front-end reader"" GUI for trace-cmd called KernelShark; both of these are also in Debian/Ubuntu repositories via sudo apt-get install trace-cmd kernelshark. These tools could be an alternative to the procedure described above. 
Finally, I'd just note that, while the above testmod example doesn't really show use in context of multiple claims, I have used the same tracing procedure to discover that an USB module I'm coding, was repeatedly claimed by pulseaudio as soon as the USB device was plugged in - so the procedure seems to work for such use cases.  
",https://stackoverflow.com//questions/448999/is-there-a-way-to-figure-out-what-is-using-a-linux-kernel-module
linux - Command to change the default home directory of a user, This question does not appear to be about programming within the scope defined in the help center.,"
Ibrahim's comment on the other answer is the correct way to alter an existing user's home directory. 
Change the user's home directory:
usermod -d /newhome/username username

usermod is the command to edit an existing user. 
-d (abbreviation for --home) will change the user's home directory.

Change the user's home directory + Move the contents of the user's current directory:
usermod -m -d /newhome/username username

-m (abbreviation for --move-home) will move the content from the user's current directory to the new directory.
",https://stackoverflow.com//questions/20797819/command-to-change-the-default-home-directory-of-a-user
excel - Convert xlsx to csv in Linux with command line,I'm looking for a way to convert xlsx files to csv files on Linux.,"
The Gnumeric spreadsheet application comes with a command line utility called ssconvert that can convert between a variety of spreadsheet formats:
$ ssconvert Book1.xlsx newfile.csv

Using exporter Gnumeric_stf:stf_csv

$ cat newfile.csv

Foo,Bar,Baz
1,2,3
123.6,7.89,
2012/05/14,,
The,last,Line

To install on Ubuntu:
apt-get install gnumeric

To install on Mac:
brew install gnumeric

",https://stackoverflow.com//questions/10557360/convert-xlsx-to-csv-in-linux-with-command-line
python - What is different between makedirs and mkdir of os?,I am confused to use about these two osmethods to create the new directory.,"
makedirs() creates all the intermediate directories if they don't exist (just like mkdir -p in bash).
mkdir() can create a single sub-directory, and will throw an exception if intermediate directories that don't exist are specified.
Either can be used to create a single 'leaf' directory (dirA):

os.mkdir('dirA')
os.makedirs('dirA')

But makedirs must be used to create 'branches':

os.makedirs('dirA/dirB') will work [the entire structure is created]

mkdir can work here if dirA already exists, but if it doesn't an error will be thrown.
Note that unlike mkdir -p in bash, either will fail if the leaf already exists.
",https://stackoverflow.com//questions/13819496/what-is-different-between-makedirs-and-mkdir-of-os
linux - select vs poll vs epoll,"I am designing a new server which needs to support thousands of UDP connections (somewhere around 100,000 sessions). Any input or suggestions on which one to use?","
The answer is epoll if you're using Linux, kqueue if you're using FreeBSD or Mac OS X, and i/o completion ports if you're on Windows.
Some additional things you'll (almost certainly) want to research are:

Load balancing techniques
Multi-threaded networking
Database architecture
Perfect hash tables

Additionally, it is important to note that UDP does not have ""connections"" as opposed to TCP. It would also be in your best interest to start small and scale larger since debugging network-based solutions can be challenging.
",https://stackoverflow.com//questions/4039832/select-vs-poll-vs-epoll
linux - How to check existence of a folder with python and then remove it?,"I want to remove dataset folder from dataset3 folder. But the following code is not removing dataset. 
First I want to check if dataset already exist in dataset then remove dataset. 
Can some one please point out my mistake in following code?","
Python's os.rmdir() only works on empty the directories, however shutil.rmtree() doesn't care (even if there are subdirectories) which makes it very similar to the Linux rm -rf command.
import os
import shutil

dirpath = os.path.join('dataset3', 'dataset')
if os.path.exists(dirpath) and os.path.isdir(dirpath):
    shutil.rmtree(dirpath)

Modern approach
In Python 3.4+ you can do same thing using the pathlib module to make the code more object-oriented and readable:
from pathlib import Path
import shutil

dirpath = Path('dataset3') / 'dataset'
if dirpath.exists() and dirpath.is_dir():
    shutil.rmtree(dirpath)

",https://stackoverflow.com//questions/43765117/how-to-check-existence-of-a-folder-with-python-and-then-remove-it
ruby on rails - cache resources exhausted Imagemagick,I'm using Imagemagick on a rails app with Minimagick and I generate some pictogram with it.,"
Find the policy.xml with find / -name ""policy.xml""
something like /etc/ImageMagick-6/policy.xml
and change
<policy domain=""resource"" name=""disk"" value=""1GiB""/>

to
<policy domain=""resource"" name=""disk"" value=""8GiB""/>

refer to convert fails due to resource limits
Memory issues
",https://stackoverflow.com//questions/31407010/cache-resources-exhausted-imagemagick
gzip - Extract and delete all .gz in a directory- Linux,I have a directory. It has about 500K .gz files. ,"
This should do it:
gunzip *.gz

",https://stackoverflow.com//questions/16038087/extract-and-delete-all-gz-in-a-directory-linux
environment variables - How does /usr/bin/env work in a Linux shebang line?,I know shebang line like this: ,"
env is the name of a Unix program. If you read the manual (man env) you can see that one way to use it is env COMMAND, where in your case, COMMAND is python3.
According to the manual, this will

Set each NAME to VALUE in the environment and run COMMAND.

Running env alone will show you what NAMEs and VALUEs are set:
$ env
TERM=xterm-256color
SHELL=/bin/bash
PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin
…

Therefore, /usr/bin/env python3 is an instruction to set the PATH (as well as all the other NAME+VALUE pairs), and then run python3, using the first directory in the PATH that contains the python3 executable.
",https://stackoverflow.com//questions/43793040/how-does-usr-bin-env-work-in-a-linux-shebang-line
"linux - Docker error response from daemon: ""Conflict ... already in use by container""",I've been using Docker on my PC to run Quantum GIS with the following instructions I've found here: docker-qgis-desktop - A simple docker container that runs QGIS desktop,"
It looks like a container with the name qgis-desktop-2-4 already exists in the system. You can check the output of the below command to confirm if it indeed exists:
$ docker ps -a

The last column in the above command's output is for names.
If the container exists, remove it using:
$ docker rm qgis-desktop-2-4

Or forcefully using,
$ docker rm -f qgis-desktop-2-4

And then try creating a new container.
",https://stackoverflow.com//questions/31676155/docker-error-response-from-daemon-conflict-already-in-use-by-container
linux - How to list files in a directory in a C program?,I'm trying to write an ftp server on Linux. In this matter how can I list files in the directory on terminal by a C program? Maybe I can use exec function to run find command but I want file name as a string to send client program. How can I do this?,"
An example, available for POSIX compliant systems :
/*
 * This program displays the names of all files in the current directory.
 */

#include <dirent.h> 
#include <stdio.h> 

int main(void) {
  DIR *d;
  struct dirent *dir;
  d = opendir(""."");
  if (d) {
    while ((dir = readdir(d)) != NULL) {
      printf(""%s\n"", dir->d_name);
    }
    closedir(d);
  }
  return(0);
}

Beware that such an operation is platform dependent in C.
Source : http://faq.cprogramming.com/cgi-bin/smartfaq.cgi?answer=1046380353&id=1044780608
",https://stackoverflow.com//questions/4204666/how-to-list-files-in-a-directory-in-a-c-program
linux - How to force 'cp' to overwrite directory instead of creating another one inside?,I'm trying to write a Bash script that will overwrite an existing directory. I have a directory foo/ and I am trying to overwrite bar/ with it. But when I do this:,"
You can do this using -T option in cp.
See Man page for cp.
-T, --no-target-directory
    treat DEST as a normal file

So as per your example, following is the file structure.
$ tree test
test
|-- bar
|   |-- a
|   `-- b
`-- foo
    |-- a
    `-- b
2 directories, 4 files

You can see the clear difference when you use -v for Verbose.
When you use just -R option.
$ cp -Rv foo/ bar/
`foo/' -> `bar/foo'
`foo/b' -> `bar/foo/b'
`foo/a' -> `bar/foo/a'
 $ tree
 |-- bar
 |   |-- a
 |   |-- b
 |   `-- foo
 |       |-- a
 |       `-- b
 `-- foo
     |-- a
     `-- b
3 directories, 6 files

When you use the option -T it overwrites the contents, treating the destination like a normal file and not directory.
$ cp -TRv foo/ bar/
`foo/b' -> `bar/b'
`foo/a' -> `bar/a'

$ tree
|-- bar
|   |-- a
|   `-- b
`-- foo
    |-- a
    `-- b
2 directories, 4 files

This should solve your problem.
",https://stackoverflow.com//questions/23698183/how-to-force-cp-to-overwrite-directory-instead-of-creating-another-one-inside
centos - How do I download a file from the internet to my linux server with Bash,Want to improve this question? Update the question so it's on-topic for Stack Overflow.,"
Using wget
wget -O /tmp/myfile 'http://www.google.com/logo.jpg'

or curl:
curl -o /tmp/myfile 'http://www.google.com/logo.jpg'

",https://stackoverflow.com//questions/14300794/how-do-i-download-a-file-from-the-internet-to-my-linux-server-with-bash
linux - How to print the ld(linker) search path,What is the way to print the search paths that in looked by ld in the order it searches.,"
You can do this by executing the following command:
ld --verbose | grep SEARCH_DIR | tr -s ' ;' \\012

gcc passes a few extra -L paths to the linker, which you can list with the following command:
gcc -print-search-dirs | sed '/^lib/b 1;d;:1;s,/[^/.][^/]*/\.\./,/,;t 1;s,:[^=]*=,:;,;s,;,;  ,g' | tr \; \\012

The answers suggesting to use ld.so.conf and ldconfig are not correct because they refer to the paths searched by the runtime dynamic linker (i.e. whenever a program is executed), which is not the same as the path searched by ld (i.e. whenever a program is linked).
",https://stackoverflow.com//questions/9922949/how-to-print-the-ldlinker-search-path
Process list on Linux via Python,How can I get running process list using Python on Linux?,"
IMO looking at the /proc filesystem is less nasty than hacking the text output of ps.
import os
pids = [pid for pid in os.listdir('/proc') if pid.isdigit()]

for pid in pids:
    try:
        print open(os.path.join('/proc', pid, 'cmdline'), 'rb').read().split('\0')
    except IOError: # proc has already terminated
        continue

",https://stackoverflow.com//questions/2703640/process-list-on-linux-via-python
linux - How do I edit /etc/sudoers from a script?,I need to edit /etc/sudoers from a script to add/remove stuff from white lists.,"
Old thread, but what about:
echo 'foobar ALL=(ALL:ALL) ALL' | sudo EDITOR='tee -a' visudo

",https://stackoverflow.com//questions/323957/how-do-i-edit-etc-sudoers-from-a-script
Shebang Notation: Python Scripts on Windows and Linux?,"I have some small utility scripts written in Python that I want to be usable on both Windows and Linux.  I want to avoid having to explicitly invoke the Python interpreter.  Is there an easy way to point shebang notation to the correct locations on both Windows and Linux?  If not, is there another way to allow implicit invocation of the Python interpreter on both Windows and Linux without having to modify the script when transferring between operating systems?","
Read up on the Python Launcher for Windows in the docs, which was initially described in PEP 397. It lets
you define custom shebang configurations in ""py.ini"" (e.g. to use pypy),
and out of the box you can use virtual shebangs such as #!/usr/bin/env python3, or shebangs with real paths such as #!""C:\Python33\python.exe"". (Quoting is required for paths containing spaces.) You can also add command-line options to a shebang. For example, the following shebang adds the option to enter interactive mode after the script terminates: #!/usr/bin/python3 -i.
The installer associates .py (console) and .pyw (GUI) script file types with the respectively named launchers, py.exe and pyw.exe, in order to enable shebang support for scripts in Windows. For an all-users installation, the launchers are installed to the Windows folder (i.e. %SystemRoot%). For a per-user installation, you may need to manually add the installation directory to PATH in order to use py.exe in the shell (*). Then from the command line you can run Python via py -2, py -3, py -2.6, py -3.3-32 (32-bit), and so on. The launcher is handy when combined with -m to run a module as a script using a particular version of the interpreter, e.g. py -3 -m pip install. 

(*) The new installer in 3.5+ defaults to ""%LocalAppData%\Programs\Python\Launcher"" for a per-user installation of the launcher, instead of installing it beside ""python.exe"", and it automatically adds this directory to PATH.
",https://stackoverflow.com//questions/7574453/shebang-notation-python-scripts-on-windows-and-linux
linux - fork: retry: Resource temporarily unavailable,Want to improve this question? Update the question so it's on-topic for Stack Overflow.,"
This is commonly caused by running out of file descriptors. 
There is the systems total file descriptor limit, what do you get from the command:
sysctl fs.file-nr

This returns counts of file descriptors:
<in_use> <unused_but_allocated> <maximum>

To find out what a users file descriptor limit is run the commands:
sudo su - <username>
ulimit -Hn

To find out how many file descriptors are in use by a user run the command:
sudo lsof -u <username> 2>/dev/null | wc -l

So now if you are having a system file descriptor limit issue you will need to edit your /etc/sysctl.conf file and add, or modify it it already exists, a line with fs.file-max and set it to a value large enough to deal with the number of file descriptors you need and reboot.
fs.file-max = 204708

",https://stackoverflow.com//questions/12079087/fork-retry-resource-temporarily-unavailable
linux - grep exclude multiple strings,I am trying to see a log file using tail -f and want to exclude all lines containing the following strings:,"
Filtering out multiple lines with grep:
Put these lines in filename.txt to test:
abc
def
ghi
jkl

grep command using -E flag with a pipe between tokens in a string:
grep -Ev 'def|jkl' filename.txt

prints:
abc
ghi

egrep using -v flag with pipe between tokens surrounded by parens:
egrep -v '(def|jkl)' filename.txt

prints:
abc
ghi

Or if stacking -e flags through grep parameters is okay (credit -> @Frizlab):
grep -Fv -e def -e jkl filename.txt

prints:
abc
ghi

",https://stackoverflow.com//questions/16212656/grep-exclude-multiple-strings
logging - View a log file in Linux dynamically," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
tail -f yourlog.csv
Newly appended lines will continuously show.
",https://stackoverflow.com//questions/2099149/view-a-log-file-in-linux-dynamically
"linux - What is better, curl or wget?",Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.,"
If you are programming, you should use curl. It has a nice api and is available for most languages. Shelling out to the os to run wget is a kludge and shouldn't be done if you have an API interface!
",https://stackoverflow.com//questions/636339/what-is-better-curl-or-wget
c - What is the role of libc(glibc) in our linux app?,"When we debug a program using gdb, we usually see functions with strange names defined in libc(glibc?). My questions are:","
libc implements both standard C functions like strcpy() and POSIX functions (which may be system calls) like getpid().  Note that not all standard C functions are in libc - most math functions are in libm.
You cannot directly make system calls in the same way that you call normal functions because calls to the kernel aren't normal function calls, so they can't be resolved by the linker.  Instead, architecture-specific assembly language thunks are used to call into the kernel - you can of course write these directly in your own program too, but you don't need to because libc provides them for you.
Note that in Linux it is the combination of the kernel and libc that provides the POSIX API.  libc adds a decent amount of value - not every POSIX function is necessarily a system call, and for the ones that are, the kernel behaviour isn't always POSIX conforming.
libc is a single library file (both .so and .a versions are available) and in most cases resides in /usr/lib.  However, the glibc (GNU libc) project provides more than just libc - it also provides the libm mentioned earlier, and other core libraries like libpthread.  So libc is just one of the libraries provided by glibc - and there are other alternate implementations of libc other than glibc.
",https://stackoverflow.com//questions/11372872/what-is-the-role-of-libcglibc-in-our-linux-app
linux - How many socket connections possible?,Has anyone an idea how many tcp-socket connections are possible on a modern standard Linux server?,"
I achieved 1600k concurrent idle socket connections, and at the same time 57k req/s on a Linux desktop (16G RAM, I7 2600 CPU). It's a single thread http server written in C with epoll. Source code is on github, a blog here.
Edit:
I did 600k concurrent HTTP connections (client & server) on both the same computer, with JAVA/Clojure . detail info post, HN discussion: http://news.ycombinator.com/item?id=5127251 
The cost of a connection(with epoll):

application need some RAM per connection
TCP buffer  2 * 4k ~ 10k, or more
epoll need some memory for a file descriptor, from epoll(7)


Each registered file descriptor costs roughly 90
                bytes on a 32-bit kernel, and roughly 160 bytes on a 64-bit kernel. 

",https://stackoverflow.com//questions/651665/how-many-socket-connections-possible
Best way to find os name and version in Unix/Linux platform,I need to find the OS name and version on Unix/Linux platform. For this I tried following:,"
This work fine for all Linux environment.
#!/bin/sh
cat /etc/*-release

In Ubuntu:
$ cat /etc/*-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=10.04
DISTRIB_CODENAME=lucid
DISTRIB_DESCRIPTION=""Ubuntu 10.04.4 LTS""

or 12.04:
$ cat /etc/*-release

DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=12.04
DISTRIB_CODENAME=precise
DISTRIB_DESCRIPTION=""Ubuntu 12.04.4 LTS""
NAME=""Ubuntu""
VERSION=""12.04.4 LTS, Precise Pangolin""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu precise (12.04.4 LTS)""
VERSION_ID=""12.04""

In RHEL:
$ cat /etc/*-release
Red Hat Enterprise Linux Server release 6.5 (Santiago)
Red Hat Enterprise Linux Server release 6.5 (Santiago)

Or Use this Script:
#!/bin/sh
# Detects which OS and if it is Linux then it will detect which Linux
# Distribution.

OS=`uname -s`
REV=`uname -r`
MACH=`uname -m`

GetVersionFromFile()
{
    VERSION=`cat $1 | tr ""\n"" ' ' | sed s/.*VERSION.*=\ // `
}

if [ ""${OS}"" = ""SunOS"" ] ; then
    OS=Solaris
    ARCH=`uname -p` 
    OSSTR=""${OS} ${REV}(${ARCH} `uname -v`)""
elif [ ""${OS}"" = ""AIX"" ] ; then
    OSSTR=""${OS} `oslevel` (`oslevel -r`)""
elif [ ""${OS}"" = ""Linux"" ] ; then
    KERNEL=`uname -r`
    if [ -f /etc/redhat-release ] ; then
        DIST='RedHat'
        PSUEDONAME=`cat /etc/redhat-release | sed s/.*\(// | sed s/\)//`
        REV=`cat /etc/redhat-release | sed s/.*release\ // | sed s/\ .*//`
    elif [ -f /etc/SuSE-release ] ; then
        DIST=`cat /etc/SuSE-release | tr ""\n"" ' '| sed s/VERSION.*//`
        REV=`cat /etc/SuSE-release | tr ""\n"" ' ' | sed s/.*=\ //`
    elif [ -f /etc/mandrake-release ] ; then
        DIST='Mandrake'
        PSUEDONAME=`cat /etc/mandrake-release | sed s/.*\(// | sed s/\)//`
        REV=`cat /etc/mandrake-release | sed s/.*release\ // | sed s/\ .*//`
    elif [ -f /etc/debian_version ] ; then
        DIST=""Debian `cat /etc/debian_version`""
        REV=""""

    fi
    if [ -f /etc/UnitedLinux-release ] ; then
        DIST=""${DIST}[`cat /etc/UnitedLinux-release | tr ""\n"" ' ' | sed s/VERSION.*//`]""
    fi

    OSSTR=""${OS} ${DIST} ${REV}(${PSUEDONAME} ${KERNEL} ${MACH})""

fi

echo ${OSSTR}

",https://stackoverflow.com//questions/26988262/best-way-to-find-os-name-and-version-in-unix-linux-platform
java - Linux command for extracting war file?,How can I extract a .war file with Linux command prompt?,"
Using unzip
unzip -c whatever.war META-INF/MANIFEST.MF  

It will print the output in terminal.
And for extracting all the files,
 unzip whatever.war

Using jar
jar xvf test.war

Note! The jar command will extract war contents to current directory. Not to a subdirectory (like Tomcat does).
",https://stackoverflow.com//questions/3833578/linux-command-for-extracting-war-file
How to list empty folders in linux,In Linux how do I check all folders in a directory and output the name of all directories that are empty to a list.,"
Try the following:
find . -type d -empty

",https://stackoverflow.com//questions/9417967/how-to-list-empty-folders-in-linux
Linux error while loading shared libraries: cannot open shared object file: No such file or directory,"Program is part of the Xenomai test suite, cross-compiled from Linux PC into Linux+Xenomai ARM toolchain.","
Your library is a dynamic library.
You need to tell the operating system where it can locate it at runtime.
To do so,
we will need to do those easy steps:

Find where the library is placed if you don't know it.
sudo find / -name the_name_of_the_file.so


Check for the existence of the dynamic library path environment variable(LD_LIBRARY_PATH)
echo $LD_LIBRARY_PATH

If there is nothing to be displayed, add a default path value (or not if you wish to)
LD_LIBRARY_PATH=/usr/local/lib


We add the desired path, export it and try the application.
Note that the path should be the directory where the path.so.something is. So if path.so.something is in /my_library/path.so.something, it should be:
LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/my_library/
export LD_LIBRARY_PATH
./my_app



Reference to source
",https://stackoverflow.com//questions/480764/linux-error-while-loading-shared-libraries-cannot-open-shared-object-file-no-s
Shebang Notation: Python Scripts on Windows and Linux?,"I have some small utility scripts written in Python that I want to be usable on both Windows and Linux.  I want to avoid having to explicitly invoke the Python interpreter.  Is there an easy way to point shebang notation to the correct locations on both Windows and Linux?  If not, is there another way to allow implicit invocation of the Python interpreter on both Windows and Linux without having to modify the script when transferring between operating systems?","
Read up on the Python Launcher for Windows in the docs, which was initially described in PEP 397. It lets
you define custom shebang configurations in ""py.ini"" (e.g. to use pypy),
and out of the box you can use virtual shebangs such as #!/usr/bin/env python3, or shebangs with real paths such as #!""C:\Python33\python.exe"". (Quoting is required for paths containing spaces.) You can also add command-line options to a shebang. For example, the following shebang adds the option to enter interactive mode after the script terminates: #!/usr/bin/python3 -i.
The installer associates .py (console) and .pyw (GUI) script file types with the respectively named launchers, py.exe and pyw.exe, in order to enable shebang support for scripts in Windows. For an all-users installation, the launchers are installed to the Windows folder (i.e. %SystemRoot%). For a per-user installation, you may need to manually add the installation directory to PATH in order to use py.exe in the shell (*). Then from the command line you can run Python via py -2, py -3, py -2.6, py -3.3-32 (32-bit), and so on. The launcher is handy when combined with -m to run a module as a script using a particular version of the interpreter, e.g. py -3 -m pip install. 

(*) The new installer in 3.5+ defaults to ""%LocalAppData%\Programs\Python\Launcher"" for a per-user installation of the launcher, instead of installing it beside ""python.exe"", and it automatically adds this directory to PATH.
",https://stackoverflow.com//questions/7574453/shebang-notation-python-scripts-on-windows-and-linux
linux - How many socket connections possible?,Has anyone an idea how many tcp-socket connections are possible on a modern standard Linux server?,"
I achieved 1600k concurrent idle socket connections, and at the same time 57k req/s on a Linux desktop (16G RAM, I7 2600 CPU). It's a single thread http server written in C with epoll. Source code is on github, a blog here.
Edit:
I did 600k concurrent HTTP connections (client & server) on both the same computer, with JAVA/Clojure . detail info post, HN discussion: http://news.ycombinator.com/item?id=5127251 
The cost of a connection(with epoll):

application need some RAM per connection
TCP buffer  2 * 4k ~ 10k, or more
epoll need some memory for a file descriptor, from epoll(7)


Each registered file descriptor costs roughly 90
                bytes on a 32-bit kernel, and roughly 160 bytes on a 64-bit kernel. 

",https://stackoverflow.com//questions/651665/how-many-socket-connections-possible
linux - nginx not listening to port 80,"I've just installed a Ubuntu 12.04 server and nginx 1.2.7, removed default from sites-enabled and added my own file into sites-available and symlink at sites-enabled. Then restarted nginx.","
I had this same problem, the solution was that I had not symlinked my siteconf file correctly. Try running vim /etc/nginx/sites-enabled/mysite.com—can you get to it? I was getting ""Permission Denied.""
If not run:
rm /etc/nginx/sites-enabled/mysite.com
ln -s /etc/nginx/sites-available/mysite.com /etc/nginx/sites-enabled/mysite.com

",https://stackoverflow.com//questions/16021481/nginx-not-listening-to-port-80
"Locking Executing Files: Windows does, Linux doesn't. Why?","I noticed when a file is executed on Windows (.exe or .dll), it is locked and cannot be deleted, moved or modified.","
Linux has a reference-count mechanism, so you can delete the file while it is executing, and it will continue to exist as long as some process (Which previously opened it) has an open handle for it. The directory entry for the file is removed when you delete it, so it cannot be opened any more, but processes already using this file can still use it. Once all processes using this file terminate, the file is deleted automatically.
Windows does not have this capability, so it is forced to lock the file until all processes executing from it have finished.
I believe that the Linux behavior is preferable. There are probably some deep architectural reasons, but the prime (and simple) reason I find most compelling is that in Windows, you sometimes cannot delete a file, you have no idea why, and all you know is that some process is keeping it in use. In Linux it never happens.
",https://stackoverflow.com//questions/196897/locking-executing-files-windows-does-linux-doesnt-why
How can I identify the request queue for a linux block device,"I am working on this driver that connects the hard disk over the network. There is a bug that if I enable two or more hard disks on the computer, only the first one gets the partitions looked over and identified. The result is, if I have 1 partition on hda and 1 partitions on hdb, as soon as I connect hda there is a partition that can be mounted. So hda1 gets a blkid xyz123 as soon as it mounts. But when I go ahead and mount hdb1 it also comes up with the same blkid and in fact, the driver is reading it from hda, not hdb. ","
Queue = blk_init_queue(sbd_request, &Device.lock);

",https://stackoverflow.com//questions/6785651/how-can-i-identify-the-request-queue-for-a-linux-block-device
c - What does opening a file actually do?,"In all programming languages (that I use at least), you must open a file before you can read or write to it. ","
In almost every high-level language, the function that opens a file is a wrapper around the corresponding kernel system call. It may do other fancy stuff as well, but in contemporary operating systems, opening a file must always go through the kernel.
This is why the arguments of the fopen library function, or Python's open closely resemble the arguments of the open(2) system call.
In addition to opening the file, these functions usually set up a buffer that will be consequently used with the read/write operations. The purpose of this buffer is to ensure that whenever you want to read N bytes, the corresponding library call will return N bytes, regardless of whether the calls to the underlying system calls return less.

I am not actually interested in implementing my own function; just in understanding what the hell is going on...'beyond the language' if you like.

In Unix-like operating systems, a successful call to open returns a ""file descriptor"" which is merely an integer in the context of the user process. This descriptor is consequently passed to any call that interacts with the opened file, and after calling close on it, the descriptor becomes invalid.
It is important to note that the call to open acts like a validation point at which various checks are made. If not all of the conditions are met, the call fails by returning -1 instead of the descriptor, and the kind of error is indicated in errno. The essential checks are:

Whether the file exists;
Whether the calling process is privileged to open this file in the specified mode. This is determined by matching the file permissions, owner ID and group ID to the respective ID's of the calling process.

In the context of the kernel, there has to be some kind of mapping between the process' file descriptors and the physically opened files. The internal data structure that is mapped to the descriptor may contain yet another buffer that deals with block-based devices, or an internal pointer that points to the current read/write position.
",https://stackoverflow.com//questions/33495283/what-does-opening-a-file-actually-do
linux - IPC performance: Named Pipe vs Socket,Everyone seems to say named pipes are faster than sockets IPC. How much faster are they? I would prefer to use sockets because they can do two-way communication and are very flexible but will choose speed over flexibility if it is by considerable amount.,"
Best results you'll get with Shared Memory solution. 
Named pipes are only 16% better than TCP sockets.
Results are get with IPC benchmarking:

System: Linux (Linux ubuntu 4.4.0 x86_64 i7-6700K 4.00GHz)
Message: 128 bytes 
Messages count: 1000000

Pipe benchmark:
Message size:       128
Message count:      1000000
Total duration:     27367.454 ms
Average duration:   27.319 us
Minimum duration:   5.888 us
Maximum duration:   15763.712 us
Standard deviation: 26.664 us
Message rate:       36539 msg/s

FIFOs (named pipes) benchmark:
Message size:       128
Message count:      1000000
Total duration:     38100.093 ms
Average duration:   38.025 us
Minimum duration:   6.656 us
Maximum duration:   27415.040 us
Standard deviation: 91.614 us
Message rate:       26246 msg/s

Message Queue benchmark:
Message size:       128
Message count:      1000000
Total duration:     14723.159 ms
Average duration:   14.675 us
Minimum duration:   3.840 us
Maximum duration:   17437.184 us
Standard deviation: 53.615 us
Message rate:       67920 msg/s

Shared Memory benchmark:
Message size:       128
Message count:      1000000
Total duration:     261.650 ms
Average duration:   0.238 us
Minimum duration:   0.000 us
Maximum duration:   10092.032 us
Standard deviation: 22.095 us
Message rate:       3821893 msg/s

TCP sockets benchmark:
Message size:       128
Message count:      1000000
Total duration:     44477.257 ms
Average duration:   44.391 us
Minimum duration:   11.520 us
Maximum duration:   15863.296 us
Standard deviation: 44.905 us
Message rate:       22483 msg/s

Unix domain sockets benchmark:
Message size:       128
Message count:      1000000
Total duration:     24579.846 ms
Average duration:   24.531 us
Minimum duration:   2.560 us
Maximum duration:   15932.928 us
Standard deviation: 37.854 us
Message rate:       40683 msg/s

ZeroMQ benchmark:
Message size:       128
Message count:      1000000
Total duration:     64872.327 ms
Average duration:   64.808 us
Minimum duration:   23.552 us
Maximum duration:   16443.392 us
Standard deviation: 133.483 us
Message rate:       15414 msg/s

",https://stackoverflow.com//questions/1235958/ipc-performance-named-pipe-vs-socket
linux - Why doesn't a shell get variables exported by a script run in a subshell?,I have two scripts 1.sh and 2.sh.,"
If you are executing your files like sh 1.sh or ./1.sh Then you are executing it in a sub-shell. 
If you want the changes to be made in your current shell, you could do:
. 1.sh
# OR
source 1.sh

Please consider going through the reference-documentation.
""When a script is run using source [or .] it runs within the existing shell, any variables created or modified by the script will remain available after the script completes. In contrast if the script is run just as filename, then a separate subshell (with a completely separate set of variables) would be spawned to run the script.""
",https://stackoverflow.com//questions/10781824/why-doesnt-a-shell-get-variables-exported-by-a-script-run-in-a-subshell
java - Moving from JDK 1.7 to JDK 1.8 on Ubuntu,I am on UBUNTU. JDK version currently installed is:,"
This is what I do on debian - I suspect it should work on ubuntu (amend the version as required + adapt the folder where you want to copy the JDK files as you wish, I'm using /opt/jdk):
wget --header ""Cookie: oraclelicense=accept-securebackup-cookie"" http://download.oracle.com/otn-pub/java/jdk/8u71-b15/jdk-8u71-linux-x64.tar.gz
sudo mkdir /opt/jdk
sudo tar -zxf jdk-8u71-linux-x64.tar.gz -C /opt/jdk/
rm jdk-8u71-linux-x64.tar.gz

Then update-alternatives:
sudo update-alternatives --install /usr/bin/java java /opt/jdk/jdk1.8.0_71/bin/java 1
sudo update-alternatives --install /usr/bin/javac javac /opt/jdk/jdk1.8.0_71/bin/javac 1

Select the number corresponding to the /opt/jdk/jdk1.8.0_71/bin/java when running the following commands:
sudo update-alternatives --config java
sudo update-alternatives --config javac

Finally, verify that the correct version is selected:
java -version
javac -version

",https://stackoverflow.com//questions/30177455/moving-from-jdk-1-7-to-jdk-1-8-on-ubuntu
linux - Run an Ansible task only when the variable contains a specific string,I have multiple tasks depend from the value of variable1. I want to check if the value is in {{ variable1 }} but I get an error:,"
If variable1 is a string, and you are searching for a substring in it, this should work:
when: '""value"" in variable1'

if variable1 is an array or dict instead, in will search for the exact string as one of its items.
",https://stackoverflow.com//questions/36496911/run-an-ansible-task-only-when-the-variable-contains-a-specific-string
linux - How do I measure separate CPU core usage for a process?,Is there any way to measure a specific process CPU usage by cores?,"
You can still do this in top.  While top is running, press '1' on your keyboard, it will then show CPU usage per core.
Limit the processes shown by having that specific process run under a specific user account and use Type 'u' to limit to that user
",https://stackoverflow.com//questions/3342889/how-do-i-measure-separate-cpu-core-usage-for-a-process
linux - Avoid gnome-terminal close after script execution?,"I created a bash script that opens several gnome-terminals, connect to classroom computers via ssh and run a script.","
As I understand you want gnome-terminal to open, have it execute some commands, and then drop to the prompt so you can enter some more commands. Gnome-terminal is not designed for this use case, but there are workarounds:

Let gnome-terminal run bash and tell bash to run your commands and then start a new bash
$ gnome-terminal -- bash -c ""echo foo; echo bar; exec bash""

or if the commands are in a script
$ gnome-terminal -- bash -c ""./scripttorun; exec bash""

The first bash will terminate once all the commands are done. But the last command is a new bash which will then just keep running. And since something is still running gnome-terminal will not close.

Let gnome-terminal run bash with a prepared rcfile which runs your commands
Prepare somercfile:
source ~/.bashrc
echo foo
echo bar

Then run:
$ gnome-terminal -- bash --rcfile somercfile

bash will stay open after running somercfile.
i must admit i do not understand completely why --rcfile has this behaviour but it does.

Let gnome-terminal run a script which runs your commands and then drops to bash
Prepare scripttobash:
#!/bin/sh
echo foo
echo bar
exec bash

Set this file as executable.
Then run:
$ gnome-terminal -- ./scripttobash


for completeness
if you just want to be able read the output of the command and need no interactivity

go to preferences (hamburger button -> preferences)
go to profiles (standard or create a new one)
go to command tab
when command exits -> hold the terminal open

i recommend to create a new profile for just for this use case.
use the profile like this:
gnome-terminal --profile=holdopen -- ./scripttorun


Every method has it's quirks. You must choose, but choose wisely.
I like the first solution. it does not need extra files or profiles. and the command says what it does: run commands then run bash again.
All that said, since you used ssh in your example, you might want to take a look at pssh (parallel ssh). here an article: https://www.cyberciti.biz/cloud-computing/how-to-use-pssh-parallel-ssh-program-on-linux-unix/
",https://stackoverflow.com//questions/3512055/avoid-gnome-terminal-close-after-script-execution
linux - Read and write to binary files in C?,Does anyone have an example of code that can write to a binary file. And also code that can read a binary file and output to screen.  Looking at examples I can write to a file ok But when I try to read from a file it is not outputting correctly.,"
Reading and writing binary files is pretty much the same as any other file, the only difference is how you open it:
unsigned char buffer[10];
FILE *ptr;

ptr = fopen(""test.bin"",""rb"");  // r for read, b for binary

fread(buffer,sizeof(buffer),1,ptr); // read 10 bytes to our buffer

You said you can read it, but it's not outputting correctly... keep in mind that when you ""output"" this data, you're not reading ASCII, so it's not like printing a string to the screen:
for(int i = 0; i<10; i++)
    printf(""%u "", buffer[i]); // prints a series of bytes

Writing to a file is pretty much the same, with the exception that you're using fwrite() instead of fread():
FILE *write_ptr;

write_ptr = fopen(""test.bin"",""wb"");  // w for write, b for binary

fwrite(buffer,sizeof(buffer),1,write_ptr); // write 10 bytes from our buffer


Since we're talking Linux.. there's an easy way to do a sanity check. Install hexdump on your system (if it's not already on there) and dump your file:
mike@mike-VirtualBox:~/C$ hexdump test.bin
0000000 457f 464c 0102 0001 0000 0000 0000 0000
0000010 0001 003e 0001 0000 0000 0000 0000 0000
...

Now compare that to your output:
mike@mike-VirtualBox:~/C$ ./a.out 
127 69 76 70 2 1 1 0 0 0

hmm, maybe change the printf to a %x to make this a little clearer:
mike@mike-VirtualBox:~/C$ ./a.out 
7F 45 4C 46 2 1 1 0 0 0

Hey, look! The data matches up now*. Awesome, we must be reading the binary file correctly!
*Note the bytes are just swapped on the output but that data is correct, you can adjust for this sort of thing
",https://stackoverflow.com//questions/17598572/read-and-write-to-binary-files-in-c
linux - tcp_tw_reuse vs tcp_tw_recycle : Which to use (or both)?,"I have a website and application which use a significant number of connections.  It normally has about 3,000 connections statically open, and can receive anywhere from 5,000 to 50,000 connection attempts in a few seconds time frame.","
According to Linux documentation, you should use the TCP_TW_REUSE flag to allow reusing sockets in TIME_WAIT state for new connections. 
It seems to be a good option when dealing with a web server that have to handle many short TCP connections left in a TIME_WAIT state.
As described here, The TCP_TW_RECYCLE could cause some problems when using load balancers...
EDIT (to add some warnings ;) ):
as mentionned in comment by @raittes, the ""problems when using load balancers"" is about public-facing servers. When recycle is enabled, the server can't distinguish new incoming connections from different clients behind the same NAT device.
",https://stackoverflow.com//questions/6426253/tcp-tw-reuse-vs-tcp-tw-recycle-which-to-use-or-both
windows - Cross-platform space remaining on volume using python,"I need a way to determine the space remaining on a disk volume using python on linux, Windows and OS X. I'm currently parsing the output of the various system calls (df, dir) to accomplish this - is there a better way?","
import ctypes
import os
import platform
import sys

def get_free_space_mb(dirname):
    """"""Return folder/drive free space (in megabytes).""""""
    if platform.system() == 'Windows':
        free_bytes = ctypes.c_ulonglong(0)
        ctypes.windll.kernel32.GetDiskFreeSpaceExW(ctypes.c_wchar_p(dirname), None, None, ctypes.pointer(free_bytes))
        return free_bytes.value / 1024 / 1024
    else:
        st = os.statvfs(dirname)
        return st.f_bavail * st.f_frsize / 1024 / 1024

Note that you must pass a directory name for GetDiskFreeSpaceEx() to work
(statvfs() works on both files and directories). You can get a directory name
from a file with os.path.dirname().
Also see the documentation for os.statvfs() and GetDiskFreeSpaceEx.
",https://stackoverflow.com//questions/51658/cross-platform-space-remaining-on-volume-using-python
linux - Using iconv to convert from UTF-16LE to UTF-8,"Hi I am trying to convert some log files from a Microsoft SQL server, but the files are encoded using UTf-16LE and iconv does not seem to be able to convert them. ","
I forgot the -o switch!
The final command is :
iconv -f UTF-16LE -t UTF-8 <filename> -o <new-filename>

",https://stackoverflow.com//questions/17287713/using-iconv-to-convert-from-utf-16le-to-utf-8
"mysql - Job for mysqld.service failed See ""systemctl status mysqld.service""",Console says,"
This amazingly worked.
/etc/init.d/mysql stop
service mysql stop
killall -KILL mysql mysqld_safe mysqld
/etc/init.d/mysql start
service mysql start

",https://stackoverflow.com//questions/42317139/job-for-mysqld-service-failed-see-systemctl-status-mysqld-service
linux - Is it better to use git grep than plain grep if we want to search in versioned source code?,"In a git repository, is there any difference/benefit using git grep over good old grep?
An example would be?","
The two are very similar. The main difference is that git grep defaults to searching in the files that are tracked by git.
Examples
If I want to find foo within my project I can use git grep or good ol' stand-alone grep:
git grep foo
grep -R foo .

The git grep version will only search in files tracked by git, whereas the grep version will search everything in the directory. So far so similar; either one could be better depending on what you want to achieve.
What if we want to limit the search to only .rb files?
git grep foo -- *.rb
grep -R --include=*.rb foo .

The plain old grep version is getting a bit more wordy, but if you're used to using grep that may not be a problem. They're still not going to search exactly the same files, but again it depends on what you want to achieve.
What about searching in the previous version of the project?
git grep foo HEAD^
git checkout HEAD^; grep -R foo .; git checkout -

This is where git grep makes a real difference: You can search in another revision of the project without checking it out first. This isn't a situation that comes up too often for me though; I usually want to search in the version of the project I have checked out.
Configuring git grep
There are some git config variables that modify the behaviour of git grep and avoid the need to pass a couple of command line arguments:

grep.lineNumber: Always show line numbers of matches (you can pass -n to both grep and git grep to get this behaviour)
grep.extendedRegexp: Always use extended regular expressions (you can pass -E to both grep and git grep to get this behaviour)

In practice
In practice I have gg aliased to git grep -En, and this almost always does what I want.
",https://stackoverflow.com//questions/17557684/is-it-better-to-use-git-grep-than-plain-grep-if-we-want-to-search-in-versioned-s
linux - How to write stdout to file with colors?,"A lot of times (not always) the stdout is displayed in colors. Normally I keep every output log in a different file too. Naturally in the file, the colors are not displayed anymore. ","
Since many programs will only output color sequences if their stdout is a terminal, a general solution to this problem requires tricking them into believing that the pipe they write to is a terminal. This is possible with the script command from bsdutils:
script -q -c ""vagrant up"" filename.txt

This will write the output from vagrant up to filename.txt (and the terminal). If echoing is not desirable, 
script -q -c ""vagrant up"" filename > /dev/null

will write it only to the file.
",https://stackoverflow.com//questions/27397865/how-to-write-stdout-to-file-with-colors
linux - Get exit code of a background process,I have a command CMD called from my main bourne shell script that takes forever.,"
1:  In bash, $! holds the PID of the last background process that was executed. That will tell you what process to monitor, anyway.
4: wait <n> waits until the process with PID <n> is complete (it will block until the process completes, so you might not want to call this until you are sure the process is done), and then returns the exit code of the completed process.
2, 3: ps or ps | grep "" $! "" can tell you whether the process is still running. It is up to you how to understand the output and decide how close it is to finishing. (ps | grep isn't idiot-proof. If you have time you can come up with a more robust way to tell whether the process is still running).
Here's a skeleton script:
# simulate a long process that will have an identifiable exit code
(sleep 15 ; /bin/false) &
my_pid=$!

while   ps | grep "" $my_pid ""     # might also need  | grep -v grep  here
do
    echo $my_pid is still in the ps output. Must still be running.
    sleep 3
done

echo Oh, it looks like the process is done.
wait $my_pid
# The variable $? always holds the exit code of the last command to finish.
# Here it holds the exit code of $my_pid, since wait exits with that code. 
my_status=$?
echo The exit status of the process was $my_status

",https://stackoverflow.com//questions/1570262/get-exit-code-of-a-background-process
linux - Is Mac OS X a POSIX OS?,"What is it that makes an OS a POSIX system? All versions of Linux are POSIX, right? What about Mac OS X?","

Is Mac OS X a POSIX OS?

Yes.
POSIX is a group of standards that determine a portable API for Unix-like operating systems. Mac OS X is Unix-based (and has been certified as such), and in accordance with this is POSIX compliant. POSIX guarantees that certain system calls will be available.
Essentially, Mac satisfies the API required to be POSIX compliant, which makes it a POSIX OS.
All versions of Linux are not POSIX-compliant. Kernel versions prior to 2.6 were not compliant, and today Linux isn't officially POSIX-compliant because they haven't gone out of their way to get certified (which will likely never happen). Regardless, Linux can be treated as a POSIX system for almost all intents and purposes.
",https://stackoverflow.com//questions/5785516/is-mac-os-x-a-posix-os
linux - Avoid gnome-terminal close after script execution?,"I created a bash script that opens several gnome-terminals, connect to classroom computers via ssh and run a script.","
As I understand you want gnome-terminal to open, have it execute some commands, and then drop to the prompt so you can enter some more commands. Gnome-terminal is not designed for this use case, but there are workarounds:

Let gnome-terminal run bash and tell bash to run your commands and then start a new bash
$ gnome-terminal -- bash -c ""echo foo; echo bar; exec bash""

or if the commands are in a script
$ gnome-terminal -- bash -c ""./scripttorun; exec bash""

The first bash will terminate once all the commands are done. But the last command is a new bash which will then just keep running. And since something is still running gnome-terminal will not close.

Let gnome-terminal run bash with a prepared rcfile which runs your commands
Prepare somercfile:
source ~/.bashrc
echo foo
echo bar

Then run:
$ gnome-terminal -- bash --rcfile somercfile

bash will stay open after running somercfile.
i must admit i do not understand completely why --rcfile has this behaviour but it does.

Let gnome-terminal run a script which runs your commands and then drops to bash
Prepare scripttobash:
#!/bin/sh
echo foo
echo bar
exec bash

Set this file as executable.
Then run:
$ gnome-terminal -- ./scripttobash


for completeness
if you just want to be able read the output of the command and need no interactivity

go to preferences (hamburger button -> preferences)
go to profiles (standard or create a new one)
go to command tab
when command exits -> hold the terminal open

i recommend to create a new profile for just for this use case.
use the profile like this:
gnome-terminal --profile=holdopen -- ./scripttorun


Every method has it's quirks. You must choose, but choose wisely.
I like the first solution. it does not need extra files or profiles. and the command says what it does: run commands then run bash again.
All that said, since you used ssh in your example, you might want to take a look at pssh (parallel ssh). here an article: https://www.cyberciti.biz/cloud-computing/how-to-use-pssh-parallel-ssh-program-on-linux-unix/
",https://stackoverflow.com//questions/3512055/avoid-gnome-terminal-close-after-script-execution
linux - Setting up FTP on Amazon Cloud Server," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
Jaminto did a great job of answering the question, but I recently went through the process myself and wanted to expand on Jaminto's answer.
I'm assuming that you already have an EC2 instance created and have associated an Elastic IP Address to it.

Step #1: Install vsftpd
SSH to your EC2 server.  Type:
> sudo yum install vsftpd

This should install vsftpd.

Step #2: Open up the FTP ports on your EC2 instance
Next, you'll need to open up the FTP ports on your EC2 server.  Log in to the AWS EC2 Management Console and select Security Groups from the navigation tree on the left.  Select the security group assigned to your EC2 instance.  Then select the Inbound tab, then click Edit:

Add two Custom TCP Rules with port ranges 20-21 and 1024-1048.  For Source, you can select 'Anywhere'.  If you decide to set Source to your own IP address, be aware that your IP address might change if it is being assigned via DHCP.


Step #3: Make updates to the vsftpd.conf file
Edit your vsftpd conf file by typing:
> sudo vi /etc/vsftpd/vsftpd.conf

Disable anonymous FTP by changing this line:
anonymous_enable=YES

to 
anonymous_enable=NO

Then add the following lines to the bottom of the vsftpd.conf file:
pasv_enable=YES
pasv_min_port=1024
pasv_max_port=1048
pasv_address=<Public IP of your instance> 

Your vsftpd.conf file should look something like the following - except make sure to replace the pasv_address with your public facing IP address:

To save changes, press escape, then  type :wq, then hit enter.

Step #4: Restart vsftpd
Restart vsftpd by typing:
> sudo /etc/init.d/vsftpd restart

You should see a message that looks like:


If this doesn't work, try:
> sudo /sbin/service vsftpd restart


Step #5: Create an FTP user
If you take a peek at /etc/vsftpd/user_list, you'll see the following:
# vsftpd userlist
# If userlist_deny=NO, only allow users in this file
# If userlist_deny=YES (default), never allow users in this file, and
# do not even prompt for a password.
# Note that the default vsftpd pam config also checks /etc/vsftpd/ftpusers
# for users that are denied.
root
bin
daemon
adm
lp
sync
shutdown
halt
mail
news
uucp
operator
games
nobody

This is basically saying, ""Don't allow these users FTP access.""  vsftpd will allow FTP access to any user not on this list.
So, in order to create a new FTP account, you may need to create a new user on your server.  (Or, if you already have a user account that's not listed in /etc/vsftpd/user_list, you can skip to the next step.)
Creating a new user on an EC2 instance is pretty simple.  For example, to create the user 'bret', type:
> sudo adduser bret
> sudo passwd bret

Here's what it will look like:


Step #6: Restricting users to their home directories
At this point, your FTP users are not restricted to their home directories. That's not very secure, but we can fix it pretty easily.  
Edit your vsftpd conf file again by typing:
> sudo vi /etc/vsftpd/vsftpd.conf

Un-comment out the line:
chroot_local_user=YES

It should look like this once you're done:
 
Restart the vsftpd server again like so:
> sudo /etc/init.d/vsftpd restart

All done!

Appendix A: Surviving a reboot
vsftpd doesn't automatically start when your server boots.  If you're like me, that means that after rebooting your EC2 instance, you'll feel a moment of terror when FTP seems to be broken - but in reality, it's just not running!.  Here's a handy way to fix that:
> sudo chkconfig --level 345 vsftpd on

Alternatively, if you are using redhat, another way to manage your services is by using this nifty graphic user interface to control which services should automatically start:
>  sudo ntsysv


Now vsftpd will automatically start up when your server boots up.

Appendix B: Changing a user's FTP home directory
* NOTE: Iman Sedighi has posted a more elegant solution for restricting users access to a specific directory.  Please refer to his excellent solution posted as an answer *
You might want to create a user and restrict their FTP access to a specific folder, such as /var/www.  In order to do this, you'll need to change the user's default home directory:
> sudo usermod -d /var/www/ username

In this specific example, it's typical to give the user permissions to the 'www' group, which is often associated with the /var/www folder:  
> sudo usermod -a -G www username

",https://stackoverflow.com//questions/7052875/setting-up-ftp-on-amazon-cloud-server
linux - Finding Docker container processes? (from host point of view),I am doing some tests on docker and containers and I was wondering:,"
You can use docker top command.
This command lists all processes running within your container.
For instance this command on a single process container on my box displays:
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                14097               13930               0                   23:17               pts/6               00:00:00            /bin/bash

All methods mentioned by others are also possible to use but this one should be easiest.
Update:
To simply get the main process id within the container use this command:
 docker inspect -f '{{.State.Pid}}' <container id>

",https://stackoverflow.com//questions/34878808/finding-docker-container-processes-from-host-point-of-view
linux - tcp_tw_reuse vs tcp_tw_recycle : Which to use (or both)?,"I have a website and application which use a significant number of connections.  It normally has about 3,000 connections statically open, and can receive anywhere from 5,000 to 50,000 connection attempts in a few seconds time frame.","
According to Linux documentation, you should use the TCP_TW_REUSE flag to allow reusing sockets in TIME_WAIT state for new connections. 
It seems to be a good option when dealing with a web server that have to handle many short TCP connections left in a TIME_WAIT state.
As described here, The TCP_TW_RECYCLE could cause some problems when using load balancers...
EDIT (to add some warnings ;) ):
as mentionned in comment by @raittes, the ""problems when using load balancers"" is about public-facing servers. When recycle is enabled, the server can't distinguish new incoming connections from different clients behind the same NAT device.
",https://stackoverflow.com//questions/6426253/tcp-tw-reuse-vs-tcp-tw-recycle-which-to-use-or-both
JSON command line formatter tool for Linux,Want to improve this question? Update the question so it's on-topic for Stack Overflow.,"
 alias pp='python -mjson.tool'
 pp mydata.json

From the first link in the accepted answer: http://ruslanspivak.com/2010/10/12/pretty-print-json-from-the-command-line/
",https://stackoverflow.com//questions/5243885/json-command-line-formatter-tool-for-linux
c++ - How to fix: /usr/lib/libstdc++.so.6: version `GLIBCXX_3.4.15' not found,"So I'm now desperate in finding a fix for this. I'm compiling a shared library .so in Ubuntu 32 bit (Have tried doing it under Debian and Ubuntu 64 bit, but none worked either)","
Link statically to libstdc++ with -static-libstdc++ gcc option.
",https://stackoverflow.com//questions/19386651/how-to-fix-usr-lib-libstdc-so-6-version-glibcxx-3-4-15-not-found
c++ - Finding current executable's path without /proc/self/exe,"It seems to me that Linux has it easy with /proc/self/exe. But I'd like to know if there is a convenient way to find the current application's directory in C/C++ with cross-platform interfaces. I've seen some projects mucking around with argv[0], but it doesn't seem entirely reliable. ","
Some OS-specific interfaces:

Mac OS X: _NSGetExecutablePath() (man 3 dyld)
Linux: readlink /proc/self/exe
Solaris: getexecname()
FreeBSD: sysctl CTL_KERN KERN_PROC KERN_PROC_PATHNAME -1
FreeBSD if it has procfs: readlink /proc/curproc/file (FreeBSD doesn't have procfs by default)
NetBSD: readlink /proc/curproc/exe
DragonFly BSD: readlink /proc/curproc/file 
Windows: GetModuleFileName() with hModule = NULL

There are also third party libraries that can be used to get this information, such as whereami as mentioned in prideout's answer, or if you are using Qt,  QCoreApplication::applicationFilePath() as mentioned in the comments.
The portable (but less reliable) method is to use argv[0].  Although it could be set to anything by the calling program, by convention it is set to either a path name of the executable or a name that was found using $PATH.
Some shells, including bash and ksh, set the environment variable ""_"" to the full path of the executable before it is executed.  In that case you can use getenv(""_"") to get it.  However this is unreliable because not all shells do this, and it could be set to anything or be left over from a parent process which did not change it before executing your program.
",https://stackoverflow.com//questions/1023306/finding-current-executables-path-without-proc-self-exe
"Locking Executing Files: Windows does, Linux doesn't. Why?","I noticed when a file is executed on Windows (.exe or .dll), it is locked and cannot be deleted, moved or modified.","
Linux has a reference-count mechanism, so you can delete the file while it is executing, and it will continue to exist as long as some process (Which previously opened it) has an open handle for it. The directory entry for the file is removed when you delete it, so it cannot be opened any more, but processes already using this file can still use it. Once all processes using this file terminate, the file is deleted automatically.
Windows does not have this capability, so it is forced to lock the file until all processes executing from it have finished.
I believe that the Linux behavior is preferable. There are probably some deep architectural reasons, but the prime (and simple) reason I find most compelling is that in Windows, you sometimes cannot delete a file, you have no idea why, and all you know is that some process is keeping it in use. In Linux it never happens.
",https://stackoverflow.com//questions/196897/locking-executing-files-windows-does-linux-doesnt-why
python - How can I prevent Google Colab from disconnecting?,Is there a way to programmatically prevent Google Colab from disconnecting on a timeout?,"
As of March 2021, none of these methods will work as Google added a CAPTCHA button that randomly pops up after some time.
Prior to that, the solution was very easy, and didn't need any JavaScript. Just create a new cell at the bottom having the following line:
while True:pass

Now keep the cell in the run sequence so that the infinite loop won't stop and thus keep your session alive.
Old method:
Set a JavaScript interval to click on the connect button every 60 seconds.
Open developer-settings (in your web-browser) with Ctrl+Shift+I then click on console tab and type this on the console prompt. (for mac press Option+Command+I)
function ConnectButton(){
  console.log(""Connect pushed"");
  document.querySelector(""#top-toolbar > colab-connectbutton"").shadowRoot.querySelector(""#connect"").click()
}
setInterval(ConnectButton,60000);

",https://stackoverflow.com//questions/57113226/how-can-i-prevent-google-colab-from-disconnecting
linux - Is it better to use git grep than plain grep if we want to search in versioned source code?,"In a git repository, is there any difference/benefit using git grep over good old grep?
An example would be?","
The two are very similar. The main difference is that git grep defaults to searching in the files that are tracked by git.
Examples
If I want to find foo within my project I can use git grep or good ol' stand-alone grep:
git grep foo
grep -R foo .

The git grep version will only search in files tracked by git, whereas the grep version will search everything in the directory. So far so similar; either one could be better depending on what you want to achieve.
What if we want to limit the search to only .rb files?
git grep foo -- *.rb
grep -R --include=*.rb foo .

The plain old grep version is getting a bit more wordy, but if you're used to using grep that may not be a problem. They're still not going to search exactly the same files, but again it depends on what you want to achieve.
What about searching in the previous version of the project?
git grep foo HEAD^
git checkout HEAD^; grep -R foo .; git checkout -

This is where git grep makes a real difference: You can search in another revision of the project without checking it out first. This isn't a situation that comes up too often for me though; I usually want to search in the version of the project I have checked out.
Configuring git grep
There are some git config variables that modify the behaviour of git grep and avoid the need to pass a couple of command line arguments:

grep.lineNumber: Always show line numbers of matches (you can pass -n to both grep and git grep to get this behaviour)
grep.extendedRegexp: Always use extended regular expressions (you can pass -E to both grep and git grep to get this behaviour)

In practice
In practice I have gg aliased to git grep -En, and this almost always does what I want.
",https://stackoverflow.com//questions/17557684/is-it-better-to-use-git-grep-than-plain-grep-if-we-want-to-search-in-versioned-s
linux - How to set proxy for wget?,I want to download something with wget using a proxy:,"
For all users of the system via the /etc/wgetrc or for the user only with the ~/.wgetrc file:
use_proxy=yes
http_proxy=127.0.0.1:8080
https_proxy=127.0.0.1:8080

or via -e options placed after the URL:
wget ... -e use_proxy=yes -e http_proxy=127.0.0.1:8080 ...

",https://stackoverflow.com//questions/11211705/how-to-set-proxy-for-wget
Kill python interpeter in linux from the terminal,"I want to kill python interpeter - The intention is that all the python files that are running in this moment will stop (without any informantion about this files).
obviously the processes should be closed.","
pkill -9 python

should kill any running python process. 
",https://stackoverflow.com//questions/18428750/kill-python-interpeter-in-linux-from-the-terminal
c++ - How do I install g++ for Fedora?,How do I install g++ for Fedora Linux? I have been searching the dnf command to install g++ but didn't find anything.,"
The package you're looking for is confusingly named gcc-c++.
",https://stackoverflow.com//questions/12952913/how-do-i-install-g-for-fedora
linux - How can I set an environment variable only for the duration of the script?,"On Linux (Ubuntu 11.04 (Natty Narwhal)) in Bash, is it possible to temporarily set an environment variable that will only be different from the normal variable for the duration of the script?","
VAR1=value1 VAR2=value2 myScript args ...

",https://stackoverflow.com//questions/7128542/how-can-i-set-an-environment-variable-only-for-the-duration-of-the-script
linux - make -j 8 g++: internal compiler error: Killed (program cc1plus),"When I deploy Apache Mesos on Ubuntu12.04, I follow the official document, in step ""make -j 8"" I'm getting this error in the console:","
Try running (just after the failure) dmesg.
Do you see a line like this?
Out of memory: Kill process 23747 (cc1plus) score 15 or sacrifice child
Killed process 23747, UID 2243, (cc1plus) total-vm:214456kB, anon-rss:178936kB, file-rss:5908kB

Most likely that is your problem. Running make -j 8 runs lots of process which use more memory. The problem above occurs when your system runs out of memory. In this case rather than the whole system falling over, the operating systems runs a process to score each process on the system. The one that scores the highest gets killed by the operating system to free up memory. If the process that is killed is cc1plus, gcc (perhaps incorrectly) interprets this as the process crashing and hence assumes that it must be a compiler bug. But it isn't really, the problem is the OS killed cc1plus, rather than it crashed.
If this is the case, you are running out of memory. So run perhaps make -j 4 instead. This will mean fewer parallel jobs and will mean the compilation will take longer but hopefully will not exhaust your system memory.
",https://stackoverflow.com//questions/30887143/make-j-8-g-internal-compiler-error-killed-program-cc1plus
linux - nginx not listening to port 80,"I've just installed a Ubuntu 12.04 server and nginx 1.2.7, removed default from sites-enabled and added my own file into sites-available and symlink at sites-enabled. Then restarted nginx.","
I had this same problem, the solution was that I had not symlinked my siteconf file correctly. Try running vim /etc/nginx/sites-enabled/mysite.com—can you get to it? I was getting ""Permission Denied.""
If not run:
rm /etc/nginx/sites-enabled/mysite.com
ln -s /etc/nginx/sites-available/mysite.com /etc/nginx/sites-enabled/mysite.com

",https://stackoverflow.com//questions/16021481/nginx-not-listening-to-port-80
multithreading - How can I monitor the thread count of a process on linux?,"I would like to monitor the number of threads used by a specific process on Linux.
Is there an easy way to get this information without impacting the performance of the process?","
try
ps huH p <PID_OF_U_PROCESS> | wc -l

or htop
",https://stackoverflow.com//questions/268680/how-can-i-monitor-the-thread-count-of-a-process-on-linux
linux - How to concatenate two strings to build a complete path,"I am trying to write a bash script. In this script I want user to enter a path of a directory. Then I want to append some strings at the end of this string and build a path to some subdirectories.
For example assume user enters an string like this:","
The POSIX standard mandates that multiple / are treated as a single / in a file name. Thus
//dir///subdir////file is the same as /dir/subdir/file.
As such concatenating a two strings to build a complete path is a simple as:
full_path=""$part1/$part2""

",https://stackoverflow.com//questions/11226322/how-to-concatenate-two-strings-to-build-a-complete-path
"linux - When grep ""\\"" XXFile I got ""Trailing Backslash""","Now I want to find whether there are lines containing '\' character. I tried grep ""\\"" XXFile but it hints ""Trailing Backslash"". But when I tried grep '\\' XXFile it is OK. Could anyone explain why the first case cannot run? Thanks.","
The difference is in how the shell treats the backslashes:

When you write ""\\"" in double quotes, the shell interprets the backslash escape and ends up passing the string \ to grep. Grep then sees a backslash with no following character, so it emits a ""trailing backslash"" warning. If you want to use double quotes you need to apply two levels of escaping, one for the shell and one for grep. The result: ""\\\\"".
When you write '\\' in single quotes, the shell does not do any interpretation, which means grep receives the string \\ with both backslashes intact. Grep interprets this as an escaped backslash, so it searches the file for a literal backslash character.

If that's not clear, we can use echo to see exactly what the shell is doing. echo doesn't do any backslash interpretation itself, so what it prints is what the shell passed to it.
$ echo ""\\""
\
$ echo '\\'
\\

",https://stackoverflow.com//questions/20342464/when-grep-xxfile-i-got-trailing-backslash
linux - how to find the target file's full(absolute path) of the symbolic link or soft link in python,"when i give 
ls -l /etc/fonts/conf.d/70-yes-bitmaps.conf ","
os.path.realpath(path)

os.path.realpath returns the canonical path of the specified filename, eliminating any symbolic links encountered in the path.
",https://stackoverflow.com//questions/3220755/how-to-find-the-target-files-fullabsolute-path-of-the-symbolic-link-or-soft-l
linux - rsync - mkstemp failed: Permission denied (13)," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
Make sure the user you're rsync'd into on the remote machine has write access to the contents of the folder AND the folder itself, as rsync tried to update the modification time on the folder itself.
",https://stackoverflow.com//questions/11039559/rsync-mkstemp-failed-permission-denied-13
linux - sh: 0: getcwd() failed: No such file or directory on cited drive,I am trying to compile ARM code on Ubuntu 12.04 (Precise Pangolin).,"
This error is usually caused by running a command from a directory that no longer exists.
Try changing your directory and rerun the command.
",https://stackoverflow.com//questions/12758125/sh-0-getcwd-failed-no-such-file-or-directory-on-cited-drive
linux - how to find the target file's full(absolute path) of the symbolic link or soft link in python,"when i give 
ls -l /etc/fonts/conf.d/70-yes-bitmaps.conf ","
os.path.realpath(path)

os.path.realpath returns the canonical path of the specified filename, eliminating any symbolic links encountered in the path.
",https://stackoverflow.com//questions/3220755/how-to-find-the-target-files-fullabsolute-path-of-the-symbolic-link-or-soft-l
c - Undefined reference to pthread_create in Linux,I picked up the following demo off the web from https://computing.llnl.gov/tutorials/pthreads/,"
For Linux the correct command is:
gcc -pthread -o term term.c

In general, libraries should follow sources and objects on command line, and -lpthread is not an ""option"", it's a library specification. On a system with only libpthread.a installed,
gcc -lpthread ...

will fail to link.
Read this or this detailed explanation.
",https://stackoverflow.com//questions/1662909/undefined-reference-to-pthread-create-in-linux
linux - Debugging crontab jobs,I have added a crontab entry on a Linux server that will run a Java executable. The Java code uses its own class for logging errors and messages into a log file.,"
You can enable logging for cron jobs in order to track problems. You need to edit the /etc/rsyslog.conf or /etc/rsyslog.d/50-default.conf (on Ubuntu) file and make sure you have the following line uncommented or add it if it is missing:
cron.*                         /var/log/cron.log

Then restart rsyslog and cron:
sudo service rsyslog restart
sudo service cron restart

Cron jobs will log to /var/log/cron.log.
",https://stackoverflow.com//questions/4883069/debugging-crontab-jobs
"linux - Explain the effects of export LANG, LC_CTYPE, and LC_ALL"," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
I'll explain with detail:
export LANG=ru_RU.UTF-8

That is a shell command that will export an environment variable named LANG with the given value ru_RU.UTF-8. That instructs internationalized programs to use the Russian language (ru), variant from Russia (RU), and the UTF-8 encoding for console output.
Generally this single line is enough.
This other one:
export LC_CTYPE=ru_RU.UTF-8

Does a similar thing, but it tells the program not to change the language, but only the CTYPE to Russian. If a program can change a text to uppercase, then it will use the Russian rules to do so, even though the text itself may be in English.
It is worth saying that mixing LANG and LC_CTYPE can give unexpected results, because few people do that, so it is quite untested, unless maybe:
export LANG=ru_RU.UTF-8
export LC_CTYPE=C

That will make the program output in Russian, but the CTYPE standard old C style.
The last line, LC_ALL is a last resort override, that will make the program ignore all the other LC_* variables and use this. I think that you should never write it in a profile line, but use it to run a program in a given language. For example, if you want to write a bug report, and you don't want any kind of localized output, and you don't know which LC_* variables are set:
LC_ALL=C program

About changing the language of all your programs or only the console, that depends on where you put these lines. I put mine in ~/.bashrc so they don't apply to the GUI, only to the bash consoles.
",https://stackoverflow.com//questions/30479607/explain-the-effects-of-export-lang-lc-ctype-and-lc-all
linux - Rename all files in a folder with a prefix in a single command,"Rename all the files within a folder with prefix ""Unix_"" ","
If your filenames contain no whitepace and you don't have any subdirectories, you can use a simple for loop:
$ for FILENAME in *; do mv $FILENAME Unix_$FILENAME; done 

Otherwise use the convenient rename command (which is a perl script) - although it might not be available out of the box on every Unix (e.g. OS X doesn't come with rename).
A short overview at debian-administration.org:

Easily renaming multiple files

If your filenames contain whitespace it's easier to use find, on Linux the following should work:
$ find . -type f -name '*' -printf ""echo mv '%h/%f' '%h/Unix_%f\n'"" | sh

On BSD systems, there is no -printf option, unfortunately. But GNU findutils should be installable (on e.g. Mac OS X with brew install findutils).
$ gfind . -type f -name '*' -printf ""mv \""%h/%f\"" \""%h/Unix_%f\""\n"" | sh

",https://stackoverflow.com//questions/6329505/rename-all-files-in-a-folder-with-a-prefix-in-a-single-command
How is the Linux kernel tested?,How do the Linux kernel developers test their code locally and after they have it committed? Do they use some kind of unit testing and build automation? Test plans?,"
The Linux kernel has a heavy emphasis on community testing.
Typically, any developer will test their own code before submitting, and quite often they will be using a development version of the kernel from Linus, or one of the other unstable/development trees for a project relevant to their work. This means they are often testing both their changes and other people's changes.
There tends not to be much in the way of formal test plans, but extra testing may be asked for before features are merged into upstream trees.
As Dean pointed out, there's also some automated testing: The Linux Test Project and the kernel Autotest (good overview).
Developers will often also write automated tests targeted to test their change, but I'm not sure there's a (often used) mechanism to centrally collect these ad hoc tests.
It depends a lot on which area of the kernel is being changed of course - the testing you'd do for a new network driver is quite different to the testing you'd do when replacing the core scheduling algorithm.
",https://stackoverflow.com//questions/3177338/how-is-the-linux-kernel-tested
linux - How can I open some ports on Ubuntu?,"I know a little about Linux. Today I created a VPN server on my Ubuntu installation according to Set up a simple IPsec/L2TP VPN server for Ubuntu, Arch Linux and Debian.","
Ubuntu these days comes with UFW - Uncomplicated Firewall. UFW is an easy-to-use method of handling iptables rules.
Try using this command to allow a port:
sudo ufw allow 1701

To test connectivity, you could try shutting down the VPN software (freeing up the ports) and using netcat to listen, like this:
nc -l 1701

Then use telnet from your Windows host and see what shows up on your Ubuntu terminal. This can be repeated for each port you'd like to test.
",https://stackoverflow.com//questions/30251889/how-can-i-open-some-ports-on-ubuntu
java - Detecting Windows or Linux?,I am seeking to run a common Java program in both Windows and Linux.,"
apache commons lang has a class SystemUtils.java
you can use :
SystemUtils.IS_OS_LINUX
SystemUtils.IS_OS_WINDOWS

",https://stackoverflow.com//questions/14288185/detecting-windows-or-linux
linux - couldn't connect to server 127.0.0.1 shell/mongo.js,"when i setup mongodb in my ubuntu , i try : ./mongo it show this error :","

Manually remove the lockfile: sudo rm /var/lib/mongodb/mongod.lock
Run the repair script: sudo -u mongodb mongod -f /etc/mongodb.conf    --repair

Please note the following:

You must run this command as the mongodb user. If you run it as root,
then root will own files in /var/lib/mongodb/ that are necessary to
run the mongodb daemon and therefore when the daemon trys to run
later as the mongodb user, it won't have permissions to start. In
that case you'll get this error: Unable to create / open lock file
for lockfilepath: /var/lib/mongodb/mongod.lock errno:13 Permission
denied, terminating.
On Ubuntu, you must specify the configuration file /etc/mongodb.conf
using the -f flag. Otherwise it will look for the data files in the
wrong place and you will see the following error: dbpath (/data/db/)
does not exist, terminating.

",https://stackoverflow.com//questions/5726032/couldnt-connect-to-server-127-0-0-1-shell-mongo-js
"linux - What's the difference between .so, .la and .a library files?",I know an .so file is a kind of dynamic library (lots of threads can share such libraries so there is no need to have more than one copy of it in memory). But what is the difference between .a and .la? Are these all static libraries?,"
File type breakdown
.so files are dynamic libraries. The suffix stands for ""shared object"", because all the applications that are linked with the library use the same file, rather than making a copy in the resulting executable.
.a files are static libraries. The suffix stands for ""archive"", because they're actually just an archive (made with the ar command -- a predecessor of tar that's now just used for making libraries) of the original .o object files.
.la files are text files used by the GNU ""libtools"" package to describe the files that make up the corresponding library. You can find more information about them in this question: What are libtool's .la file for?
Static vs Dynamic
Static

Pro: The user always uses the version of the library that you've tested with your application, so there shouldn't be any surprising compatibility problems.

Con: If a problem is fixed in a library, you need to redistribute your application to take advantage of it. However, unless it's a library that users are likely to update on their own, you'd might need to do this anyway.


Dynamic

Pro: Your process's memory footprint is smaller, because the memory used for the library is amortized among all the processes using the library.

Pro: Libraries can be loaded on demand at run time; this is good for plugins, so you don't have to choose the plugins to be used when compiling and installing the software. New plugins can be added on the fly.

Con: The library might not exist on the system where someone is trying to install the application, or they might have a version that's not compatible with the application. To mitigate this, the application package might need to include a copy of the library, so it can install it if necessary. This is also often mitigated by package managers, which can download and install any necessary dependencies.

Con: Link-Time Optimization is generally not possible, so there could possibly be efficiency implications in high-performance applications. See the Wikipedia discussion of WPO and LTO.


Dynamic libraries are especially useful for system libraries, like libc. These libraries often need to include code that's dependent on the specific OS and version, because kernel interfaces have changed. If you link a program with a static system library, it will only run on the version of the OS that this library version was written for. But if you use a dynamic library, it will automatically pick up the library that's installed on the system you run on.
",https://stackoverflow.com//questions/12237282/whats-the-difference-between-so-la-and-a-library-files
linux - /bin/sh: pushd: not found,I am doing the following inside a make file ,"
pushd is a bash enhancement to the POSIX-specified Bourne Shell. pushd cannot be easily implemented as a command, because the current working directory is a feature of a process that cannot be changed by child processes. (A hypothetical pushd command might do the chdir(2) call and then start a new shell, but ... it wouldn't be very usable.) pushd is a shell builtin, just like cd.
So, either change your script to start with #!/bin/bash or store the current working directory in a variable, do your work, then change back. Depends if you want a shell script that works on very reduced systems (say, a Debian build server) or if you're fine always requiring bash.
",https://stackoverflow.com//questions/5193048/bin-sh-pushd-not-found
What is a good easy to use profiler for C++ on Linux?," Questions asking us to recommend or find a tool, library or favorite off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, describe the problem and what has been done so far to solve it.","
Use gprof.
Just compile with -pg flag (I think (but am not sure) you have to turn of optimizations though.) and use gprof to analyze the gmon.out file that your executable will then produce.
eg:
gcc -pg -o whatever whatever.c

./whatever

gprof whatever gmon.out

Same thing with g++ and cpp.
",https://stackoverflow.com//questions/1168766/what-is-a-good-easy-to-use-profiler-for-c-on-linux
"c - Difference between ""system"" and ""exec"" in Linux?",What is the difference between system and exec family commands?  Especially I want to know which one of them creates child process to work?,"
system() calls out to sh to handle your command line, so you can get wildcard expansion, etc.  exec() and its friends replace the current process image with a new process image.
With system(), your program continues running and you get back some status about the external command you called.  With exec(), your process is obliterated.
In general, I guess you could think of system() as a higher-level interface.  You could duplicate its functionality yourself using some combination fork(), exec(), and wait().
To answer your final question, system() causes a child process to be created, and the exec() family do not.  You would need to use fork() for that.
",https://stackoverflow.com//questions/1697440/difference-between-system-and-exec-in-linux
linux - Debugging crontab jobs,I have added a crontab entry on a Linux server that will run a Java executable. The Java code uses its own class for logging errors and messages into a log file.,"
You can enable logging for cron jobs in order to track problems. You need to edit the /etc/rsyslog.conf or /etc/rsyslog.d/50-default.conf (on Ubuntu) file and make sure you have the following line uncommented or add it if it is missing:
cron.*                         /var/log/cron.log

Then restart rsyslog and cron:
sudo service rsyslog restart
sudo service cron restart

Cron jobs will log to /var/log/cron.log.
",https://stackoverflow.com//questions/4883069/debugging-crontab-jobs
c - Writing programs to cope with I/O errors causing lost writes on Linux,"TL;DR: If the Linux kernel loses a buffered I/O write, is there any way for the application to find out?","
fsync() returns -EIO if the kernel lost a write
(Note: early part references older kernels; updated below to reflect modern kernels)
It looks like async buffer write-out in end_buffer_async_write(...) failures set an -EIO flag on the failed dirty buffer page for the file:
set_bit(AS_EIO, &page->mapping->flags);
set_buffer_write_io_error(bh);
clear_buffer_uptodate(bh);
SetPageError(page);

which is then detected by wait_on_page_writeback_range(...) as called by do_sync_mapping_range(...) as called by sys_sync_file_range(...) as called by sys_sync_file_range2(...) to implement the C library call fsync().
But only once!
This comment on  sys_sync_file_range
168  * SYNC_FILE_RANGE_WAIT_BEFORE and SYNC_FILE_RANGE_WAIT_AFTER will detect any
169  * I/O errors or ENOSPC conditions and will return those to the caller, after
170  * clearing the EIO and ENOSPC flags in the address_space.

suggests that when fsync() returns -EIO or (undocumented in the manpage) -ENOSPC, it will clear the error state so a subsequent fsync() will report success even though the pages never got written.
Sure enough wait_on_page_writeback_range(...) clears the error bits when it tests them:
301         /* Check for outstanding write errors */
302         if (test_and_clear_bit(AS_ENOSPC, &mapping->flags))
303                 ret = -ENOSPC;
304         if (test_and_clear_bit(AS_EIO, &mapping->flags))
305                 ret = -EIO;

So if the application expects it can re-try fsync() until it succeeds and trust that the data is on-disk, it is terribly wrong.
I'm pretty sure this is the source of the data corruption I found in the DBMS. It retries fsync() and thinks all will be well when it succeeds.
Is this allowed?
The POSIX/SuS docs on fsync() don't really specify this either way:

If the fsync() function fails, outstanding I/O operations are not guaranteed to have been completed.

Linux's man-page for fsync() just doesn't say anything about what happens on failure.
So it seems that the meaning of fsync() errors is ""I don't know what happened to your writes, might've worked or not, better try again to be sure"".
Newer kernels
On 4.9 end_buffer_async_write sets -EIO on the page, just via mapping_set_error.
    buffer_io_error(bh, "", lost async page write"");
    mapping_set_error(page->mapping, -EIO);
    set_buffer_write_io_error(bh);
    clear_buffer_uptodate(bh);
    SetPageError(page);

On the sync side I think it's similar, though the structure is now pretty complex to follow. filemap_check_errors in mm/filemap.c now does:
    if (test_bit(AS_EIO, &mapping->flags) &&
        test_and_clear_bit(AS_EIO, &mapping->flags))
            ret = -EIO;

which has much the same effect. Error checks seem to all go through filemap_check_errors which does a test-and-clear:
    if (test_bit(AS_EIO, &mapping->flags) &&
        test_and_clear_bit(AS_EIO, &mapping->flags))
            ret = -EIO;
    return ret;

I'm using btrfs on my laptop, but when I create an ext4 loopback for testing on /mnt/tmp and set up a perf probe on it:
sudo dd if=/dev/zero of=/tmp/ext bs=1M count=100
sudo mke2fs -j -T ext4 /tmp/ext
sudo mount -o loop /tmp/ext /mnt/tmp

sudo perf probe filemap_check_errors

sudo perf record -g -e probe:end_buffer_async_write -e probe:filemap_check_errors dd if=/dev/zero of=/mnt/tmp/test bs=4k count=1 conv=fsync

I find the following call stack in perf report -T:
        ---__GI___libc_fsync
           entry_SYSCALL_64_fastpath
           sys_fsync
           do_fsync
           vfs_fsync_range
           ext4_sync_file
           filemap_write_and_wait_range
           filemap_check_errors

A read-through suggests that yeah, modern kernels behave the same.
This seems to mean that if fsync() (or presumably write() or close()) returns -EIO, the file is in some undefined state between when you last successfully fsync()d or close()d it and its most recently write()ten state.
Test
I've implemented a test case to demonstrate this behaviour.
Implications
A DBMS can cope with this by entering crash recovery. How on earth is a normal user application supposed to cope with this? The fsync() man page gives no warning that it means ""fsync-if-you-feel-like-it"" and I expect a lot of apps won't cope well with this behaviour.
Bug reports

https://bugzilla.kernel.org/show_bug.cgi?id=194755
https://bugzilla.kernel.org/show_bug.cgi?id=194757

Further reading
lwn.net touched on this in the article ""Improved block-layer error handling"".
postgresql.org mailing list thread.
",https://stackoverflow.com//questions/42434872/writing-programs-to-cope-with-i-o-errors-causing-lost-writes-on-linux
linux - how to detect invalid utf8 unicode/binary in a text file,"I need to detect corrupted text file where there are invalid (non-ASCII) utf-8, Unicode or binary characters. ","
Assuming you have your locale set to UTF-8 (see locale output), this works well to recognize invalid UTF-8 sequences:
grep -axv '.*' file.txt

Explanation (from grep man page):

-a, --text: treats file as text, essential prevents grep to abort once finding an invalid byte sequence (not being utf8)
-v, --invert-match: inverts the output showing lines not matched
-x '.*' (--line-regexp): means to match a complete line consisting of any utf8 character.

Hence, there will be output, which is the lines containing the invalid not utf8 byte sequence containing lines (since inverted -v)
",https://stackoverflow.com//questions/29465612/how-to-detect-invalid-utf8-unicode-binary-in-a-text-file
linux - How to check existence of a folder with python and then remove it?,"I want to remove dataset folder from dataset3 folder. But the following code is not removing dataset. 
First I want to check if dataset already exist in dataset then remove dataset. 
Can some one please point out my mistake in following code?","
Python's os.rmdir() only works on empty the directories, however shutil.rmtree() doesn't care (even if there are subdirectories) which makes it very similar to the Linux rm -rf command.
import os
import shutil

dirpath = os.path.join('dataset3', 'dataset')
if os.path.exists(dirpath) and os.path.isdir(dirpath):
    shutil.rmtree(dirpath)

Modern approach
In Python 3.4+ you can do same thing using the pathlib module to make the code more object-oriented and readable:
from pathlib import Path
import shutil

dirpath = Path('dataset3') / 'dataset'
if dirpath.exists() and dirpath.is_dir():
    shutil.rmtree(dirpath)

",https://stackoverflow.com//questions/43765117/how-to-check-existence-of-a-folder-with-python-and-then-remove-it
linux - rsync - mkstemp failed: Permission denied (13)," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
Make sure the user you're rsync'd into on the remote machine has write access to the contents of the folder AND the folder itself, as rsync tried to update the modification time on the folder itself.
",https://stackoverflow.com//questions/11039559/rsync-mkstemp-failed-permission-denied-13
JSON command line formatter tool for Linux,Want to improve this question? Update the question so it's on-topic for Stack Overflow.,"
 alias pp='python -mjson.tool'
 pp mydata.json

From the first link in the accepted answer: http://ruslanspivak.com/2010/10/12/pretty-print-json-from-the-command-line/
",https://stackoverflow.com//questions/5243885/json-command-line-formatter-tool-for-linux
linux - mysql_config not found when installing mysqldb python interface,"I am trying to get a Python script to run on the linux server I'm connected to via ssh. The script uses mysqldb. I have all the other components I need, but when I try to install mySQLdb via setuptools like so:, ","
mySQLdb is a python interface for mysql, but it is not mysql itself. And apparently mySQLdb needs the command 'mysql_config', so you need to install that first.
Can you confirm that you did or did not install mysql itself, by running ""mysql"" from the shell? That should give you a response other than ""mysql: command not found"". 
Which linux distribution are you using? Mysql is pre-packaged for most linux distributions. For example, for debian / ubuntu, installing mysql is as easy as 
sudo apt-get install mysql-server

mysql-config is in a different package, which can be installed from (again, assuming debian / ubuntu):
sudo apt-get install libmysqlclient-dev

if you are using mariadb, the drop in replacement for mysql, then run
sudo apt-get install libmariadbclient-dev

Reference:
https://github.com/JudgeGirl/Judge-sender/issues/4#issuecomment-186542797
",https://stackoverflow.com//questions/7475223/mysql-config-not-found-when-installing-mysqldb-python-interface
linux - Use sudo with password as parameter,Want to improve this question? Update the question so it's on-topic for Stack Overflow.,"
The -S switch makes sudo read the password from STDIN. This means you can do 
echo mypassword | sudo -S command

to pass the password to sudo
However, the suggestions by others that do not involve passing the password as part of a command such as checking if the user is root are probably much better ideas for security reasons
",https://stackoverflow.com//questions/11955298/use-sudo-with-password-as-parameter
c - error: Libtool library used but 'LIBTOOL' is undefined,"I am trying to automake the OrientDb C++ library, but getting some errors.","
A good answer for me was to install libtool:
sudo apt-get install libtool

",https://stackoverflow.com//questions/18978252/error-libtool-library-used-but-libtool-is-undefined
linux - When should we use mutex and when should we use semaphore,When should we use mutex and when should we use semaphore ?,"
Here is how I remember when to use what - 
Semaphore:
Use a semaphore when you (thread) want to sleep till some other thread tells you to wake up. Semaphore 'down' happens in one thread (producer) and semaphore 'up' (for same semaphore) happens in another thread (consumer)
e.g.: In producer-consumer problem, producer wants to sleep till at least one buffer slot is empty - only the consumer thread can tell when a buffer slot is empty. 
Mutex:
Use a mutex when you (thread) want to execute code that should not be executed by any other thread at the same time. Mutex 'down' happens in one thread and mutex 'up' must happen in the same thread later on.
e.g.: If you are deleting a node from a global linked list, you do not want another thread to muck around with pointers while you are deleting the node. When you acquire a mutex and are busy deleting a node, if another thread tries to acquire the same mutex, it will be put to sleep till you release the mutex.
Spinlock:
Use a spinlock when you really want to use a mutex but your thread is not allowed to sleep. 
e.g.: An interrupt handler within OS kernel must never sleep. If it does the system will freeze / crash. If you need to insert a node to globally shared linked list from the interrupt handler, acquire a spinlock - insert node - release spinlock.
",https://stackoverflow.com//questions/4039899/when-should-we-use-mutex-and-when-should-we-use-semaphore
c - How to capture Control+D signal?,"I want to capture the Ctrl+D signal in my program and write a signal handler for it.
How can I do that?
I am working on C and using a Linux system.","
As others have already said, to handle Control+D, handle ""end of file""s.
Control+D is a piece of communication between the user and the pseudo-file that you see as stdin. It does not mean specifically ""end of file"", but more generally ""flush the input I typed so far"". Flushing means that any read() call on stdin in your program returns with the length of the input typed since the last flush. If the line is nonempty, the input becomes available to your program although the user did not type ""return"" yet. If the line is empty, then read() returns with zero, and that is interpreted as ""end of file"".
So when using Control+D to end a program, it only works at the beginning of a line, or if you do it twice (first time to flush, second time for read() to return zero).
Try it:
$ cat
foo
   (type Control-D once)
foofoo (read has returned ""foo"")
   (type Control-D again)
$

",https://stackoverflow.com//questions/1516122/how-to-capture-controld-signal
linux - How to create a hex dump of file containing only the hex characters without spaces in bash?,How do I create an unmodified hex dump of a binary file in Linux using bash? The od and hexdump commands both insert spaces in the dump and this is not ideal.,"
xxd -p file

Or if you want it all on a single line:
xxd -p file | tr -d '\n'

",https://stackoverflow.com//questions/2614764/how-to-create-a-hex-dump-of-file-containing-only-the-hex-characters-without-spac
linux - How to fix 'sudo: no tty present and no askpass program specified' error?,I am trying to compile some sources using a makefile. In the makefile there is a bunch of commands that need to be ran as sudo. ,"
Granting the user to use that command without prompting for password should resolve the problem. First open a shell console and type:
sudo visudo

Then edit that file to add to the very end:
username ALL = NOPASSWD: /fullpath/to/command, /fullpath/to/othercommand

eg
john ALL = NOPASSWD: /sbin/poweroff, /sbin/start, /sbin/stop

will allow user john to sudo poweroff, start and stop without being prompted for password.
Look at the bottom of the screen for the keystrokes you need to use in visudo - this is not vi by the way - and exit without saving at the first sign of any problem. Health warning: corrupting this file will have serious consequences, edit with care!
",https://stackoverflow.com//questions/21659637/how-to-fix-sudo-no-tty-present-and-no-askpass-program-specified-error
scp from Linux to Windows," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
This one worked for me.
scp /home/ubuntu/myfile username@IP_of_windows_machine:/C:/Users/Anshul/Desktop 

",https://stackoverflow.com//questions/10235778/scp-from-linux-to-windows
linux - Expand a possible relative path in bash,"As arguments to my script there are some file paths.  Those can, of course, be relative (or contain ~). But for the functions I've written I need paths that are absolute, but do not have their symlinks resolved.","
MY_PATH=$(readlink -f $YOUR_ARG) will resolve relative paths like ""./"" and ""../""
Consider this as well (source):
#!/bin/bash
dir_resolve()
{
cd ""$1"" 2>/dev/null || return $?  # cd to desired directory; if fail, quell any error messages but return exit status
echo ""`pwd -P`"" # output full, link-resolved path
}

# sample usage
if abs_path=""`dir_resolve \""$1\""`""
then
echo ""$1 resolves to $abs_path""
echo pwd: `pwd` # function forks subshell, so working directory outside function is not affected
else
echo ""Could not reach $1""
fi

",https://stackoverflow.com//questions/7126580/expand-a-possible-relative-path-in-bash
linux - 64 bit ntohl() in C++?,"The man pages for htonl() seem to suggest that you can only use it for up to 32 bit values. (In reality, ntohl() is defined for unsigned long, which on my platform is 32 bits. I suppose if the unsigned long were 8 bytes, it would work for 64 bit ints).","
Documentation: man htobe64 on Linux (glibc >= 2.9) or FreeBSD.
Unfortunately OpenBSD, FreeBSD and glibc (Linux) did not quite work together smoothly to create one (non-kernel-API) libc standard for this, during an attempt in 2009.
Currently, this short bit of preprocessor code:
#if defined(__linux__)
#  include <endian.h>
#elif defined(__FreeBSD__) || defined(__NetBSD__)
#  include <sys/endian.h>
#elif defined(__OpenBSD__)
#  include <sys/types.h>
#  define be16toh(x) betoh16(x)
#  define be32toh(x) betoh32(x)
#  define be64toh(x) betoh64(x)
#endif

(tested on Linux and OpenBSD) should hide the differences. It gives you the Linux/FreeBSD-style macros on those 4 platforms.
Use example:
  #include <stdint.h>    // For 'uint64_t'

  uint64_t  host_int = 123;
  uint64_t  big_endian;

  big_endian = htobe64( host_int );
  host_int = be64toh( big_endian );

It's the most ""standard C library""-ish approach available at the moment.
",https://stackoverflow.com//questions/809902/64-bit-ntohl-in-c
c++ - How do you find what version of libstdc++ library is installed on your linux machine?,I found the following command: strings /usr/lib/libstdc++.so.6 | grep GLIBC from here. It seems to work but this is an ad-hoc/heuristic method.,"
To find which library is being used you could run
 $ /sbin/ldconfig -p | grep stdc++
    libstdc++.so.6 (libc6) => /usr/lib/libstdc++.so.6

The list of compatible versions for libstdc++ version 3.4.0 and above is provided by
 $ strings /usr/lib/libstdc++.so.6 | grep LIBCXX
 GLIBCXX_3.4
 GLIBCXX_3.4.1
 GLIBCXX_3.4.2
 ...

For earlier versions the symbol GLIBCPP is defined.
The date stamp of the library is defined in a macro __GLIBCXX__ or __GLIBCPP__ depending on the version:
// libdatestamp.cxx
#include <cstdio>

int main(int argc, char* argv[]){
#ifdef __GLIBCPP__
    std::printf(""GLIBCPP: %d\n"",__GLIBCPP__);
#endif
#ifdef __GLIBCXX__
    std::printf(""GLIBCXX: %d\n"",__GLIBCXX__);
#endif
   return 0;
}

$ g++ libdatestamp.cxx -o libdatestamp
$ ./libdatestamp
GLIBCXX: 20101208

The table of datestamps of libstdc++ versions is listed in the documentation:
",https://stackoverflow.com//questions/10354636/how-do-you-find-what-version-of-libstdc-library-is-installed-on-your-linux-mac
How to find Java Heap Size and Memory Used (Linux)?,How can I check Heap Size (and Used Memory) of a Java Application on Linux through the command line?,"
Each Java process has a pid, which you first need to find with the jps command. 
Once you have the pid, you can use jstat -gc [insert-pid-here] to find statistics of the behavior of the garbage collected heap.

jstat -gccapacity [insert-pid-here] will present information about memory pool generation and space capabilities.
jstat -gcutil [insert-pid-here] will present the utilization of each generation as a percentage of its capacity. Useful to get an at a glance view of usage.

See jstat docs on Oracle's site.
",https://stackoverflow.com//questions/12797560/how-to-find-java-heap-size-and-memory-used-linux
Randomly shuffling lines in Linux / Bash,I have some files in linux. For example 2 and i need shuffling the files in one file.,"
You should use shuf command =)
cat file1 file2 | shuf

Or with Perl :
cat file1 file2 | perl -MList::Util=shuffle -wne 'print shuffle <>;'

",https://stackoverflow.com//questions/17578873/randomly-shuffling-lines-in-linux-bash
What is RSS and VSZ in Linux memory management,What are RSS and VSZ in Linux memory management?  In a multithreaded environment how can both of these can be managed and tracked?,"
RSS is the Resident Set Size and is used to show how much memory is allocated to that process and is in RAM.  It does not include memory that is swapped out.  It does include memory from shared libraries as long as the pages from those libraries are actually in memory.  It does include all stack and heap memory.
VSZ is the Virtual Memory Size.  It includes all memory that the process can access, including memory that is swapped out, memory that is allocated, but not used, and memory that is from shared libraries.
So if process A has a 500K binary and is linked to 2500K of shared libraries, has 200K of stack/heap allocations of which 100K is actually in memory (rest is swapped or unused), and it has only actually loaded 1000K of the shared libraries and 400K of its own binary then:
RSS: 400K + 1000K + 100K = 1500K
VSZ: 500K + 2500K + 200K = 3200K

Since part of the memory is shared, many processes may use it, so if you add up all of the RSS values you can easily end up with more space than your system has.
The memory that is allocated also may not be in RSS until it is actually used by the program.  So if your program allocated a bunch of memory up front, then uses it over time, you could see RSS going up and VSZ staying the same.
There is also PSS (proportional set size).  This is a newer measure which tracks the shared memory as a proportion used by the current process.  So if there were two processes using the same shared library from before:
PSS: 400K + (1000K/2) + 100K = 400K + 500K + 100K = 1000K

Threads all share the same address space, so the RSS, VSZ and PSS for each thread is identical to all of the other threads in the process.  Use ps or top to view this information in linux/unix.
There is way more to it than this, to learn more check the following references:

http://manpages.ubuntu.com/manpages/en/man1/ps.1.html
https://web.archive.org/web/20120520221529/http://emilics.com/blog/article/mconsumption.html

Also see:

A way to determine a process's ""real"" memory usage, i.e. private dirty RSS?

",https://stackoverflow.com//questions/7880784/what-is-rss-and-vsz-in-linux-memory-management
linux - Copy text from nano editor to shell," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
Nano to Shell:
1. Using mouse to mark the text.
2. Right-Click the mouse in the Shell.
Within Nano:
1. CTRL+6 (or CTRL+Shift+6 or hold Shift and move cursor) for Mark Set and mark what you want (the end could do some extra help).
2. ALT+6 for copying the marked text.
3. CTRL+u at the place you want to paste.
or
1. CTRL+6 (or CTRL+Shift+6 or hold Shift and move cursor) for Mark Set and mark what you want (the end could do some extra help).
2. CTRL+k for cutting what you want to copy
3. CTRL+u for pasting what you have just cut because you just want to copy.
4. CTRL+u at the place you want to paste.
",https://stackoverflow.com//questions/30507022/copy-text-from-nano-editor-to-shell
How to send HTML email using linux command line,"I need to send email with html format. I have only linux command line and command ""mail"".","
This worked for me:
echo ""<b>HTML Message goes here</b>"" | mail -s ""$(echo -e ""This is the subject\nContent-Type: text/html"")"" foo@example.com

",https://stackoverflow.com//questions/2591755/how-to-send-html-email-using-linux-command-line
Good Linux (Ubuntu) SVN client,"Subversion has a superb client on Windows (Tortoise, of course). Everything I've tried on Linux just - well - sucks in comparison....","
Disclaimer: A long long time ago I was one of the developers for RabbitVCS (previously known as NautilusSvn).
If you use Nautilus then you might be interested in RabbitVCS (mentioned earlier by Trevor Bramble). It's an unadulterated clone of TortoiseSVN for Nautilus written in Python. While there's still a lot of improvement to be made (especially in the area of performance) some people seem to be quite satisfied with it.
The name is quite fitting for the project, because the story it refers to quite accurately depicts the development pace (meaning long naps). If you do choose to start using RabbitVCS as your version control client, you're probably going to have to get your hands dirty.
",https://stackoverflow.com//questions/86550/good-linux-ubuntu-svn-client
linux - couldn't connect to server 127.0.0.1 shell/mongo.js,"when i setup mongodb in my ubuntu , i try : ./mongo it show this error :","

Manually remove the lockfile: sudo rm /var/lib/mongodb/mongod.lock
Run the repair script: sudo -u mongodb mongod -f /etc/mongodb.conf    --repair

Please note the following:

You must run this command as the mongodb user. If you run it as root,
then root will own files in /var/lib/mongodb/ that are necessary to
run the mongodb daemon and therefore when the daemon trys to run
later as the mongodb user, it won't have permissions to start. In
that case you'll get this error: Unable to create / open lock file
for lockfilepath: /var/lib/mongodb/mongod.lock errno:13 Permission
denied, terminating.
On Ubuntu, you must specify the configuration file /etc/mongodb.conf
using the -f flag. Otherwise it will look for the data files in the
wrong place and you will see the following error: dbpath (/data/db/)
does not exist, terminating.

",https://stackoverflow.com//questions/5726032/couldnt-connect-to-server-127-0-0-1-shell-mongo-js
linux - Run Python script at startup in Ubuntu,I have a short Python script that needs to run at startup - Ubuntu 13.10. I have tried everything I can think of but can't get it to run. The script:,"
Instructions

Copy the python file to /bin:
sudo cp -i /path/to/your_script.py /bin
Add A New Cron Job:
sudo crontab -e
Scroll to the bottom and add the following line (after all the #'s):
@reboot python /bin/your_script.py &
The “&” at the end of the line means the command is run in the background and it won’t stop the system booting up.
Test it:
sudo reboot

Practical example:

Add this file to your Desktop: test_code.py (run it to check that it works for you)
from os.path import expanduser
import datetime

file = open(expanduser(""~"") + '/Desktop/HERE.txt', 'w')
file.write(""It worked!\n"" + str(datetime.datetime.now()))
file.close()

Run the following commands:
sudo cp -i ~/Desktop/test_code.py /bin
sudo crontab -e
Add the following line and save it:
@reboot python /bin/test_code.py &
Now reboot your computer and you should find a new file on your Desktop: HERE.txt

",https://stackoverflow.com//questions/24518522/run-python-script-at-startup-in-ubuntu
linux - Read and write to binary files in C?,Does anyone have an example of code that can write to a binary file. And also code that can read a binary file and output to screen.  Looking at examples I can write to a file ok But when I try to read from a file it is not outputting correctly.,"
Reading and writing binary files is pretty much the same as any other file, the only difference is how you open it:
unsigned char buffer[10];
FILE *ptr;

ptr = fopen(""test.bin"",""rb"");  // r for read, b for binary

fread(buffer,sizeof(buffer),1,ptr); // read 10 bytes to our buffer

You said you can read it, but it's not outputting correctly... keep in mind that when you ""output"" this data, you're not reading ASCII, so it's not like printing a string to the screen:
for(int i = 0; i<10; i++)
    printf(""%u "", buffer[i]); // prints a series of bytes

Writing to a file is pretty much the same, with the exception that you're using fwrite() instead of fread():
FILE *write_ptr;

write_ptr = fopen(""test.bin"",""wb"");  // w for write, b for binary

fwrite(buffer,sizeof(buffer),1,write_ptr); // write 10 bytes from our buffer


Since we're talking Linux.. there's an easy way to do a sanity check. Install hexdump on your system (if it's not already on there) and dump your file:
mike@mike-VirtualBox:~/C$ hexdump test.bin
0000000 457f 464c 0102 0001 0000 0000 0000 0000
0000010 0001 003e 0001 0000 0000 0000 0000 0000
...

Now compare that to your output:
mike@mike-VirtualBox:~/C$ ./a.out 
127 69 76 70 2 1 1 0 0 0

hmm, maybe change the printf to a %x to make this a little clearer:
mike@mike-VirtualBox:~/C$ ./a.out 
7F 45 4C 46 2 1 1 0 0 0

Hey, look! The data matches up now*. Awesome, we must be reading the binary file correctly!
*Note the bytes are just swapped on the output but that data is correct, you can adjust for this sort of thing
",https://stackoverflow.com//questions/17598572/read-and-write-to-binary-files-in-c
How do I find all the files that were created today in Unix/Linux?,How do I find all the files that were create only today and not in 24 hour period in unix/linux,"
On my Fedora 10 system, with findutils-4.4.0-1.fc10.i386:
find <path> -daystart -ctime 0 -print

The -daystart flag tells it to calculate from the start of today instead of from 24 hours ago.
Note however that this will actually list files created or modified in the last day.  find has no options that look at the true creation date of the file.
",https://stackoverflow.com//questions/801095/how-do-i-find-all-the-files-that-were-created-today-in-unix-linux
How to send HTML email using linux command line,"I need to send email with html format. I have only linux command line and command ""mail"".","
This worked for me:
echo ""<b>HTML Message goes here</b>"" | mail -s ""$(echo -e ""This is the subject\nContent-Type: text/html"")"" foo@example.com

",https://stackoverflow.com//questions/2591755/how-to-send-html-email-using-linux-command-line
ruby on rails - cache resources exhausted Imagemagick,I'm using Imagemagick on a rails app with Minimagick and I generate some pictogram with it.,"
Find the policy.xml with find / -name ""policy.xml""
something like /etc/ImageMagick-6/policy.xml
and change
<policy domain=""resource"" name=""disk"" value=""1GiB""/>

to
<policy domain=""resource"" name=""disk"" value=""8GiB""/>

refer to convert fails due to resource limits
Memory issues
",https://stackoverflow.com//questions/31407010/cache-resources-exhausted-imagemagick
"linux - Compare integer in bash, unary operator expected",The following code gives ,"
Your problem arises from the fact that $i has a blank value when your statement fails.  Always quote your variables when performing comparisons if there is the slightest chance that one of them may be empty, e.g.:
if [ ""$i"" -ge 2 ] ; then
  ...
fi

This is because of how the shell treats variables.  Assume the original example,
if [ $i -ge 2 ] ; then ...

The first thing that the shell does when executing that particular line of code is substitute the value of $i, just like your favorite editor's search & replace function would.  So assume that $i is empty or, even more illustrative, assume that $i is a bunch of spaces!  The shell will replace $i as follows:
if [     -ge 2 ] ; then ...

Now that variable substitutions are done, the shell proceeds with the comparison and.... fails because it cannot see anything intelligible to the left of -gt.  However, quoting $i:
if [ ""$i"" -ge 2 ] ; then ...

becomes:
if [ ""    "" -ge 2 ] ; then ...

The shell now sees the double-quotes, and knows that you are actually comparing four blanks to 2 and will skip the if.
You also have the option of specifying a default value for $i if $i is blank, as follows:
if [ ""${i:-0}"" -ge 2 ] ; then ...

This will substitute the value 0 instead of $i is $i is undefined.  I still maintain the quotes because, again, if $i is a bunch of blanks then it does not count as undefined, it will not be replaced with 0, and you will run into the problem once again.
Please read this when you have the time.  The shell is treated like a black box by many, but it operates with very few and very simple rules - once you are aware of what those rules are (one of them being how variables work in the shell, as explained above) the shell will have no more secrets for you.
",https://stackoverflow.com//questions/408975/compare-integer-in-bash-unary-operator-expected
Quickly create a large file on a Linux system,How can I quickly create a large file on a Linux (Red Hat Linux) system?,"
dd from the other answers is a good solution, but it is slow for this purpose. In Linux (and other POSIX systems), we have fallocate, which uses the desired space without having to actually writing to it, works with most modern disk based file systems, very fast:
For example:
fallocate -l 10G gentoo_root.img

",https://stackoverflow.com//questions/257844/quickly-create-a-large-file-on-a-linux-system
linux - How to get memory usage at runtime using C++?,I need to get the mem usage VIRT and RES at run time of my program and display them.,"
On Linux, I've never found an ioctl() solution. For our applications, we coded a general utility routine based on reading files in /proc/pid. There are a number of these files which give differing results. Here's the one we settled on (the question was tagged C++, and we handled I/O using C++ constructs, but it should be easily adaptable to C i/o routines if you need to):
#include <unistd.h>
#include <ios>
#include <iostream>
#include <fstream>
#include <string>

//////////////////////////////////////////////////////////////////////////////
//
// process_mem_usage(double &, double &) - takes two doubles by reference,
// attempts to read the system-dependent data for a process' virtual memory
// size and resident set size, and return the results in KB.
//
// On failure, returns 0.0, 0.0

void process_mem_usage(double& vm_usage, double& resident_set)
{
   using std::ios_base;
   using std::ifstream;
   using std::string;

   vm_usage     = 0.0;
   resident_set = 0.0;

   // 'file' stat seems to give the most reliable results
   //
   ifstream stat_stream(""/proc/self/stat"",ios_base::in);

   // dummy vars for leading entries in stat that we don't care about
   //
   string pid, comm, state, ppid, pgrp, session, tty_nr;
   string tpgid, flags, minflt, cminflt, majflt, cmajflt;
   string utime, stime, cutime, cstime, priority, nice;
   string O, itrealvalue, starttime;

   // the two fields we want
   //
   unsigned long vsize;
   long rss;

   stat_stream >> pid >> comm >> state >> ppid >> pgrp >> session >> tty_nr
               >> tpgid >> flags >> minflt >> cminflt >> majflt >> cmajflt
               >> utime >> stime >> cutime >> cstime >> priority >> nice
               >> O >> itrealvalue >> starttime >> vsize >> rss; // don't care about the rest

   stat_stream.close();

   long page_size_kb = sysconf(_SC_PAGE_SIZE) / 1024; // in case x86-64 is configured to use 2MB pages
   vm_usage     = vsize / 1024.0;
   resident_set = rss * page_size_kb;
}

int main()
{
   using std::cout;
   using std::endl;

   double vm, rss;
   process_mem_usage(vm, rss);
   cout << ""VM: "" << vm << ""; RSS: "" << rss << endl;
}

",https://stackoverflow.com//questions/669438/how-to-get-memory-usage-at-runtime-using-c
bash - How to change the output color of echo in Linux,I am trying to print a text in the terminal using echo command. ,"
You can use these ANSI escape codes:
Black        0;30     Dark Gray     1;30
Red          0;31     Light Red     1;31
Green        0;32     Light Green   1;32
Brown/Orange 0;33     Yellow        1;33
Blue         0;34     Light Blue    1;34
Purple       0;35     Light Purple  1;35
Cyan         0;36     Light Cyan    1;36
Light Gray   0;37     White         1;37

And then use them like this in your script:
#    .---------- constant part!
#    vvvv vvvv-- the code from above
RED='\033[0;31m'
NC='\033[0m' # No Color
printf ""I ${RED}love${NC} Stack Overflow\n""

which prints love in red.
From @james-lim's comment, if you are using the echo command, be sure to use the -e flag to allow backslash escapes.
#    .---------- constant part!
#    vvvv vvvv-- the code from above
RED='\033[0;31m'
NC='\033[0m' # No Color
echo -e ""I ${RED}love${NC} Stack Overflow""

(don't add ""\n"" when using echo unless you want to add an additional empty line)
",https://stackoverflow.com//questions/5947742/how-to-change-the-output-color-of-echo-in-linux
linux - rsync not synchronizing .htaccess file,I am trying to rsync directory A of server1 with directory B of server2.,"
This is due to the fact that * is by default expanded to all files in the current working directory except the files whose name starts with a dot. Thus, rsync never receives these files as arguments.
You can pass . denoting current working directory to rsync:
rsync -av . server2::sharename/B

This way rsync will look for files to transfer in the current working directory as opposed to looking for them in what * expands to.
Alternatively, you can use the following command to make * expand to all files including those which start with a dot:
shopt -s dotglob

See also shopt manpage.
",https://stackoverflow.com//questions/9046749/rsync-not-synchronizing-htaccess-file
How do I get Windows to go as fast as Linux for compiling C++?,I know this is not so much a programming question but it is relevant.,"
Unless a hardcore Windows systems hacker comes along, you're not going to get more than partisan comments (which I won't do) and speculation (which is what I'm going to try).

File system - You should try the same operations (including the dir) on the same filesystem. I came across this which benchmarks a few filesystems for various parameters.
Caching. I once tried to run a compilation on Linux on a RAM disk and found that it was slower than running it on disk thanks to the way the kernel takes care of caching. This is a solid selling point for Linux and might be the reason why the performance is so different. 
Bad dependency specifications on Windows. Maybe the chromium dependency specifications for Windows are not as correct as for Linux. This might result in unnecessary compilations when you make a small change. You might be able to validate this using the same compiler toolchain on Windows.

",https://stackoverflow.com//questions/6916011/how-do-i-get-windows-to-go-as-fast-as-linux-for-compiling-c
linux - Find unique lines,"How can I find the unique lines and remove all duplicates from a file?
My input file is ","
uniq has the option you need:
   -u, --unique
          only print unique lines

$ cat file.txt
1
1
2
3
5
5
7
7
$ uniq -u file.txt
2
3

",https://stackoverflow.com//questions/13778273/find-unique-lines
"What does the ""no version information available"" error from linux dynamic linker mean?","In our product we ship some linux binaries that dynamically link to system libraries like ""libpam"".  On some customer systems we get the following error on stderr when the program runs:","
The ""no version information available"" means that the library version number is lower on the shared object.  For example, if your major.minor.patch number is 7.15.5 on the machine where you build the binary, and the major.minor.patch number is 7.12.1 on the installation machine, ld will print the warning.
You can fix this by compiling with a library (headers and shared objects) that matches the shared object version shipped with your target OS.  E.g., if you are going to install to RedHat 3.4.6-9 you don't want to compile on Debian 4.1.1-21.  This is one of the reasons that most distributions ship for specific linux distro numbers.
Otherwise, you can statically link.  However, you don't want to do this with something like PAM, so you want to actually install a development environment that matches your client's production environment (or at least install and link against the correct library versions.)
Advice you get to rename the .so files (padding them with version numbers,) stems from a time when shared object libraries did not use versioned symbols.  So don't expect that playing with the .so.n.n.n naming scheme is going to help (much - it might help if you system has been trashed.)
You last option will be compiling with a library with a different minor version number, using a custom linking script:
http://www.redhat.com/docs/manuals/enterprise/RHEL-4-Manual/gnu-linker/scripts.html
To do this, you'll need to write a custom script, and you'll need a custom installer that runs ld against your client's shared objects, using the custom script.  This requires that your client have gcc or ld on their production system.
",https://stackoverflow.com//questions/137773/what-does-the-no-version-information-available-error-from-linux-dynamic-linker
linux - How to update-alternatives to Python 3 without breaking apt?,The other day I decided that I wanted the command python to default to firing up python3 instead of python2.,"
Per Debian policy, python refers to Python 2 and python3 refers to Python 3. Don't try to change this system-wide or you are in for the sort of trouble you already discovered.
Virtual environments allow you to run an isolated Python installation with whatever version of Python and whatever libraries you need without messing with the system Python install.
With recent Python 3, venv is part of the standard library; with older versions, you might need to install python3-venv or a similar package.
$HOME~$ python --version
Python 2.7.11

$HOME~$ python3 -m venv myenv
... stuff happens ...

$HOME~$ . ./myenv/bin/activate

(myenv) $HOME~$ type python   # ""type"" is preferred over which; see POSIX
python is /home/you/myenv/bin/python

(myenv) $HOME~$ python --version
Python 3.5.1

A common practice is to have a separate environment for each project you work on, anyway; but if you want this to look like it's effectively system-wide for your own login, you could add the activation stanza to your .profile or similar.
",https://stackoverflow.com//questions/43062608/how-to-update-alternatives-to-python-3-without-breaking-apt
Is there a way to figure out what is using a Linux kernel module?,"If I load a kernel module and list the loaded modules with lsmod, I can get the ""use count"" of the module (number of other modules with a reference to the module). Is there a way to figure out what is using a module, though?","
Actually, there seems to be a way to list processes that claim a module/driver - however, I haven't seen it advertised (outside of Linux kernel documentation), so I'll jot down my notes here:
First of all, many thanks for @haggai_e's answer; the pointer to the functions try_module_get and try_module_put as those responsible for managing the use count (refcount) was the key that allowed me to track down the procedure. 
Looking further for this online, I somehow stumbled upon the post Linux-Kernel Archive: [PATCH 1/2] tracing: Reduce overhead of module tracepoints; which finally pointed to a facility present in the kernel, known as (I guess) ""tracing""; the documentation for this is in the directory Documentation/trace - Linux kernel source tree. In particular, two files explain the tracing facility, events.txt and ftrace.txt.
But, there is also a short ""tracing mini-HOWTO"" on a running Linux system in /sys/kernel/debug/tracing/README (see also I'm really really tired of people saying that there's no documentation…); note that in the kernel source tree, this file is actually generated by the file kernel/trace/trace.c. I've tested this on Ubuntu natty, and note that since /sys is owned by root, you have to use sudo to read this file, as in sudo cat or 
sudo less /sys/kernel/debug/tracing/README

... and that goes for pretty much all other operations under /sys which will be described here. 

First of all, here is a simple minimal module/driver code (which I put together from the referred resources), which simply creates a /proc/testmod-sample file node, which returns the string ""This is testmod."" when it is being read; this is testmod.c:
/*
https://github.com/spotify/linux/blob/master/samples/tracepoints/tracepoint-sample.c
https://www.linux.com/learn/linux-training/37985-the-kernel-newbie-corner-kernel-debugging-using-proc-qsequenceq-files-part-1
*/

#include <linux/module.h>
#include <linux/sched.h>
#include <linux/proc_fs.h>
#include <linux/seq_file.h> // for sequence files

struct proc_dir_entry *pentry_sample;

char *defaultOutput = ""This is testmod."";


static int my_show(struct seq_file *m, void *v)
{
  seq_printf(m, ""%s\n"", defaultOutput);
  return 0;
}

static int my_open(struct inode *inode, struct file *file)
{
  return single_open(file, my_show, NULL);
}

static const struct file_operations mark_ops = {
  .owner    = THIS_MODULE,
  .open = my_open,
  .read = seq_read,
  .llseek   = seq_lseek,
  .release  = single_release,
};


static int __init sample_init(void)
{
  printk(KERN_ALERT ""sample init\n"");
  pentry_sample = proc_create(
    ""testmod-sample"", 0444, NULL, &mark_ops);
  if (!pentry_sample)
    return -EPERM;
  return 0;
}

static void __exit sample_exit(void)
{
    printk(KERN_ALERT ""sample exit\n"");
    remove_proc_entry(""testmod-sample"", NULL);
}

module_init(sample_init);
module_exit(sample_exit);

MODULE_LICENSE(""GPL"");
MODULE_AUTHOR(""Mathieu Desnoyers et al."");
MODULE_DESCRIPTION(""based on Tracepoint sample"");

This module can be built with the following Makefile (just have it placed in the same directory as testmod.c, and then run make in that same directory):
CONFIG_MODULE_FORCE_UNLOAD=y
# for oprofile
DEBUG_INFO=y
EXTRA_CFLAGS=-g -O0

obj-m += testmod.o

# mind the tab characters needed at start here:
all:
    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules

clean:
    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean

When this module/driver is built, the output is a kernel object file, testmod.ko. 

At this point, we can prepare the event tracing related to try_module_get and try_module_put; those are in /sys/kernel/debug/tracing/events/module:
$ sudo ls /sys/kernel/debug/tracing/events/module
enable  filter  module_free  module_get  module_load  module_put  module_request

Note that on my system, tracing is by default enabled:
$ sudo cat /sys/kernel/debug/tracing/tracing_enabled
1

... however, the module tracing (specifically) is not: 
$ sudo cat /sys/kernel/debug/tracing/events/module/enable
0

Now, we should first make a filter, that will react on the module_get, module_put etc events, but only for the testmod module. To do that, we should first check the format of the event: 
$ sudo cat /sys/kernel/debug/tracing/events/module/module_put/format
name: module_put
ID: 312
format:
...
    field:__data_loc char[] name;   offset:20;  size:4; signed:1;

print fmt: ""%s call_site=%pf refcnt=%d"", __get_str(name), (void *)REC->ip, REC->refcnt

Here we can see that there is a field called name, which holds the driver name, which we can filter against. To create a filter, we simply echo the filter string into the corresponding file: 
sudo bash -c ""echo name == testmod > /sys/kernel/debug/tracing/events/module/filter""

Here, first note that since we have to call sudo, we have to wrap the whole echo redirection as an argument command of a sudo-ed bash. Second, note that since we wrote to the ""parent"" module/filter, not the specific events (which would be module/module_put/filter etc), this filter will be applied to all events listed as ""children"" of module directory. 
Finally, we enable tracing for module: 
sudo bash -c ""echo 1 > /sys/kernel/debug/tracing/events/module/enable""

From this point on, we can read the trace log file; for me, reading the blocking, 
""piped"" version of the trace file worked - like this: 
sudo cat /sys/kernel/debug/tracing/trace_pipe | tee tracelog.txt

At this point, we will not see anything in the log - so it is time to load (and utilize, and remove) the driver (in a different terminal from where trace_pipe is being read):
$ sudo insmod ./testmod.ko
$ cat /proc/testmod-sample 
This is testmod.
$ sudo rmmod testmod

If we go back to the terminal where trace_pipe is being read, we should see something like:
# tracer: nop
#
#           TASK-PID    CPU#    TIMESTAMP  FUNCTION
#              | |       |          |         |
          insmod-21137 [001] 28038.101509: module_load: testmod
          insmod-21137 [001] 28038.103904: module_put: testmod call_site=sys_init_module refcnt=2
           rmmod-21354 [000] 28080.244448: module_free: testmod

That is pretty much all we will obtain for our testmod driver - the refcount changes only when the driver is loaded (insmod) or unloaded (rmmod), not when we do a read through cat. So we can simply interrupt the read from trace_pipe with CTRL+C in that terminal; and to stop the tracing altogether:
sudo bash -c ""echo 0 > /sys/kernel/debug/tracing/tracing_enabled""

Here, note that most examples refer to reading the file /sys/kernel/debug/tracing/trace instead of trace_pipe as here. However, one problem is that this file is not meant to be ""piped"" (so you shouldn't run a tail -f on this trace file); but instead you should re-read the trace after each operation. After the first insmod, we would obtain the same output from cat-ing both trace and trace_pipe; however, after the rmmod, reading the trace file would give:
   <...>-21137 [001] 28038.101509: module_load: testmod
   <...>-21137 [001] 28038.103904: module_put: testmod call_site=sys_init_module refcnt=2
   rmmod-21354 [000] 28080.244448: module_free: testmod

... that is: at this point, the insmod had already been exited for long, and so it doesn't exist anymore in the process list - and therefore cannot be found via the recorded process ID (PID) at the time - thus we get a blank <...> as process name. Therefore, it is better to log (via tee) a running output from trace_pipe in this case. Also, note that in order to clear/reset/erase the trace file, one simply writes a 0 to it: 
sudo bash -c ""echo 0 > /sys/kernel/debug/tracing/trace""

If this seems counterintuitive, note that trace is a special file, and will always report a file size of zero anyways: 
$ sudo ls -la /sys/kernel/debug/tracing/trace
-rw-r--r-- 1 root root 0 2013-03-19 06:39 /sys/kernel/debug/tracing/trace

... even if it is ""full"". 
Finally, note that if we didn't implement a filter, we would have obtained a log of all module calls on the running system - which would log any call (also background) to grep and such, as those use the binfmt_misc module: 
...
  tr-6232  [001] 25149.815373: module_put: binfmt_misc call_site=search_binary_handler refcnt=133194
..
  grep-6231  [001] 25149.816923: module_put: binfmt_misc call_site=search_binary_handler refcnt=133196
..
  cut-6233  [000] 25149.817842: module_put: binfmt_misc call_site=search_binary_handler refcnt=129669
..
  sudo-6234  [001] 25150.289519: module_put: binfmt_misc call_site=search_binary_handler refcnt=133198
..
  tail-6235  [000] 25150.316002: module_put: binfmt_misc call_site=search_binary_handler refcnt=129671

... which adds quite a bit of overhead (in both log data ammount, and processing time required to generate it).

While looking this up, I stumbled upon Debugging Linux Kernel by Ftrace PDF, which refers to a tool trace-cmd, which pretty much does the similar as above - but through an easier command line interface. There is also a ""front-end reader"" GUI for trace-cmd called KernelShark; both of these are also in Debian/Ubuntu repositories via sudo apt-get install trace-cmd kernelshark. These tools could be an alternative to the procedure described above. 
Finally, I'd just note that, while the above testmod example doesn't really show use in context of multiple claims, I have used the same tracing procedure to discover that an USB module I'm coding, was repeatedly claimed by pulseaudio as soon as the USB device was plugged in - so the procedure seems to work for such use cases.  
",https://stackoverflow.com//questions/448999/is-there-a-way-to-figure-out-what-is-using-a-linux-kernel-module
linux - Docker can't connect to docker daemon,"After I update my Docker version to 0.8.0, I get an error message while entering sudo docker version:","
Linux
The Post-installation steps for Linux documentation reveals the following steps:

Create the docker group.
sudo groupadd docker
Add the user to the docker group.
sudo usermod -aG docker $(whoami)
Log out and log back in to ensure docker runs with correct permissions.
Start docker.
sudo service docker start

Mac OS X
As Dayel Ostraco says is necessary to add environments variables:
docker-machine start # Start virtual machine for docker
docker-machine env  # It's helps to get environment variables
eval ""$(docker-machine env default)"" # Set environment variables

The docker-machine start command outputs the comments to guide the process.
",https://stackoverflow.com//questions/21871479/docker-cant-connect-to-docker-daemon
linux - Keep the window's name fixed in tmux,"I want to keep the windows' name fixed after I rename it. But after I renaming it, they keep changing when I execute commands.","
As shown in a comment to the main post: set-option -g allow-rename off in your .tmux.conf file
",https://stackoverflow.com//questions/6041178/keep-the-windows-name-fixed-in-tmux
linux - How do I uninstall a program installed with the Appimage Launcher?," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
Since an AppImage is not ""installed"", you don't need to ""uninstall"" it. Just delete the AppImage file and the application is gone. Additionally you may want to remove menu entry by deleting the desktop file from $HOME/.local/share/applications/. 
Files and directories with names starting with a full stop (dot) (.example) are hidden - you might need to turn hidden files visible. You can probably find it somewhere in the settings of the file manager you use or in many file managers you can do that with ctrl+h.
",https://stackoverflow.com//questions/43680226/how-do-i-uninstall-a-program-installed-with-the-appimage-launcher
linux - git submodule update failed with 'fatal: detected dubious ownership in repository at',"I mounted a new hdd in my linux workstation. It looks working well. I want to download some repo in the new disk. So I execute git clone XXX, and it works well. But when I cd in the folder, and execute git submodule update --init --recursive. It failed with","
Silence all safe.directory warnings
tl;dr
Silence all warnings related to git's safe.directory system. Be sure to understand what you're doing.
git config --global --add safe.directory '*'

Long version
Adapted from this post on I cannot add the parent directory to safe.directory in Git.
I had the same issue and resolved it by disabling safe directory checks, which will end all the ""unsafe repository"" errors.
This can be done by running the following command1:
git config --global --add safe.directory '*'

Which will add the following setting to your global .gitconfig file:
[safe]
    directory = *

Before disabling, make sure you understand this security measure, and why it exists. You should not do this if your repositories are stored on a shared drive.
However, if you are the sole user of your machine 100% of the time, and your repositories are stored locally, then disabling this check should, theoretically, pose no increased risk.
Also note that you can't currently combine this with a file path, which would be relevant in my case. The command doesn't interpret the wildcard * as an operator per say– it just takes the ""*"" argument to mean ""disable safe repository checks/ consider all repositories as safe"".

1 - If this fails in your particular terminal program in Windows, try surrounding the wildcard with double quotes instead of single (Via this GitHub issue):
git config --global --add safe.directory ""*""
",https://stackoverflow.com//questions/72978485/git-submodule-update-failed-with-fatal-detected-dubious-ownership-in-repositor
linux - Difference between checkout and export in SVN,What is the exact difference between SVN checkout and SVN export?,"
svn export simply extracts all the files from a revision and does not allow revision control on it. It also does not litter each directory with .svn directories.
svn checkout allows you to use version control in the directory made, e.g. your standard commands such as svn update and svn commit.
",https://stackoverflow.com//questions/419467/difference-between-checkout-and-export-in-svn
linux - How do I SET the GOPATH environment variable on Ubuntu? What file must I edit?,I am trying to do a go get:,"
New Way:
Check out this answer.
Note: Not for trying out a go application / binaries on your host machine using go install [repo url], in such cases you still have to use the old way.
Old Way:
Just add the following lines to ~/.bashrc and this will persist. However, you can use other paths you like as GOPATH instead of $HOME/go in my sample.
export GOPATH=$HOME/go
export PATH=$PATH:$GOROOT/bin:$GOPATH/bin

",https://stackoverflow.com//questions/21001387/how-do-i-set-the-gopath-environment-variable-on-ubuntu-what-file-must-i-edit
memory - Understanding the Linux oom-killer's logs,My app was killed by the oom-killer.  It is Ubuntu 11.10 running on a live USB with no swap and the PC has 1 Gig of RAM.  The only app running (other than all the built in Ubuntu stuff) is my program flasherav.  Note that /tmp is memory mapped and at the time of the crash had about 200MB of files in it (so was taking up ~200MB of RAM).,"
Memory management in Linux is a bit tricky to understand, and I can't say I fully understand it yet, but I'll try to share a little bit of my experience and knowledge.
Short answer to your question: Yes there are other stuff included than whats in the list.
What's being shown in your list is applications run in userspace. The kernel uses memory for itself and modules, on top of that it also has a lower limit of free memory that you can't go under. When you've reached that level it will try to free up resources, and when it can't do that anymore, you end up with an OOM problem.
From the last line of your list you can read that the kernel reports a total-vm usage of: 1498536kB (1,5GB), where the total-vm includes both your physical RAM and swap space. You stated you don't have any swap but the kernel seems to think otherwise since your swap space is reported to be full (Total swap = 524284kB, Free swap = 0kB) and it reports a total vmem size of 1,5GB. 
Another thing that can complicate things further is memory fragmentation. You can hit the OOM killer when the kernel tries to allocate lets say 4096kB of continous memory, but there are no free ones availible.
Now that alone probably won't help you solve the actual problem. I don't know if it's normal for your program to require that amount of memory, but I would recommend to try a static code analyzer like cppcheck to check for memory leaks or file descriptor leaks. You could also try to run it through Valgrind to get a bit more information out about memory usage.
",https://stackoverflow.com//questions/9199731/understanding-the-linux-oom-killers-logs
linux - Total size of the contents of all the files in a directory," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
If you want the 'apparent size' (that is the number of bytes in each file), not size taken up by files on the disk, use the -b or --bytes option (if you got a Linux system with GNU coreutils):
% du -sbh <directory>

",https://stackoverflow.com//questions/1241801/total-size-of-the-contents-of-all-the-files-in-a-directory
linux - select vs poll vs epoll,"I am designing a new server which needs to support thousands of UDP connections (somewhere around 100,000 sessions). Any input or suggestions on which one to use?","
The answer is epoll if you're using Linux, kqueue if you're using FreeBSD or Mac OS X, and i/o completion ports if you're on Windows.
Some additional things you'll (almost certainly) want to research are:

Load balancing techniques
Multi-threaded networking
Database architecture
Perfect hash tables

Additionally, it is important to note that UDP does not have ""connections"" as opposed to TCP. It would also be in your best interest to start small and scale larger since debugging network-based solutions can be challenging.
",https://stackoverflow.com//questions/4039832/select-vs-poll-vs-epoll
math - How do I divide in the Linux console?,I have to variables and I want to find the value of one divided by the other. What commands should I use to do this?,"
In the bash shell, surround arithmetic expressions with $(( ... )) 
$ echo $(( 7 / 3 ))
2

Although I think you are limited to integers.
",https://stackoverflow.com//questions/1088098/how-do-i-divide-in-the-linux-console
linux - FUSE error: Transport endpoint is not connected,I'm trying to implement the FUSE filesystem.  I am receiving this error:,"
This typically is caused by the mount directory being left mounted due to a crash of your filesystem. Go to the parent directory of the mount point and enter fusermount -u YOUR_MNT_DIR.
If this doesn't do the trick, do sudo umount -l YOUR_MNT_DIR.
",https://stackoverflow.com//questions/16002539/fuse-error-transport-endpoint-is-not-connected
linux - How to recursively find and list the latest modified files in a directory with subdirectories and times,Operating system: Linux,"
Try this one:
#!/bin/bash
find $1 -type f -exec stat --format '%Y :%y %n' ""{}"" \; | sort -nr | cut -d: -f2- | head

Execute it with the path to the directory where it should start scanning recursively (it supports filenames with spaces).
If there are lots of files it may take a while before it returns anything. Performance can be improved if we use xargs instead:
#!/bin/bash
find $1 -type f -print0 | xargs -0 stat --format '%Y :%y %n' | sort -nr | cut -d: -f2- | head

which is a bit faster. 
",https://stackoverflow.com//questions/5566310/how-to-recursively-find-and-list-the-latest-modified-files-in-a-directory-with-s
"How to remove ^[, and all of the escape sequences in a file using linux shell scripting","We want to remove  ^[, and all of the escape sequences. ","
Are you looking for ansifilter? 

Two things you can do: enter the literal escape (in bash:)
Using keyboard entry:
sed 's/Ctrl-vEsc//g'
alternatively
sed 's/Ctrl-vCtrl-[//g'
Or you can use character escapes:
sed 's/\x1b//g'

or for all control characters:
sed 's/[\x01-\x1F\x7F]//g' # NOTE: zaps TAB character too!

",https://stackoverflow.com//questions/6534556/how-to-remove-and-all-of-the-escape-sequences-in-a-file-using-linux-shell-sc
java - Moving from JDK 1.7 to JDK 1.8 on Ubuntu,I am on UBUNTU. JDK version currently installed is:,"
This is what I do on debian - I suspect it should work on ubuntu (amend the version as required + adapt the folder where you want to copy the JDK files as you wish, I'm using /opt/jdk):
wget --header ""Cookie: oraclelicense=accept-securebackup-cookie"" http://download.oracle.com/otn-pub/java/jdk/8u71-b15/jdk-8u71-linux-x64.tar.gz
sudo mkdir /opt/jdk
sudo tar -zxf jdk-8u71-linux-x64.tar.gz -C /opt/jdk/
rm jdk-8u71-linux-x64.tar.gz

Then update-alternatives:
sudo update-alternatives --install /usr/bin/java java /opt/jdk/jdk1.8.0_71/bin/java 1
sudo update-alternatives --install /usr/bin/javac javac /opt/jdk/jdk1.8.0_71/bin/javac 1

Select the number corresponding to the /opt/jdk/jdk1.8.0_71/bin/java when running the following commands:
sudo update-alternatives --config java
sudo update-alternatives --config javac

Finally, verify that the correct version is selected:
java -version
javac -version

",https://stackoverflow.com//questions/30177455/moving-from-jdk-1-7-to-jdk-1-8-on-ubuntu
c - Is it possible to determine the thread holding a mutex?,"Firstly, I use pthread library to write multithreading C programs. Threads always hung by their waited mutexes. When I use the strace utility to find a thread in the FUTEX_WAIT status, I want to know which thread holds that mutex at that time. But I don't know how I could I do it. Are there any utilities that could do that?","
You can use knowledge of the mutex internals to do this.  Ordinarily this wouldn't be a very good idea, but it's fine for debugging.
Under Linux with the NPTL implementation of pthreads (which is any modern glibc), you can examine the __data.__owner member of the pthread_mutex_t structure to find out the thread that currently has it locked.  This is how to do it after attaching to the process with gdb:
(gdb) thread 2
[Switching to thread 2 (Thread 0xb6d94b90 (LWP 22026))]#0  0xb771f424 in __kernel_vsyscall ()
(gdb) bt
#0  0xb771f424 in __kernel_vsyscall ()
#1  0xb76fec99 in __lll_lock_wait () from /lib/i686/cmov/libpthread.so.0
#2  0xb76fa0c4 in _L_lock_89 () from /lib/i686/cmov/libpthread.so.0
#3  0xb76f99f2 in pthread_mutex_lock () from /lib/i686/cmov/libpthread.so.0
#4  0x080484a6 in thread (x=0x0) at mutex_owner.c:8
#5  0xb76f84c0 in start_thread () from /lib/i686/cmov/libpthread.so.0
#6  0xb767784e in clone () from /lib/i686/cmov/libc.so.6
(gdb) up 4
#4  0x080484a6 in thread (x=0x0) at mutex_owner.c:8
8               pthread_mutex_lock(&mutex);
(gdb) print mutex.__data.__owner
$1 = 22025
(gdb)

(I switch to the hung thread; do a backtrace to find the pthread_mutex_lock() it's stuck on; change stack frames to find out the name of the mutex that it's trying to lock; then print the owner of that mutex).  This tells me that the thread with LWP ID 22025 is the culprit.
You can then use thread find 22025 to find out the gdb thread number for that thread and switch to it.
",https://stackoverflow.com//questions/3483094/is-it-possible-to-determine-the-thread-holding-a-mutex
"linux - When grep ""\\"" XXFile I got ""Trailing Backslash""","Now I want to find whether there are lines containing '\' character. I tried grep ""\\"" XXFile but it hints ""Trailing Backslash"". But when I tried grep '\\' XXFile it is OK. Could anyone explain why the first case cannot run? Thanks.","
The difference is in how the shell treats the backslashes:

When you write ""\\"" in double quotes, the shell interprets the backslash escape and ends up passing the string \ to grep. Grep then sees a backslash with no following character, so it emits a ""trailing backslash"" warning. If you want to use double quotes you need to apply two levels of escaping, one for the shell and one for grep. The result: ""\\\\"".
When you write '\\' in single quotes, the shell does not do any interpretation, which means grep receives the string \\ with both backslashes intact. Grep interprets this as an escaped backslash, so it searches the file for a literal backslash character.

If that's not clear, we can use echo to see exactly what the shell is doing. echo doesn't do any backslash interpretation itself, so what it prints is what the shell passed to it.
$ echo ""\\""
\
$ echo '\\'
\\

",https://stackoverflow.com//questions/20342464/when-grep-xxfile-i-got-trailing-backslash
linux - shell-init: error retrieving current directory: getcwd -- The usual fixes do not wor,I have a simple script:,"
I believe the error is not related to the script at all. The issue is: the directory at which you are when you try to run the script does not exist anymore. for example you have two terminals, cd somedir/ at the first one then mv somedir/ somewhere_else/ at the second one, then try to run whatsoever in the first terminal - you'll receive this error message.
Please note you'll get this error even if you re-create directory with the same name because the new directory will have different inode index.
At least this was in my case.
",https://stackoverflow.com//questions/29396928/shell-init-error-retrieving-current-directory-getcwd-the-usual-fixes-do-not
c - Turn a simple socket into an SSL socket,"I wrote simple C programs, which are using sockets ('client' and 'server'). 
(UNIX/Linux usage)","
There are several steps when using OpenSSL. You must have an SSL certificate made which can contain the certificate with the private key be sure to specify the exact location of the certificate (this example has it in the root). There are a lot of good tutorials out there.

Some documentation and tools from HP (see chapter 2)
Command line for OpenSSL

Some includes:
#include <openssl/applink.c>
#include <openssl/bio.h>
#include <openssl/ssl.h>
#include <openssl/err.h>

You will need to initialize OpenSSL:
void InitializeSSL()
{
    SSL_load_error_strings();
    SSL_library_init();
    OpenSSL_add_all_algorithms();
}

void DestroySSL()
{
    ERR_free_strings();
    EVP_cleanup();
}

void ShutdownSSL()
{
    SSL_shutdown(cSSL);
    SSL_free(cSSL);
}

Now for the bulk of the functionality. You may want to add a while loop on connections.
int sockfd, newsockfd;
SSL_CTX *sslctx;
SSL *cSSL;

InitializeSSL();
sockfd = socket(AF_INET, SOCK_STREAM, 0);
if (sockfd< 0)
{
    //Log and Error
    return;
}
struct sockaddr_in saiServerAddress;
bzero((char *) &saiServerAddress, sizeof(saiServerAddress));
saiServerAddress.sin_family = AF_INET;
saiServerAddress.sin_addr.s_addr = serv_addr;
saiServerAddress.sin_port = htons(aPortNumber);

bind(sockfd, (struct sockaddr *) &serv_addr, sizeof(serv_addr));

listen(sockfd,5);
newsockfd = accept(sockfd, (struct sockaddr *) &cli_addr, &clilen);

sslctx = SSL_CTX_new( SSLv23_server_method());
SSL_CTX_set_options(sslctx, SSL_OP_SINGLE_DH_USE);
int use_cert = SSL_CTX_use_certificate_file(sslctx, ""/serverCertificate.pem"" , SSL_FILETYPE_PEM);

int use_prv = SSL_CTX_use_PrivateKey_file(sslctx, ""/serverCertificate.pem"", SSL_FILETYPE_PEM);

cSSL = SSL_new(sslctx);
SSL_set_fd(cSSL, newsockfd );
//Here is the SSL Accept portion.  Now all reads and writes must use SSL
ssl_err = SSL_accept(cSSL);
if(ssl_err <= 0)
{
    //Error occurred, log and close down ssl
    ShutdownSSL();
}

You are then able read or write using:
SSL_read(cSSL, (char *)charBuffer, nBytesToRead);
SSL_write(cSSL, ""Hi :3\n"", 6);

Update
The SSL_CTX_new should be called with the TLS method that best fits your needs in order to support the newer versions of security, instead of SSLv23_server_method().  See:
OpenSSL SSL_CTX_new description

TLS_method(), TLS_server_method(), TLS_client_method().
  These are the general-purpose version-flexible SSL/TLS methods. The actual protocol version used will be negotiated to the highest version mutually supported by the client and the server. The supported protocols are SSLv3, TLSv1, TLSv1.1, TLSv1.2 and TLSv1.3. 

",https://stackoverflow.com//questions/7698488/turn-a-simple-socket-into-an-ssl-socket
linux - PGP: Not enough random bytes available. Please do some other work to give the OS a chance to collect more entropy,Setup : Ubuntu Server on Virtual Machine with 6 cores and 3GB of RAM.,"
Run the following:
find / > /dev/null

That helped me quickly to complete my key generation.
",https://stackoverflow.com//questions/11708334/pgp-not-enough-random-bytes-available-please-do-some-other-work-to-give-the-os
linux - FUSE error: Transport endpoint is not connected,I'm trying to implement the FUSE filesystem.  I am receiving this error:,"
This typically is caused by the mount directory being left mounted due to a crash of your filesystem. Go to the parent directory of the mount point and enter fusermount -u YOUR_MNT_DIR.
If this doesn't do the trick, do sudo umount -l YOUR_MNT_DIR.
",https://stackoverflow.com//questions/16002539/fuse-error-transport-endpoint-is-not-connected
linux - How to find files modified in last x minutes (find -mmin does not work as expected),"I'm trying to find files modified in last x minutes, for example in the last hour. Many forums and tutorials on the net suggest to use the find command with the -mmin option, like this:","
I can reproduce your problem if there are no files in the directory that were modified in the last hour.  In that case, find . -mmin -60 returns nothing. The command find . -mmin -60 |xargs ls -l, however, returns every file in the directory which is consistent with what happens when ls -l is run without an argument.
To make sure that ls -l is only run when a file is found, try:
find . -mmin -60 -type f -exec ls -l {} +

",https://stackoverflow.com//questions/33407344/how-to-find-files-modified-in-last-x-minutes-find-mmin-does-not-work-as-expect
linux - Hourly rotation of files using logrotate?," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
The manpage of logrotate.conf contains an important advice for the hourly option:

Log files are rotated every hour. Note that usually logrotate is configured to be run by cron daily. You have to change this configuration and run logrotate hourly to be able to really  rotate logs hourly.

As pointed out by yellow1pl the solution is to copy the file /etc/cron.daily/logrotate into the /etc/cron.hourly/ directory. This works at least for Debian and possibly some Debian derivates.
",https://stackoverflow.com//questions/25485047/hourly-rotation-of-files-using-logrotate
linux - make -j 8 g++: internal compiler error: Killed (program cc1plus),"When I deploy Apache Mesos on Ubuntu12.04, I follow the official document, in step ""make -j 8"" I'm getting this error in the console:","
Try running (just after the failure) dmesg.
Do you see a line like this?
Out of memory: Kill process 23747 (cc1plus) score 15 or sacrifice child
Killed process 23747, UID 2243, (cc1plus) total-vm:214456kB, anon-rss:178936kB, file-rss:5908kB

Most likely that is your problem. Running make -j 8 runs lots of process which use more memory. The problem above occurs when your system runs out of memory. In this case rather than the whole system falling over, the operating systems runs a process to score each process on the system. The one that scores the highest gets killed by the operating system to free up memory. If the process that is killed is cc1plus, gcc (perhaps incorrectly) interprets this as the process crashing and hence assumes that it must be a compiler bug. But it isn't really, the problem is the OS killed cc1plus, rather than it crashed.
If this is the case, you are running out of memory. So run perhaps make -j 4 instead. This will mean fewer parallel jobs and will mean the compilation will take longer but hopefully will not exhaust your system memory.
",https://stackoverflow.com//questions/30887143/make-j-8-g-internal-compiler-error-killed-program-cc1plus
"linux - Difference between Real User ID, Effective User ID and Saved User ID",I am already aware of the real user id. It is the unique number for a user in the system.,"
The distinction between a real and an effective user id is made because you may have the need to temporarily take another user's identity (most of the time, that would be root, but it could be any user). If you only had one user id, then there would be no way of changing back to your original user id afterwards (other than taking your word for granted, and in case you are root, using root's privileges to change to any user).
So, the real user id is who you really are (the one who owns the process), and the effective user id is what the operating system looks at to make a decision whether or not you are allowed to do something (most of the time, there are some exceptions).
When you log in, the login shell sets both the real and effective user id to the same value (your real user id) as supplied by the password file.
Now, it also happens that you execute a setuid program, and besides running as another user (e.g. root) the setuid program is also supposed to do something on your behalf. How does this work?
After executing the setuid program, it will have your real id (since you're the process owner) and the effective user id of the file owner (for example root) since it is setuid.  
The program does whatever magic it needs to do with superuser privileges and then wants to do something on your behalf. That means, attempting to do something that you shouldn't be able to do should fail. How does it do that? Well, obviously by changing its effective user id to the real user id!
Now that setuid program has no way of switching back since all the kernel knows is your id and... your id. Bang, you're dead.
This is what the saved set-user id is for.
",https://stackoverflow.com//questions/32455684/difference-between-real-user-id-effective-user-id-and-saved-user-id
Best way to find os name and version in Unix/Linux platform,I need to find the OS name and version on Unix/Linux platform. For this I tried following:,"
This work fine for all Linux environment.
#!/bin/sh
cat /etc/*-release

In Ubuntu:
$ cat /etc/*-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=10.04
DISTRIB_CODENAME=lucid
DISTRIB_DESCRIPTION=""Ubuntu 10.04.4 LTS""

or 12.04:
$ cat /etc/*-release

DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=12.04
DISTRIB_CODENAME=precise
DISTRIB_DESCRIPTION=""Ubuntu 12.04.4 LTS""
NAME=""Ubuntu""
VERSION=""12.04.4 LTS, Precise Pangolin""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu precise (12.04.4 LTS)""
VERSION_ID=""12.04""

In RHEL:
$ cat /etc/*-release
Red Hat Enterprise Linux Server release 6.5 (Santiago)
Red Hat Enterprise Linux Server release 6.5 (Santiago)

Or Use this Script:
#!/bin/sh
# Detects which OS and if it is Linux then it will detect which Linux
# Distribution.

OS=`uname -s`
REV=`uname -r`
MACH=`uname -m`

GetVersionFromFile()
{
    VERSION=`cat $1 | tr ""\n"" ' ' | sed s/.*VERSION.*=\ // `
}

if [ ""${OS}"" = ""SunOS"" ] ; then
    OS=Solaris
    ARCH=`uname -p` 
    OSSTR=""${OS} ${REV}(${ARCH} `uname -v`)""
elif [ ""${OS}"" = ""AIX"" ] ; then
    OSSTR=""${OS} `oslevel` (`oslevel -r`)""
elif [ ""${OS}"" = ""Linux"" ] ; then
    KERNEL=`uname -r`
    if [ -f /etc/redhat-release ] ; then
        DIST='RedHat'
        PSUEDONAME=`cat /etc/redhat-release | sed s/.*\(// | sed s/\)//`
        REV=`cat /etc/redhat-release | sed s/.*release\ // | sed s/\ .*//`
    elif [ -f /etc/SuSE-release ] ; then
        DIST=`cat /etc/SuSE-release | tr ""\n"" ' '| sed s/VERSION.*//`
        REV=`cat /etc/SuSE-release | tr ""\n"" ' ' | sed s/.*=\ //`
    elif [ -f /etc/mandrake-release ] ; then
        DIST='Mandrake'
        PSUEDONAME=`cat /etc/mandrake-release | sed s/.*\(// | sed s/\)//`
        REV=`cat /etc/mandrake-release | sed s/.*release\ // | sed s/\ .*//`
    elif [ -f /etc/debian_version ] ; then
        DIST=""Debian `cat /etc/debian_version`""
        REV=""""

    fi
    if [ -f /etc/UnitedLinux-release ] ; then
        DIST=""${DIST}[`cat /etc/UnitedLinux-release | tr ""\n"" ' ' | sed s/VERSION.*//`]""
    fi

    OSSTR=""${OS} ${DIST} ${REV}(${PSUEDONAME} ${KERNEL} ${MACH})""

fi

echo ${OSSTR}

",https://stackoverflow.com//questions/26988262/best-way-to-find-os-name-and-version-in-unix-linux-platform
How to append one file to another in Linux from the shell?,I have two files: file1 and file2. How do I append the contents of file2 to file1 so that contents of file1 persist the process?,"
Use bash builtin redirection (tldp):
cat file2 >> file1

",https://stackoverflow.com//questions/4969641/how-to-append-one-file-to-another-in-linux-from-the-shell
linux - How to have the cp command create any necessary folders for copying a file to a destination,"When copying a file using cp to a folder that may or may not exist, how do I get cp to create the folder if necessary? Here is what I have tried:
","
To expand upon Christian's answer, the only reliable way to do this would be to combine mkdir and cp:
mkdir -p /foo/bar && cp myfile ""$_""

As an aside, when you only need to create a single directory in an existing hierarchy, rsync can do it in one operation.  I'm quite a fan of rsync as a much more versatile cp replacement, in fact:
rsync -a myfile /foo/bar/ # works if /foo exists but /foo/bar doesn't.  bar is created.

",https://stackoverflow.com//questions/947954/how-to-have-the-cp-command-create-any-necessary-folders-for-copying-a-file-to-a
linux - How to search and replace using grep,I need to recursively search for a specified string within all files and subdirectories within a directory and replace this string with another string.,"
Another option is to use find and then pass it through sed.
find /path/to/files -type f -exec sed -i 's/oldstring/new string/g' {} \;

",https://stackoverflow.com//questions/15402770/how-to-search-and-replace-using-grep
java - adb devices => no permissions (user in plugdev group; are your udev rules wrong?),I am getting following error log if I connect my android phone with Android Oreo OS to Linux PC,"
Check device vendor id and product id:
$ lsusb
Bus 001 Device 002: ID 8087:8000 Intel Corp. 
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 003 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub
Bus 002 Device 078: ID 138a:0011 Validity Sensors, Inc. VFS5011 Fingerprint Reader
Bus 002 Device 003: ID 8087:07dc Intel Corp. 
Bus 002 Device 002: ID 5986:0652 Acer, Inc 
Bus 002 Device 081: ID 22b8:2e81 Motorola PCS 
Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub

Here my android device is Motorola PCS. So my vid=22b8 and pid=2e81.  
Now create a udev rule:  
$ sudo vi /etc/udev/rules.d/51-android.rules
SUBSYSTEM==""usb"", ATTR{idVendor}==""22b8"", ATTR{idProduct}==""2e81"", MODE=""0666"", GROUP=""plugdev""

Now the device is good to be detected once udev rule is reloaded. So, let's do it:
$ sudo udevadm control --reload-rules

After this, again check if your device is detected by adb:
$ adb devices
List of devices attached
ZF6222Q9D9  device

So, you are done.
If it still doesn't work, unplug/replug the device.
If it still doesn't work, restart your OS.
",https://stackoverflow.com//questions/53887322/adb-devices-no-permissions-user-in-plugdev-group-are-your-udev-rules-wrong
linux - ssh script returns 255 error,In my code I have the following to run a remote script.,"
This is usually happens when the remote is down/unavailable; or the remote machine doesn't have ssh installed; or a firewall doesn't allow a connection to be established to the remote host.
ssh returns 255 when an error occurred or 255 is returned by the remote script:
 EXIT STATUS

     ssh exits with the exit status of the remote command or
     with 255 if an error occurred.

Usually you would an error message something similar to:
ssh: connect to host host.domain.com port 22: No route to host

Or
ssh: connect to host HOSTNAME port 22: Connection refused

Check-list:

What happens if you run the ssh command directly from the command line? 
Are you able to ping that machine?
Does the remote has ssh installed?
If installed, then is the ssh service running? 

",https://stackoverflow.com//questions/14885748/ssh-script-returns-255-error
c++ - How to Add Linux Executable Files to .gitignore?,How do you add Linux executable files to .gitignore without giving them an explicit extension and without placing them in a specific or /bin directory? Most are named the same as the C file from which they were compiled without the .c extension.,"
Can you ignore all, but source code files?
For example:
*
!*.c
!Makefile

",https://stackoverflow.com//questions/8237645/how-to-add-linux-executable-files-to-gitignore
linux - counting number of directories in a specific directory,"How to count the number of folders in a specific directory. I am using the following command, but it always provides an extra one. ","
find is also printing the directory itself:
$ find .vim/ -maxdepth 1 -type d
.vim/
.vim/indent
.vim/colors
.vim/doc
.vim/after
.vim/autoload
.vim/compiler
.vim/plugin
.vim/syntax
.vim/ftplugin
.vim/bundle
.vim/ftdetect

You can instead test the directory's children and do not descend into them at all:
$ find .vim/* -maxdepth 0 -type d
.vim/after
.vim/autoload
.vim/bundle
.vim/colors
.vim/compiler
.vim/doc
.vim/ftdetect
.vim/ftplugin
.vim/indent
.vim/plugin
.vim/syntax

$ find .vim/* -maxdepth 0 -type d | wc -l
11
$ find .vim/ -maxdepth 1 -type d | wc -l
12

You can also use ls:
$ ls -l .vim | grep -c ^d
11


$ ls -l .vim
total 52
drwxrwxr-x  3 anossovp anossovp 4096 Aug 29  2012 after
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 autoload
drwxrwxr-x 13 anossovp anossovp 4096 Aug 29  2012 bundle
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 colors
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 compiler
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 doc
-rw-rw-r--  1 anossovp anossovp   48 Aug 29  2012 filetype.vim
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 ftdetect
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 ftplugin
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 indent
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 plugin
-rw-rw-r--  1 anossovp anossovp 2505 Aug 29  2012 README.rst
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 syntax

$ ls -l .vim | grep ^d
drwxrwxr-x  3 anossovp anossovp 4096 Aug 29  2012 after
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 autoload
drwxrwxr-x 13 anossovp anossovp 4096 Aug 29  2012 bundle
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 colors
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 compiler
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 doc
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 ftdetect
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 ftplugin
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 indent
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 plugin
drwxrwxr-x  2 anossovp anossovp 4096 Aug 29  2012 syntax

",https://stackoverflow.com//questions/17648033/counting-number-of-directories-in-a-specific-directory
linux - How to find files modified in last x minutes (find -mmin does not work as expected),"I'm trying to find files modified in last x minutes, for example in the last hour. Many forums and tutorials on the net suggest to use the find command with the -mmin option, like this:","
I can reproduce your problem if there are no files in the directory that were modified in the last hour.  In that case, find . -mmin -60 returns nothing. The command find . -mmin -60 |xargs ls -l, however, returns every file in the directory which is consistent with what happens when ls -l is run without an argument.
To make sure that ls -l is only run when a file is found, try:
find . -mmin -60 -type f -exec ls -l {} +

",https://stackoverflow.com//questions/33407344/how-to-find-files-modified-in-last-x-minutes-find-mmin-does-not-work-as-expect
linux - How to write stdout to file with colors?,"A lot of times (not always) the stdout is displayed in colors. Normally I keep every output log in a different file too. Naturally in the file, the colors are not displayed anymore. ","
Since many programs will only output color sequences if their stdout is a terminal, a general solution to this problem requires tricking them into believing that the pipe they write to is a terminal. This is possible with the script command from bsdutils:
script -q -c ""vagrant up"" filename.txt

This will write the output from vagrant up to filename.txt (and the terminal). If echoing is not desirable, 
script -q -c ""vagrant up"" filename > /dev/null

will write it only to the file.
",https://stackoverflow.com//questions/27397865/how-to-write-stdout-to-file-with-colors
performance - Threads vs Processes in Linux,Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.,"
Linux uses a 1-1 threading model, with (to the kernel) no distinction between processes and threads -- everything is simply a runnable task. *
On Linux, the system call clone clones a task, with a configurable level of sharing, among which are:

CLONE_FILES: share the same file descriptor table (instead of creating a copy)
CLONE_PARENT: don't set up a parent-child relationship between the new task and the old (otherwise, child's getppid() = parent's getpid())
CLONE_VM: share the same memory space (instead of creating a COW copy)

fork() calls clone(least sharing) and pthread_create() calls clone(most sharing). **
forking costs a tiny bit more than pthread_createing because of copying tables and creating COW mappings for memory, but the Linux kernel developers have tried (and succeeded) at minimizing those costs.
Switching between tasks, if they share the same memory space and various tables, will be a tiny bit cheaper than if they aren't shared, because the data may already be loaded in cache.  However, switching tasks is still very fast even if nothing is shared -- this is something else that Linux kernel developers try to ensure (and succeed at ensuring).
In fact, if you are on a multi-processor system, not sharing may actually be beneficial to performance: if each task is running on a different processor, synchronizing shared memory is expensive.

* Simplified.  CLONE_THREAD causes signals delivery to be shared (which needs CLONE_SIGHAND, which shares the signal handler table).
** Simplified.  There exist both SYS_fork and SYS_clone syscalls, but in the kernel, the sys_fork and sys_clone are both very thin wrappers around the same do_fork function, which itself is a thin wrapper around copy_process.  Yes, the terms process, thread, and task are used rather interchangeably in the Linux kernel...
",https://stackoverflow.com//questions/807506/threads-vs-processes-in-linux
web services - HTTP POST and GET using cURL in Linux,I have a server application written in ASP.NET on Windows that provides a web service.,"
*nix provides a nice little command which makes our lives a lot easier.
GET:
with JSON:
curl -i -H ""Accept: application/json"" -H ""Content-Type: application/json"" -X GET http://hostname/resource

with XML:
curl -H ""Accept: application/xml"" -H ""Content-Type: application/xml"" -X GET http://hostname/resource

POST:
For posting data:
curl --data ""param1=value1&param2=value2"" http://hostname/resource

For file upload:
curl --form ""fileupload=@filename.txt"" http://hostname/resource

RESTful HTTP Post:
curl -X POST -d @filename http://hostname/resource

For logging into a site (auth):
curl -d ""username=admin&password=admin&submit=Login"" --dump-header headers http://localhost/Login
curl -L -b headers http://localhost/

Pretty-printing the curl results:
For JSON:
If you use npm and nodejs, you can install json package by running this command:
npm install -g json

Usage:
curl -i -H ""Accept: application/json"" -H ""Content-Type: application/json"" -X GET http://hostname/resource | json

If you use pip and python, you can install pjson package by running this command:
pip install pjson

Usage:
curl -i -H ""Accept: application/json"" -H ""Content-Type: application/json"" -X GET http://hostname/resource | pjson

If you use Python 2.6+, json tool is bundled within.
Usage:
curl -i -H ""Accept: application/json"" -H ""Content-Type: application/json"" -X GET http://hostname/resource | python -m json.tool

If you use gem and ruby, you can install colorful_json package by running this command:
gem install colorful_json

Usage:
curl -i -H ""Accept: application/json"" -H ""Content-Type: application/json"" -X GET http://hostname/resource | cjson

If you use apt-get (aptitude package manager of your Linux distro), you can install yajl-tools package by running this command:
sudo apt-get install yajl-tools

Usage:
curl -i -H ""Accept: application/json"" -H ""Content-Type: application/json"" -X GET http://hostname/resource |  json_reformat

For XML:
If you use *nix with Debian/Gnome envrionment, install libxml2-utils:
sudo apt-get install libxml2-utils

Usage:
curl -H ""Accept: application/xml"" -H ""Content-Type: application/xml"" -X GET http://hostname/resource | xmllint --format -

or install tidy:
sudo apt-get install tidy

Usage:
curl -H ""Accept: application/xml"" -H ""Content-Type: application/xml"" -X GET http://hostname/resource | tidy -xml -i -

Saving the curl response to a file
curl http://hostname/resource >> /path/to/your/file

or
curl http://hostname/resource -o /path/to/your/file

For detailed description of the curl command, hit:
man curl

For details about options/switches of the curl command, hit:
curl -h

",https://stackoverflow.com//questions/14978411/http-post-and-get-using-curl-in-linux
c - Is it possible to determine the thread holding a mutex?,"Firstly, I use pthread library to write multithreading C programs. Threads always hung by their waited mutexes. When I use the strace utility to find a thread in the FUTEX_WAIT status, I want to know which thread holds that mutex at that time. But I don't know how I could I do it. Are there any utilities that could do that?","
You can use knowledge of the mutex internals to do this.  Ordinarily this wouldn't be a very good idea, but it's fine for debugging.
Under Linux with the NPTL implementation of pthreads (which is any modern glibc), you can examine the __data.__owner member of the pthread_mutex_t structure to find out the thread that currently has it locked.  This is how to do it after attaching to the process with gdb:
(gdb) thread 2
[Switching to thread 2 (Thread 0xb6d94b90 (LWP 22026))]#0  0xb771f424 in __kernel_vsyscall ()
(gdb) bt
#0  0xb771f424 in __kernel_vsyscall ()
#1  0xb76fec99 in __lll_lock_wait () from /lib/i686/cmov/libpthread.so.0
#2  0xb76fa0c4 in _L_lock_89 () from /lib/i686/cmov/libpthread.so.0
#3  0xb76f99f2 in pthread_mutex_lock () from /lib/i686/cmov/libpthread.so.0
#4  0x080484a6 in thread (x=0x0) at mutex_owner.c:8
#5  0xb76f84c0 in start_thread () from /lib/i686/cmov/libpthread.so.0
#6  0xb767784e in clone () from /lib/i686/cmov/libc.so.6
(gdb) up 4
#4  0x080484a6 in thread (x=0x0) at mutex_owner.c:8
8               pthread_mutex_lock(&mutex);
(gdb) print mutex.__data.__owner
$1 = 22025
(gdb)

(I switch to the hung thread; do a backtrace to find the pthread_mutex_lock() it's stuck on; change stack frames to find out the name of the mutex that it's trying to lock; then print the owner of that mutex).  This tells me that the thread with LWP ID 22025 is the culprit.
You can then use thread find 22025 to find out the gdb thread number for that thread and switch to it.
",https://stackoverflow.com//questions/3483094/is-it-possible-to-determine-the-thread-holding-a-mutex
Comparing Unix/Linux IPC,"Lots of IPCs are offered by Unix/Linux: pipes, sockets, shared memory, dbus, message-queues...","
Unix IPC
Here are the big seven:

Pipe
Useful only among processes related as parent/child. Call pipe(2) and fork(2). Unidirectional.
FIFO, or named pipe
Two unrelated processes can use FIFO unlike plain pipe. Call mkfifo(3). Unidirectional.
Socket and Unix Domain Socket
Bidirectional. Meant for network communication, but can be used locally too. Can be used for different protocol. There's no message boundary for TCP. Call socket(2).
Message Queue
OS maintains discrete message. See sys/msg.h.
Signal
Signal sends an integer to another process. Doesn't mesh well with multi-threads. Call kill(2).
Semaphore
A synchronization mechanism for multi processes or threads, similar to a queue of people waiting for bathroom. See sys/sem.h.
Shared memory
Do your own concurrency control. Call shmget(2).

Message Boundary issue
One determining factor when choosing one method over the other is the message boundary issue. You may expect ""messages"" to be discrete from each other, but it's not for byte streams like TCP or Pipe.
Consider a pair of echo client and server. The client sends string, the server receives it and sends it right back. Suppose the client sends ""Hello"", ""Hello"", and ""How about an answer?"".
With byte stream protocols, the server can receive as ""Hell"", ""oHelloHow"", and "" about an answer?""; or more realistically ""HelloHelloHow about an answer?"". The server has no clue where the message boundary is.
An age old trick is to limit the message length to CHAR_MAX or UINT_MAX and agree to send the message length first in char or uint. So, if you are at the receiving side, you have to read the message length first. This also implies that only one thread should be doing the message reading at a time.
With discrete protocols like UDP or message queues, you don't have to worry about this issue, but programmatically byte streams are easier to deal with because they behave like files and stdin/out.
",https://stackoverflow.com//questions/404604/comparing-unix-linux-ipc
python - How to activate virtualenv in Linux?,I have been searching and tried various alternatives without success and spent several days on it now - driving me mad.,"
Here is my workflow after creating a folder and cd'ing into it:
$ virtualenv venv --distribute
New python executable in venv/bin/python
Installing distribute.........done.
Installing pip................done.
$ source venv/bin/activate
(venv)$ python

",https://stackoverflow.com//questions/14604699/how-to-activate-virtualenv-in-linux
node.js - MongoError: connect ECONNREFUSED 127.0.0.1:27017,"I'm using NodeJS wih MongoDB using mongodb package. When I run mongod command it works fine and gives ""waiting for connection on port 27017"". So, mongod seems to be working. But MongoClient does not work and gives error when I run node index.js command-","
This happened probably because the MongoDB service isn't started. Follow the below steps to start it:

Go to Control Panel and click on Administrative Tools.
Double click on Services. A new window opens up.
Search MongoDB.exe. Right click on it and select Start. 

The server will start. Now execute npm start again and the code might work this time.
",https://stackoverflow.com//questions/46523321/mongoerror-connect-econnrefused-127-0-0-127017
linux - How can I open some ports on Ubuntu?,"I know a little about Linux. Today I created a VPN server on my Ubuntu installation according to Set up a simple IPsec/L2TP VPN server for Ubuntu, Arch Linux and Debian.","
Ubuntu these days comes with UFW - Uncomplicated Firewall. UFW is an easy-to-use method of handling iptables rules.
Try using this command to allow a port:
sudo ufw allow 1701

To test connectivity, you could try shutting down the VPN software (freeing up the ports) and using netcat to listen, like this:
nc -l 1701

Then use telnet from your Windows host and see what shows up on your Ubuntu terminal. This can be repeated for each port you'd like to test.
",https://stackoverflow.com//questions/30251889/how-can-i-open-some-ports-on-ubuntu
linux - How to convert Windows end of line in Unix end of line (CR/LF to LF),I'm a Java developer and I'm using Ubuntu to develop. The project was created in Windows with Eclipse and it's using the Windows-1252 encoding.,"
There should be a program called dos2unix that will fix line endings for you.  If it's not already on your Linux box, it should be available via the package manager.
",https://stackoverflow.com//questions/3891076/how-to-convert-windows-end-of-line-in-unix-end-of-line-cr-lf-to-lf
c - How to capture Control+D signal?,"I want to capture the Ctrl+D signal in my program and write a signal handler for it.
How can I do that?
I am working on C and using a Linux system.","
As others have already said, to handle Control+D, handle ""end of file""s.
Control+D is a piece of communication between the user and the pseudo-file that you see as stdin. It does not mean specifically ""end of file"", but more generally ""flush the input I typed so far"". Flushing means that any read() call on stdin in your program returns with the length of the input typed since the last flush. If the line is nonempty, the input becomes available to your program although the user did not type ""return"" yet. If the line is empty, then read() returns with zero, and that is interpreted as ""end of file"".
So when using Control+D to end a program, it only works at the beginning of a line, or if you do it twice (first time to flush, second time for read() to return zero).
Try it:
$ cat
foo
   (type Control-D once)
foofoo (read has returned ""foo"")
   (type Control-D again)
$

",https://stackoverflow.com//questions/1516122/how-to-capture-controld-signal
python - What is different between makedirs and mkdir of os?,I am confused to use about these two osmethods to create the new directory.,"
makedirs() creates all the intermediate directories if they don't exist (just like mkdir -p in bash).
mkdir() can create a single sub-directory, and will throw an exception if intermediate directories that don't exist are specified.
Either can be used to create a single 'leaf' directory (dirA):

os.mkdir('dirA')
os.makedirs('dirA')

But makedirs must be used to create 'branches':

os.makedirs('dirA/dirB') will work [the entire structure is created]

mkdir can work here if dirA already exists, but if it doesn't an error will be thrown.
Note that unlike mkdir -p in bash, either will fail if the leaf already exists.
",https://stackoverflow.com//questions/13819496/what-is-different-between-makedirs-and-mkdir-of-os
linux - Why doesn't a shell get variables exported by a script run in a subshell?,I have two scripts 1.sh and 2.sh.,"
If you are executing your files like sh 1.sh or ./1.sh Then you are executing it in a sub-shell. 
If you want the changes to be made in your current shell, you could do:
. 1.sh
# OR
source 1.sh

Please consider going through the reference-documentation.
""When a script is run using source [or .] it runs within the existing shell, any variables created or modified by the script will remain available after the script completes. In contrast if the script is run just as filename, then a separate subshell (with a completely separate set of variables) would be spawned to run the script.""
",https://stackoverflow.com//questions/10781824/why-doesnt-a-shell-get-variables-exported-by-a-script-run-in-a-subshell
ios - Starting iPhone app development in Linux?," We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.","
To provide a differing response, I'm running OS X and Xcode on a virtualised (VMware) machine on Linux.  CPU is a Core2Quad (Q8800), and it is perfectly fast.  I found a prebuilt VM online (I'll leave it to you to find)
Xcode/iPhone development works perfectly, as does debugging via USB to the phone itself.
It actually surprised me a lot - but I've had no issues at all.
",https://stackoverflow.com//questions/276907/starting-iphone-app-development-in-linux
linux - Cannot connect to the Docker daemon at unix:/var/run/docker.sock. Is the docker daemon running?,I have applied every solution available on internet but still I cannot run Docker.,"
You can try out this:
systemctl start docker

It worked fine for me.
P.S.: after if there is commands that you can't do without sudo, try this:
gpasswd -a $USER docker

",https://stackoverflow.com//questions/44678725/cannot-connect-to-the-docker-daemon-at-unix-var-run-docker-sock-is-the-docker
linux - How to check if X server is running?,Is there any way to find out if the current session user is running an Xserver (under Linux) ?,"
I often need to run an X command on a server that is running many X servers, so the ps based answers do not work. Naturally, $DISPLAY has to be set appropriately. To check that that is valid, use xset q in some fragment like:
if ! xset q &>/dev/null; then
    echo ""No X server at \$DISPLAY [$DISPLAY]"" >&2
    exit 1
fi

EDIT
Some people find that xset can pause for a annoying amount of time before deciding that $DISPLAY is not pointing at a valid X server (often when tcp/ip is the transport). The fix of course is to use timeout to keep the pause amenable, 1 second say.
if ! timeout 1s xset q &>/dev/null; then
    ⋮

",https://stackoverflow.com//questions/637005/how-to-check-if-x-server-is-running
linux - Recursively look for files with a specific extension,I'm trying to find all files with a specific extension in a directory and its subdirectories with my bash (Latest Ubuntu LTS Release).,"
find ""$directory"" -type f -name ""*.in""

is a bit shorter than that whole thing (and safer - deals with whitespace in filenames and directory names).
Your script is probably failing for entries that don't have a . in their name, making $extension empty.
",https://stackoverflow.com//questions/5927369/recursively-look-for-files-with-a-specific-extension
gzip - Extract and delete all .gz in a directory- Linux,I have a directory. It has about 500K .gz files. ,"
This should do it:
gunzip *.gz

",https://stackoverflow.com//questions/16038087/extract-and-delete-all-gz-in-a-directory-linux
linux - How to run a process with a timeout in Bash?,"Possible Duplicate:
Bash script that kills a child process after a given timeout ","
Use the timeout command:
timeout 15s command

Note: on some systems you need to install coreutils, on others it's missing or has different command line arguments. See an alternate solution posted by @ArjunShankar . Based on it you can encapsulate that boiler-plate code and create your own portable timeout script or small C app that does the same thing.
",https://stackoverflow.com//questions/10224939/how-to-run-a-process-with-a-timeout-in-bash
linux - How set multiple env variables for a bash command,"I am supposed to set the EC2_HOME and JAVA_HOME variables
before running a command (ec2-describe-regions)","
You can one-time set vars for a single command by putting them on the command line before the command:
$ EC2_HOME=/path/to/dir JAVA_HOME=/other/path ec2-describe-regions

Alternately, you can export them in the environment, in which case they'll be set for all future commands:
$ export EC2_HOME=/path/to/dir
$ export JAVA_HOME=/other/path
$ ec2-describe-regions

",https://stackoverflow.com//questions/26189662/how-set-multiple-env-variables-for-a-bash-command
Can WPF applications be run in Linux or Mac with .Net Core 3?,Microsoft announced .NET Core 3 comes with WPF and Windows Forms. So can I create a desktop application for Linux or Mac using .NET Core 3?,"
No, they have clearly stated that these are windows only. In one of the .NET Core 3.0 discussions, they have also clarified that they do not intend to make these features cross-platform in the future since the whole concept is derived from windows specific features. They talked about thinking of a whole new idea for cross-platform applications, which is not easy. 
Source: https://youtu.be/HNLZQeu05BY
Update
The newly announced .NET 5 now aims in avoiding all this confusion by no longer calling it "".NET Core"".
Update 2
With blazor client-side (releases on may, 2020), there is a new experimental project for cross-platform apps using webview that is in the works. 
Source:
https://blog.stevensanderson.com/2019/11/01/exploring-lighter-alternatives-to-electron-for-hosting-a-blazor-desktop-app/
",https://stackoverflow.com//questions/53954047/can-wpf-applications-be-run-in-linux-or-mac-with-net-core-3
linux - Terminal Multiplexer for Microsoft Windows - Installers for GNU Screen or tmux," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
Look.  This is way old, but on the off chance that someone from Google finds this, absolutely the best solution to this - (and it is AWESOME) - is to use ConEmu (or a package that includes and is built on top of ConEmu called cmder) and then either use plink or putty itself to connect to a specific machine, or, even better, set up a development environment as a local VM using Vagrant.  
This is the only way I can ever see myself developing from a Windows box again.
I am confident enough to say that every other answer - while not necessarily bad answers - offer garbage solutions compared to this.
Update:  As Of 1/8/2020 not all other solutions are garbage - Windows Terminal is getting there and WSL exists.
",https://stackoverflow.com//questions/5473384/terminal-multiplexer-for-microsoft-windows-installers-for-gnu-screen-or-tmux
linux - How to resume interrupted download automatically in curl?,"I'm working with curl in Linux. I'm downloading a part of a file in ftp server (using the -r option), but my connection is not good, it always interrupts. I want to write a script which resume download when I'm connected again.","
curl -L -O your_url

This will download the file.
Now let's say your connection is interrupted;
curl -L -O -C - your_url

This will continue downloading from the last byte downloaded
From the manpage:

Use ""-C -"" to tell curl to automatically find out where/how to resume the transfer. It then uses the given output/input files to figure that out.

",https://stackoverflow.com//questions/19728930/how-to-resume-interrupted-download-automatically-in-curl
linux - tar: add all files and directories in current directory INCLUDING .svn and so on,I try to tar.gz a directory and use,"
Don't create the tar file in the directory you are packing up:
tar -czf /tmp/workspace.tar.gz .

does the trick, except it will extract the files all over the current directory when you unpack.  Better to do:
cd ..
tar -czf workspace.tar.gz workspace

or, if you don't know the name of the directory you were in:
base=$(basename $PWD)
cd ..
tar -czf $base.tar.gz $base

(This assumes that you didn't follow symlinks to get to where you are and that the shell doesn't try to second guess you by jumping backwards through a symlink - bash is not trustworthy in this respect.  If you have to worry about that, use cd -P .. to do a physical change directory.  Stupid that it is not the default behaviour in my view - confusing, at least, for those for whom cd .. never had any alternative meaning.)

One comment in the discussion says:

I [...] need to exclude the top directory and I [...] need to place the tar in the base directory.

The first part of the comment does not make much sense - if the tar file contains the current directory, it won't be created when you extract file from that archive because, by definition, the current directory already exists (except in very weird circumstances).
The second part of the comment can be dealt with in one of two ways:

Either: create the file somewhere else - /tmp is one possible location - and then move it back to the original location after it is complete.
Or: if you are using GNU Tar, use the --exclude=workspace.tar.gz option.  The string after the = is a pattern - the example is the simplest pattern - an exact match.  You might need to specify --exclude=./workspace.tar.gz if you are working in the current directory contrary to recommendations; you might need to specify --exclude=workspace/workspace.tar.gz if you are working up one level as suggested.  If you have multiple tar files to exclude, use '*', as in --exclude=./*.gz.

",https://stackoverflow.com//questions/3651791/tar-add-all-files-and-directories-in-current-directory-including-svn-and-so-on
linux - How to check if X server is running?,Is there any way to find out if the current session user is running an Xserver (under Linux) ?,"
I often need to run an X command on a server that is running many X servers, so the ps based answers do not work. Naturally, $DISPLAY has to be set appropriately. To check that that is valid, use xset q in some fragment like:
if ! xset q &>/dev/null; then
    echo ""No X server at \$DISPLAY [$DISPLAY]"" >&2
    exit 1
fi

EDIT
Some people find that xset can pause for a annoying amount of time before deciding that $DISPLAY is not pointing at a valid X server (often when tcp/ip is the transport). The fix of course is to use timeout to keep the pause amenable, 1 second say.
if ! timeout 1s xset q &>/dev/null; then
    ⋮

",https://stackoverflow.com//questions/637005/how-to-check-if-x-server-is-running
"linux - what does ""bash:no job control in this shell” mean?","I think it's related to the parent process creating new subprocess and does not have tty. Can anyone explain the detail under the hood? i.e. the related working model of bash, process creation, etc?","
You may need to enable job control:
#! /bin/bash   
set -m

",https://stackoverflow.com//questions/11821378/what-does-bashno-job-control-in-this-shell-mean
user interface - Graphical DIFF programs for linux,"I really like Araxis Merge for a graphical DIFF program for the PC. I have no idea what's available for linux, though.  We're running SUSE linux on our z800 mainframe.
I'd be most grateful if I could get a few pointers to what programs everyone else likes.","
I know of two graphical diff programs: Meld and KDiff3. I haven't used KDiff3, but Meld works well for me.
It seems that both are in the standard package repositories for openSUSE 11.0
",https://stackoverflow.com//questions/112932/graphical-diff-programs-for-linux
linux - Curl Command to Repeat URL Request,"Whats the syntax for a linux command that hits a URL repeatedly, x number of times. I don't need to do anything with the data, I just need to replicate hitting refresh 20 times in a browser.","
You could use URL sequence substitution with a dummy query string (if you want to use CURL and save a few keystrokes):
curl http://www.myurl.com/?[1-20]

If you have other query strings in your URL, assign the sequence to a throwaway variable:
curl http://www.myurl.com/?myVar=111&fakeVar=[1-20]

Check out the URL section on the man page: https://curl.haxx.se/docs/manpage.html
",https://stackoverflow.com//questions/12409519/curl-command-to-repeat-url-request
Replace whitespaces with tabs in linux,How do I replace whitespaces with tabs in linux in a given text file?,"
Use the unexpand(1) program

UNEXPAND(1)                      User Commands                     UNEXPAND(1)

NAME
       unexpand - convert spaces to tabs

SYNOPSIS
       unexpand [OPTION]... [FILE]...

DESCRIPTION
       Convert  blanks in each FILE to tabs, writing to standard output.  With
       no FILE, or when FILE is -, read standard input.

       Mandatory arguments to long options are  mandatory  for  short  options
       too.

       -a, --all
              convert all blanks, instead of just initial blanks

       --first-only
              convert only leading sequences of blanks (overrides -a)

       -t, --tabs=N
              have tabs N characters apart instead of 8 (enables -a)

       -t, --tabs=LIST
              use comma separated LIST of tab positions (enables -a)

       --help display this help and exit

       --version
              output version information and exit
. . .
STANDARDS
       The expand and unexpand utilities conform to IEEE Std 1003.1-2001
       (``POSIX.1'').

",https://stackoverflow.com//questions/1424126/replace-whitespaces-with-tabs-in-linux
linux - PGP: Not enough random bytes available. Please do some other work to give the OS a chance to collect more entropy,Setup : Ubuntu Server on Virtual Machine with 6 cores and 3GB of RAM.,"
Run the following:
find / > /dev/null

That helped me quickly to complete my key generation.
",https://stackoverflow.com//questions/11708334/pgp-not-enough-random-bytes-available-please-do-some-other-work-to-give-the-os
linux - ssh script returns 255 error,In my code I have the following to run a remote script.,"
This is usually happens when the remote is down/unavailable; or the remote machine doesn't have ssh installed; or a firewall doesn't allow a connection to be established to the remote host.
ssh returns 255 when an error occurred or 255 is returned by the remote script:
 EXIT STATUS

     ssh exits with the exit status of the remote command or
     with 255 if an error occurred.

Usually you would an error message something similar to:
ssh: connect to host host.domain.com port 22: No route to host

Or
ssh: connect to host HOSTNAME port 22: Connection refused

Check-list:

What happens if you run the ssh command directly from the command line? 
Are you able to ping that machine?
Does the remote has ssh installed?
If installed, then is the ssh service running? 

",https://stackoverflow.com//questions/14885748/ssh-script-returns-255-error
linux - Can you attach Amazon EBS to multiple instances?,"We currently  use multiple webservers accessing one mysql server and fileserver.  Looking at moving to the cloud, can I use this same setup and attach the EBS to multiple machine instances or what's another solution? ","
UPDATE (April 2015): For this use-case, you should start looking at the new Amazon Elastic File System (EFS), which is designed to be multiply attached in exactly the way you are wanting.  The key difference between EFS and EBS is that they provide different abstractions: EFS exposes the NFSv4 protocol, whereas EBS provides raw block IO access.  
Below you'll find my original explanation as to why it's not possible to safely mount a raw block device on multiple machines.

ORIGINAL POST (2011):
Even if you were able to get an EBS volume attached to more than one instance, it would be a _REALLY_BAD_IDEA_.  To quote Kekoa, ""this is like using a hard drive in two computers at once""
Why is this a bad idea?  ...
The reason you can't attach a volume to more than one instance is that EBS provides a ""block storage"" abstraction upon which customers run a filesystem like ext2/ext3/etc.  Most of these filesystems (eg, ext2/3, FAT, NTFS, etc) are written assuming they have exclusive access to the block device.  Two instances accessing the same filesystem would almost certainly end in tears and data corruption.
In other words, double mounting an EBS volume would only work if you were running a cluster filesystem that is designed to share a block device between multiple machines.  Furthermore, even this wouldn't be enough.  EBS would need to be tested for this scenario and to ensure that it provides the same consistency guarantees as other shared block device solutions ... ie, that blocks aren't cached at intermediate non-shared levels like the Dom0 kernel, Xen layer, and DomU kernel.  And then there's the performance considerations of synchronizing blocks between multiple clients - most of the clustered filesystems are designed to work on high speed dedicated SANs, not a best-effort commodity ethernet.  It sounds so simple, but what you are asking for is a very nontrivial thing.
Alternatively, see if your data sharing scenario can be NFS, SMB/CIFS, SimpleDB, or S3.  These solutions all use higher layer protocols that are intended to share files without having a shared block device subsystem.  Many times such a solution is actually more efficient.
In your case, you can still have a single MySql instance / fileserver that is accessed by multiple web front-ends.  That fileserver could then store it's data on an EBS volume, allowing you to take nightly snapshot backups.  If the instance running the fileserver is lost, you can detach the EBS volume and reattach it to a new fileserver instance and be back up and running in minutes.
""Is there anything like S3 as a filesystem?"" - yes and no.  Yes, there are 3rd party solutions like s3fs that work ""ok"", but under the hood they still have to make relatively expensive web service calls for each read / write.  For a shared tools dir, works great.  For the kind of clustered FS usage you see in the HPC world, not a chance.  To do better, you'd need a new service that provides a binary connection-oriented protocol, like NFS.  Offering such a multi-mounted filesystem with reasonable performance and behavior would be a GREAT feature add-on for EC2.  I've long been an advocate for Amazon to build something like that. 
",https://stackoverflow.com//questions/841240/can-you-attach-amazon-ebs-to-multiple-instances
"linux - ""/usr/bin/ld: cannot find -lz""","I am trying to compile Android source code under Ubuntu 10.04. I get an error saying,","
I had the exact same error, and like you, installing zlib1g-dev did not fix it.  Installing lib32z1-dev got me past it. I have a 64 bit system and it seems like it wanted the 32 bit library.
",https://stackoverflow.com//questions/3373995/usr-bin-ld-cannot-find-lz
linux - What are stalled-cycles-frontend and stalled-cycles-backend in 'perf stat' result?,Does anybody know what is the meaning of stalled-cycles-frontend and stalled-cycles-backend in perf stat result ? I searched on the internet but did not find the answer. Thanks,"
The theory:
Let's start from this: nowaday's CPU's are superscalar, which means that they can execute more than one instruction per cycle (IPC). Latest Intel architectures can go up to 4 IPC (4 x86 instruction decoders). Let's not bring macro / micro fusion into discussion to complicate things more :).
Typically, workloads do not reach IPC=4 due to various resource contentions. This means that the CPU is wasting cycles (number of instructions is given by the software and the CPU has to execute them in as few cycles as possible).
We can divide the total cycles being spent by the CPU in 3 categories:

Cycles where instructions get retired (useful work)
Cycles being spent in the Back-End (wasted)
Cycles spent in the Front-End (wasted).

To get an IPC of 4, the number of cycles retiring has to be close to the total number of cycles. Keep in mind that in this stage, all the micro-operations (uOps) retire from the pipeline and commit their results into registers / caches. At this stage you can have even more than 4 uOps retiring, because this number is given by the number of execution ports. If you have only 25% of the cycles retiring 4 uOps then you will have an overall IPC of 1.
The cycles stalled in the back-end are a waste because the CPU has to wait for resources (usually memory) or to finish long latency instructions (e.g. transcedentals - sqrt, reciprocals, divisions, etc.).
The cycles stalled in the front-end are a waste because that means that the Front-End does not feed the Back End with micro-operations. This can mean that you have misses in the Instruction cache, or complex instructions that are not already decoded in the micro-op cache. Just-in-time compiled code usually expresses this behavior.
Another stall reason is branch prediction miss. That is called bad speculation. In that case uOps are issued but they are discarded because the BP predicted wrong.
The implementation in profilers:
How do you interpret the BE and FE stalled cycles?
Different profilers have different approaches on these metrics. In vTune, categories 1 to 3 add up to give 100% of the cycles. That seams reasonable because either you have your CPU stalled (no uOps are retiring) either it performs usefull work (uOps) retiring. See more here: https://software.intel.com/sites/products/documentation/doclib/stdxe/2013SP1/amplifierxe/snb/index.htm
In perf this usually does not happen. That's a problem because when you see 125% cycles stalled in the front end, you don't know how to really interpret this. You could link the >1 metric with the fact that there are 4 decoders but if you continue the reasoning, then the IPC won't match.
Even better, you don't know how big the problem is. 125% out of what? What do the #cycles mean then?
I personally look a bit suspicious on perf's BE and FE stalled cycles and hope this will get fixed.
Probably we will get the final answer by debugging the code from here: http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/tools/perf/builtin-stat.c
",https://stackoverflow.com//questions/22165299/what-are-stalled-cycles-frontend-and-stalled-cycles-backend-in-perf-stat-resul
"I get ""dquote>"" as a result of executing a program in linux shell","When I execute a script in a Linux shell, I get this output:","
It means you've executed a line of code with only one double-quote character, like this:
echo ""Hello

The shell is waiting for the other quote.
",https://stackoverflow.com//questions/15773278/i-get-dquote-as-a-result-of-executing-a-program-in-linux-shell
How to set the environmental variable LD_LIBRARY_PATH in linux,I have first executed the command: export LD_LIBRARY_PATH=/usr/local/lib,"
You should add more details about your distribution, for example under Ubuntu the right way to do this is to add a custom .conf file to /etc/ld.so.conf.d, for example
sudo gedit /etc/ld.so.conf.d/randomLibs.conf

inside the file you are supposed to write the complete path to the directory that contains all the libraries that you wish to add to the system, for example
/home/linux/myLocalLibs

remember to add only the path to the dir, not the full path for the file, all the libs inside that path will be automatically indexed.
Save and run sudo ldconfig to update the system with this libs.
",https://stackoverflow.com//questions/13428910/how-to-set-the-environmental-variable-ld-library-path-in-linux
linux - Difference between checkout and export in SVN,What is the exact difference between SVN checkout and SVN export?,"
svn export simply extracts all the files from a revision and does not allow revision control on it. It also does not litter each directory with .svn directories.
svn checkout allows you to use version control in the directory made, e.g. your standard commands such as svn update and svn commit.
",https://stackoverflow.com//questions/419467/difference-between-checkout-and-export-in-svn
c - Whole one core dedicated to single process,Is there any way in Linux to assign one CPU core to a particular given process and there should not be any other processes or interrupt handlers to be scheduled on this core?,"
Yes there is. In fact, there are two separate ways to do it :-)
Right now, the best way to accomplish what you want is to do the following:

Add the parameter isolcpus=[cpu_number] to the Linux kernel command line from the boot loader during boot. This will instruct the Linux scheduler not to run any regular tasks on that CPU unless specifically requested using cpu affinity.
Use IRQ affinity to set other CPUs to handle all interrupts so that your isolated CPU will not receive any interrupts.
Use CPU affinity to fix your specific task to the isolated CPU.

This will give you the best that Linux can provide with regard to CPU isolation without out-of-tree and in-development patches.
Your task will still get interrupted from time to time by Linux code, including other tasks - such as the timer tick interrupt and the scheduler code, IPIs from other CPUs and stuff like work queue kernel threads, although the interruption should be quite minimal.
For an (almost) complete list of interruption sources, check out my page at https://github.com/gby/linux/wiki
The alternative method is to use cpusets which is way more elegant and dynamic but suffers from some weaknesses at this point in time (no migration of timers for example) which makes me recommend the old, crude but effective isolcpus parameter.
Note that work is currently being done by the Linux community to address all these issues and more to give even better isolation.
",https://stackoverflow.com//questions/13583146/whole-one-core-dedicated-to-single-process
"linux - Python subprocess.Popen ""OSError: [Errno 12] Cannot allocate memory""",Note: This question was originally asked here but the bounty time expired even though an acceptable answer was not actually found. I am re-asking this question including all details provided in the original question.,"
As a general rule (i.e. in vanilla kernels), fork/clone failures with ENOMEM occur specifically because of either an honest to God out-of-memory condition (dup_mm, dup_task_struct, alloc_pid, mpol_dup, mm_init etc. croak), or because security_vm_enough_memory_mm failed you while enforcing the overcommit policy.
Start by checking the vmsize of the process that failed to fork, at the time of the fork attempt, and then compare to the amount of free memory (physical and swap) as it relates to the overcommit policy (plug the numbers in.)
In your particular case, note that Virtuozzo has additional checks in overcommit enforcement.  Moreover, I'm not sure how much control you truly have, from within your container, over swap and overcommit configuration (in order to influence the outcome of the enforcement.)
Now, in order to actually move forward I'd say you're left with two options:

switch to a larger instance, or
put some coding effort into more effectively controlling your script's memory footprint

NOTE that the coding effort may be all for naught if it turns out that it's not you, but some other guy collocated in a different instance on the same server as you running amock.
Memory-wise, we already know that subprocess.Popen uses fork/clone under the hood, meaning that every time you call it you're requesting once more as much memory as Python is already eating up, i.e. in the hundreds of additional MB, all in order to then exec a puny 10kB executable such as free or ps.  In the case of an unfavourable overcommit policy, you'll soon see ENOMEM.
Alternatives to fork that do not have this parent page tables etc. copy problem are vfork and posix_spawn.  But if you do not feel like rewriting chunks of subprocess.Popen in terms of vfork/posix_spawn, consider using suprocess.Popen only once, at the beginning of your script (when Python's memory footprint is minimal), to spawn a shell script that then runs free/ps/sleep and whatever else in a loop parallel to your script; poll the script's output or read it synchronously, possibly from a separate thread if you have other stuff to take care of asynchronously -- do your data crunching in Python but leave the forking to the subordinate process.
HOWEVER, in your particular case you can skip invoking ps and free altogether; that information is readily available to you in Python directly from procfs, whether you choose to access it yourself or via existing libraries and/or packages.  If ps and free were the only utilities you were running, then you can do away with subprocess.Popen completely.
Finally, whatever you do as far as subprocess.Popen is concerned, if your script leaks memory you will still hit the wall eventually.  Keep an eye on it, and check for memory leaks.
",https://stackoverflow.com//questions/1367373/python-subprocess-popen-oserror-errno-12-cannot-allocate-memory
linux - Tell Composer to use Different PHP Version,"I've been stuck at this for a few days. I'm using 1and1 hosting, and they have their PHP set up a bit weird.","
Ubuntu 18.04 case ... this run for me.
/usr/bin/php7.1 /usr/local/bin/composer update

",https://stackoverflow.com//questions/32750250/tell-composer-to-use-different-php-version
linux - Identify user in a Bash script called by sudo,If I create the script /root/bin/whoami.sh containing:,"
$SUDO_USER doesn't work if you are using sudo su -.
It also requires multiple checks - if $USER == 'root' then get $SUDO_USER.
Instead of the command whoami use who am i.  This runs the who command filtered for the current session.  It gives you more info than you need.  So, do this to get just the user: 
who am i | awk '{print $1}'

Alternatively (and simpler) you can use logname.  It does the same thing as the above statement.
This gives you the username that logged in to the session.
These work regardless of sudo or sudo su [whatever].  It also works regardless of how many times su and sudo are called.
",https://stackoverflow.com//questions/3522341/identify-user-in-a-bash-script-called-by-sudo
terminal - How do you scroll up/down on the console of a Linux VM," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
SHIFT+Page Up and SHIFT+Page Down. If it doesn't work try this and then it should: 
Go the terminal program, and make sure
Edit/Profile Preferences/Scrolling/Scrollback/Unlimited
is checked.
The exact location of this option might be somewhere different though, I see that you are using Redhat.
",https://stackoverflow.com//questions/15255070/how-do-you-scroll-up-down-on-the-console-of-a-linux-vm
sql server - Error: TCP Provider: Error code 0x2746. During the Sql setup in linux through terminal,"I am trying to setup the ms-sql server in my linux by following the documentation 
https://learn.microsoft.com/pl-pl/sql/linux/quickstart-install-connect-ubuntu?view=sql-server-2017","
[UPDATE 17.03.2020: Microsoft has released SQL Server 2019 CU3 with an Ubuntu 18.04 repository. See: https://techcommunity.microsoft.com/t5/sql-server/sql-server-2019-now-available-on-ubuntu-18-04-supported-on-sles/ba-p/1232210 . I hope this is now fully compatible without any ssl problems. Haven't tested it jet.]
Reverting to 14.0.3192.2-2 helps.
But it's possible to solve the problem also using the method indicated by Ola774, not only in case of upgrade from Ubuntu 16.04 to 18.04, but on every installation of SQL Server 2017 on Ubuntu 18.04. 
It seems that Microsoft now in cu16 messed up with their own patch for the ssl-version problems applied in cu10 (https://techcommunity.microsoft.com/t5/SQL-Server/Installing-SQL-Server-2017-for-Linux-on-Ubuntu-18-04-LTS/ba-p/385983). But linking the ssl 1.0.0 libraries works.
So just do the following:

Stop SQL Server 
sudo systemctl stop mssql-server 

Open the editor for the service configuration by
sudo systemctl edit mssql-server 


This will create an override for the original service config. It's correct that the override-file, or, more exactly ""drop-in-file"", is empty when used the first time.

In the editor, add the following lines to the file and save it: 
[Service]
Environment=""LD_LIBRARY_PATH=/opt/mssql/lib"" 

Create symbolic links to OpenSSL 1.0 for SQL Server to use: 
sudo ln -s /usr/lib/x86_64-linux-gnu/libssl.so.1.0.0 /opt/mssql/lib/libssl.so 
sudo ln -s /usr/lib/x86_64-linux-gnu/libcrypto.so.1.0.0 /opt/mssql/lib/libcrypto.so 

Start SQL Server 
sudo systemctl start mssql-server 


",https://stackoverflow.com//questions/57265913/error-tcp-provider-error-code-0x2746-during-the-sql-setup-in-linux-through-te
linux - How to resume interrupted download automatically in curl?,"I'm working with curl in Linux. I'm downloading a part of a file in ftp server (using the -r option), but my connection is not good, it always interrupts. I want to write a script which resume download when I'm connected again.","
curl -L -O your_url

This will download the file.
Now let's say your connection is interrupted;
curl -L -O -C - your_url

This will continue downloading from the last byte downloaded
From the manpage:

Use ""-C -"" to tell curl to automatically find out where/how to resume the transfer. It then uses the given output/input files to figure that out.

",https://stackoverflow.com//questions/19728930/how-to-resume-interrupted-download-automatically-in-curl
c++ - How to Add Linux Executable Files to .gitignore?,How do you add Linux executable files to .gitignore without giving them an explicit extension and without placing them in a specific or /bin directory? Most are named the same as the C file from which they were compiled without the .c extension.,"
Can you ignore all, but source code files?
For example:
*
!*.c
!Makefile

",https://stackoverflow.com//questions/8237645/how-to-add-linux-executable-files-to-gitignore
linux - List of Java processes,"How can I list all Java processes in bash?
I need an command line. I know there is command ps but I don't know what parameters I need to use.","
try:
ps aux | grep java

and see how you get on
",https://stackoverflow.com//questions/6283167/list-of-java-processes
"linux - ""Couldn't find a file descriptor referring to the console"" on Ubuntu bash on Windows","I have a problem with Bash on Ubuntu on Windows. If I type ""open (filename)"" on Mac terminal, it opens the file with the right program but if I try to use it on Windows bash, it says: ""Couldn't find a file descriptor referring to the console"".","
Instead of open u can use xdg-open which does the same thing, independently of application i.e. pdf, image, etc. It will open a new virtual terminal (I have tried this on Linux)
Example:
xdg-open ~/Pictures/Wallpapers/myPic.jpg
xdg-open ~/Docs/holidays.pdf
",https://stackoverflow.com//questions/42463929/couldnt-find-a-file-descriptor-referring-to-the-console-on-ubuntu-bash-on-win
How do I find all the files that were created today in Unix/Linux?,How do I find all the files that were create only today and not in 24 hour period in unix/linux,"
On my Fedora 10 system, with findutils-4.4.0-1.fc10.i386:
find <path> -daystart -ctime 0 -print

The -daystart flag tells it to calculate from the start of today instead of from 24 hours ago.
Note however that this will actually list files created or modified in the last day.  find has no options that look at the true creation date of the file.
",https://stackoverflow.com//questions/801095/how-do-i-find-all-the-files-that-were-created-today-in-unix-linux
linux - Using iconv to convert from UTF-16LE to UTF-8,"Hi I am trying to convert some log files from a Microsoft SQL server, but the files are encoded using UTf-16LE and iconv does not seem to be able to convert them. ","
I forgot the -o switch!
The final command is :
iconv -f UTF-16LE -t UTF-8 <filename> -o <new-filename>

",https://stackoverflow.com//questions/17287713/using-iconv-to-convert-from-utf-16le-to-utf-8
tcp - Simulate delayed and dropped packets on Linux,I would like to simulate packet delay and loss for UDP and TCP on Linux to measure the performance of an application.  Is there a simple way to do this?,"
netem leverages functionality already built into Linux and userspace utilities to simulate networks.  This is actually what Mark's answer refers to, by a different name.
The examples on their homepage already show how you can achieve what you've asked for:

Examples
Emulating wide area network delays
This is the simplest example, it just adds a fixed amount of delay to all packets going out of the local Ethernet.
# tc qdisc add dev eth0 root netem delay 100ms

Now a simple ping test to host on the local network should show an increase of 100 milliseconds. The delay is limited by the clock resolution of the kernel (Hz). On most 2.4 systems, the system clock runs at 100 Hz which allows delays in increments of 10 ms. On 2.6, the value is a configuration parameter from 1000 to 100 Hz.
Later examples just change parameters without reloading the qdisc
Real wide area networks show variability so it is possible to add random variation.
# tc qdisc change dev eth0 root netem delay 100ms 10ms

This causes the added delay to be 100 ± 10 ms. Network delay variation isn't purely random, so to emulate that there is a correlation value as well.
# tc qdisc change dev eth0 root netem delay 100ms 10ms 25%

This causes the added delay to be 100 ± 10 ms with the next random element depending 25% on the last one. This isn't true statistical correlation, but an approximation.
Delay distribution
Typically, the delay in a network is not uniform. It is more common to use a something like a normal distribution to describe the variation in delay. The netem discipline can take a table to specify a non-uniform distribution.
# tc qdisc change dev eth0 root netem delay 100ms 20ms distribution normal

The actual tables (normal, pareto, paretonormal) are generated as part of the iproute2 compilation and placed in /usr/lib/tc; so it is possible with some effort to make your own distribution based on experimental data.
Packet loss
Random packet loss is specified in the 'tc' command in percent. The smallest possible non-zero value is:
2−32 = 0.0000000232%
# tc qdisc change dev eth0 root netem loss 0.1%

This causes 1/10th of a percent (i.e. 1 out of 1000) packets to be randomly dropped.
An optional correlation may also be added. This causes the random number generator to be less random and can be used to emulate packet burst losses.
# tc qdisc change dev eth0 root netem loss 0.3% 25%

This will cause 0.3% of packets to be lost, and each successive probability depends by a quarter on the last one.
Probn = 0.25 × Probn-1 + 0.75 × Random

Note that you should use tc qdisc add if you have no rules for that interface or tc qdisc change if you already have rules for that interface. Attempting to use tc qdisc change on an interface with no rules will give the error RTNETLINK answers: No such file or directory.
",https://stackoverflow.com//questions/614795/simulate-delayed-and-dropped-packets-on-linux
linux - How to get terminal's Character Encoding,"Now I change my gnome-terminal's character encoding to ""GBK"" (default it is UTF-8), but how can I get the value(character encoding) in my Linux?","
The terminal uses environment variables to determine which character set to use, therefore you can determine it by looking at those variables:
echo $LC_CTYPE

or
echo $LANG

",https://stackoverflow.com//questions/5306153/how-to-get-terminals-character-encoding
linux - Docker not running on Ubuntu WSL due to error cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
Using WSL2
You simply have to activate and use WSL2, I have to install Ubuntu 20.04 as the 18.04 wasn't connecting with Docker desktop. In the windows shell:
To check the WSL mode, run
wsl -l -v
To upgrade your existing Linux distro to v2, run:
wsl --set-version (distro name) 2
WSL Integration will be enabled on your default WSL distribution. To change your default WSL distro, run
wsl --set-default <distro name>
Then in docker you have to.
...use the WSL2 engine

...access from your default WSL2

Based on this article.
A Linux Dev Environment on Windows with WSL 2, Docker Desktop
And the docker docs.
Docker Desktop WSL 2 backend
Below is valid only for WSL1
It seems that docker cannot run inside WSL. What they propose is to connect the WSL to your docker desktop running in windows: Setting Up Docker for Windows and WSL
In the docker forums they also refer to that solution: Cannot connect to the docker daemon
",https://stackoverflow.com//questions/61592709/docker-not-running-on-ubuntu-wsl-due-to-error-cannot-connect-to-the-docker-daemo
linux - docker networking namespace not visible in ip netns list,When I create a new docker container like with ,"
That's because docker is not creating the reqired symlink:
# (as root)
pid=$(docker inspect -f '{{.State.Pid}}' ${container_id})
mkdir -p /var/run/netns/
ln -sfT /proc/$pid/ns/net /var/run/netns/$container_id

Then, the container's netns namespace can be examined with ip netns ${container_id}, e.g.:
# e.g. show stats about eth0 inside the container 
ip netns exec ""${container_id}"" ip -s link show eth0

",https://stackoverflow.com//questions/31265993/docker-networking-namespace-not-visible-in-ip-netns-list
linux - Split one file into multiple files based on delimiter,I have one file with -| as delimiter after each section...need to create separate files for each section using unix.,"
A one liner, no programming. (except the regexp etc.)
csplit --digits=2  --quiet --prefix=outfile infile ""/-|/+1"" ""{*}""


tested on:
csplit (GNU coreutils) 8.30
Notes about usage on Apple Mac
""For OS X users, note that the version of csplit that comes with the OS doesn't work. You'll want the version in coreutils (installable via Homebrew), which is called gcsplit."" — @Danial
""Just to add, you can get the version for OS X to work (at least with High Sierra). You just need to tweak the args a bit csplit -k -f=outfile infile ""/-\|/+1"" ""{3}"". Features that don't seem to work are the ""{*}"", I had to be specific on the number of separators, and needed to add -k to avoid it deleting all outfiles if it can't find a final separator. Also if you want --digits, you need to use -n instead."" — @Pebbl
",https://stackoverflow.com//questions/11313852/split-one-file-into-multiple-files-based-on-delimiter
"linux - EC2 ssh Permission denied (publickey,gssapi-keyex,gssapi-with-mic)","I got this permission denied problem when I want to ssh to my ec2 host. I tried existing solution chmod 600 ""My.pem"" but still didn't work. Here is my debug information:","
I resolved this issue in my centos machine by using command:
ssh -i <Your.pem> ec2-user@<YourServerIP>

It was about userName which was ec2-user in my case.
Referenced From: AMAZONTroubleshooting
",https://stackoverflow.com//questions/33991816/ec2-ssh-permission-denied-publickey-gssapi-keyex-gssapi-with-mic
"c - How to solve ""ptrace operation not permitted"" when trying to attach GDB to a process?",I'm trying to attach a program with gdb but it returns:,"
If you are using Docker, you will probably need these options:
docker run --cap-add=SYS_PTRACE --security-opt seccomp=unconfined

If you are using Podman, you will probably need its --cap-add option too:
podman run --cap-add=SYS_PTRACE

",https://stackoverflow.com//questions/19215177/how-to-solve-ptrace-operation-not-permitted-when-trying-to-attach-gdb-to-a-pro
linux - How to shutdown a Spring Boot Application in a correct way?,"In the Spring Boot Document, they said that 'Each SpringApplication will register a shutdown hook with the JVM to ensure that the ApplicationContext is closed gracefully on exit.'","
If you are using the actuator module, you can shutdown the application via JMX or HTTP if the endpoint is enabled.
add to application.properties:
Spring Boot 2.0 and newer:

management.endpoints.shutdown.enabled=true

Following URL will be available:
/actuator/shutdown -  Allows the application to be gracefully shutdown (not enabled by default).
Depending on how an endpoint is exposed, the sensitive parameter may be used as a security hint.
For example, sensitive endpoints will require a username/password when they are accessed over HTTP (or simply disabled if web security is not enabled).
From the Spring boot documentation
",https://stackoverflow.com//questions/26547532/how-to-shutdown-a-spring-boot-application-in-a-correct-way
linux - Docker not running on Ubuntu WSL due to error cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
Using WSL2
You simply have to activate and use WSL2, I have to install Ubuntu 20.04 as the 18.04 wasn't connecting with Docker desktop. In the windows shell:
To check the WSL mode, run
wsl -l -v
To upgrade your existing Linux distro to v2, run:
wsl --set-version (distro name) 2
WSL Integration will be enabled on your default WSL distribution. To change your default WSL distro, run
wsl --set-default <distro name>
Then in docker you have to.
...use the WSL2 engine

...access from your default WSL2

Based on this article.
A Linux Dev Environment on Windows with WSL 2, Docker Desktop
And the docker docs.
Docker Desktop WSL 2 backend
Below is valid only for WSL1
It seems that docker cannot run inside WSL. What they propose is to connect the WSL to your docker desktop running in windows: Setting Up Docker for Windows and WSL
In the docker forums they also refer to that solution: Cannot connect to the docker daemon
",https://stackoverflow.com//questions/61592709/docker-not-running-on-ubuntu-wsl-due-to-error-cannot-connect-to-the-docker-daemo
linux - Fast Concatenation of Multiple GZip Files,I have list of gzip files:,"
With gzip files, you can simply concatenate the files together, like so:
cat file1.gz file2.gz file3.gz > allfiles.gz

Per the gzip RFC, 

A gzip file consists of a series of ""members"" (compressed data sets). [...] The members simply appear one after another in the file, with no additional information before, between, or after them.

Note that this is not exactly the same as building a single gzip file of the concatenated data; among other things, all of the original filenames are preserved. However, gunzip seems to handle it as equivalent to a concatenation.
Since existing tools generally ignore the filename headers for the additional members, it's not easily possible to extract individual files from the result. If you want this to be possible, build a ZIP file instead. ZIP and GZIP both use the DEFLATE algorithm for the actual compression (ZIP supports some other compression algorithms as well as an option - method 8 is the one that corresponds to GZIP's compression); the difference is in the metadata format. Since the metadata is uncompressed, it's simple enough to strip off the gzip headers and tack on ZIP file headers and a central directory record instead. Refer to the gzip format specification and the ZIP format specification.
",https://stackoverflow.com//questions/8005114/fast-concatenation-of-multiple-gzip-files
linux - How to remove non UTF-8 characters from text file,"I have a bunch of Arabic, English, Russian files which are encoded in utf-8. Trying to process these files using a Perl script, I get this error:","
This command:
iconv -f utf-8 -t utf-8 -c file.txt

will clean up your UTF-8 file, skipping all the invalid characters.
-f is the source format
-t the target format
-c skips any invalid sequence

",https://stackoverflow.com//questions/12999651/how-to-remove-non-utf-8-characters-from-text-file
Kill python interpeter in linux from the terminal,"I want to kill python interpeter - The intention is that all the python files that are running in this moment will stop (without any informantion about this files).
obviously the processes should be closed.","
pkill -9 python

should kill any running python process. 
",https://stackoverflow.com//questions/18428750/kill-python-interpeter-in-linux-from-the-terminal
linux - how to detect invalid utf8 unicode/binary in a text file,"I need to detect corrupted text file where there are invalid (non-ASCII) utf-8, Unicode or binary characters. ","
Assuming you have your locale set to UTF-8 (see locale output), this works well to recognize invalid UTF-8 sequences:
grep -axv '.*' file.txt

Explanation (from grep man page):

-a, --text: treats file as text, essential prevents grep to abort once finding an invalid byte sequence (not being utf8)
-v, --invert-match: inverts the output showing lines not matched
-x '.*' (--line-regexp): means to match a complete line consisting of any utf8 character.

Hence, there will be output, which is the lines containing the invalid not utf8 byte sequence containing lines (since inverted -v)
",https://stackoverflow.com//questions/29465612/how-to-detect-invalid-utf8-unicode-binary-in-a-text-file
Pip is not working for Python 3.10 on Ubuntu,I am new to using Ubuntu and Linux in general. I just attempted to update Python by using sudo apt-get install python3.10. When I run python3.10 -m pip install <library name> I always receive the following error:,"
This is likely caused by a too old system pip version.
Install the latest with:
curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10
and test result
python3.10 -m pip --version

e.g.
pip 22.2.2 from <home>/.local/lib/python3.10/site-packages/pip (python 3.10)

and then test upgrade
python3.10 -m pip install --upgrade pip

e.g.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pip in <home>/.local/lib/python3.10/site-packages (22.2.2)

",https://stackoverflow.com//questions/69503329/pip-is-not-working-for-python-3-10-on-ubuntu
linux - shell-init: error retrieving current directory: getcwd -- The usual fixes do not wor,I have a simple script:,"
I believe the error is not related to the script at all. The issue is: the directory at which you are when you try to run the script does not exist anymore. for example you have two terminals, cd somedir/ at the first one then mv somedir/ somewhere_else/ at the second one, then try to run whatsoever in the first terminal - you'll receive this error message.
Please note you'll get this error even if you re-create directory with the same name because the new directory will have different inode index.
At least this was in my case.
",https://stackoverflow.com//questions/29396928/shell-init-error-retrieving-current-directory-getcwd-the-usual-fixes-do-not
linux - How do I uninstall a program installed with the Appimage Launcher?," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
Since an AppImage is not ""installed"", you don't need to ""uninstall"" it. Just delete the AppImage file and the application is gone. Additionally you may want to remove menu entry by deleting the desktop file from $HOME/.local/share/applications/. 
Files and directories with names starting with a full stop (dot) (.example) are hidden - you might need to turn hidden files visible. You can probably find it somewhere in the settings of the file manager you use or in many file managers you can do that with ctrl+h.
",https://stackoverflow.com//questions/43680226/how-do-i-uninstall-a-program-installed-with-the-appimage-launcher
"linux - How do I fix the Rust error ""linker 'cc' not found"" for Debian on Windows 10?",I'm running Debian on Windows 10 (Windows Subsystem for Linux) and installed Rust using the command:,"
The Linux Rust installer doesn't check for a compiler toolchain, but seems to assume that you've already got a C linker installed! The best solution is to install the tried-and-true gcc toolchain. 
sudo apt install build-essential

If you need to target another architecture, install the appropriate toolchain and target the compilation as follows:
rustc --target=my_target_architecture -C linker=target_toolchain_linker my_rustfile.rs

",https://stackoverflow.com//questions/52445961/how-do-i-fix-the-rust-error-linker-cc-not-found-for-debian-on-windows-10
linux - How to run vi on docker container?,I have installed docker on my host virtual machine. And now want to create a file using vi.,"
login into container with the following command:
docker exec -it <container> bash

Then , run the following command .
apt-get update
apt-get install vim

",https://stackoverflow.com//questions/31515863/how-to-run-vi-on-docker-container
"linux - EC2 ssh Permission denied (publickey,gssapi-keyex,gssapi-with-mic)","I got this permission denied problem when I want to ssh to my ec2 host. I tried existing solution chmod 600 ""My.pem"" but still didn't work. Here is my debug information:","
I resolved this issue in my centos machine by using command:
ssh -i <Your.pem> ec2-user@<YourServerIP>

It was about userName which was ec2-user in my case.
Referenced From: AMAZONTroubleshooting
",https://stackoverflow.com//questions/33991816/ec2-ssh-permission-denied-publickey-gssapi-keyex-gssapi-with-mic
"linux - what does ""bash:no job control in this shell” mean?","I think it's related to the parent process creating new subprocess and does not have tty. Can anyone explain the detail under the hood? i.e. the related working model of bash, process creation, etc?","
You may need to enable job control:
#! /bin/bash   
set -m

",https://stackoverflow.com//questions/11821378/what-does-bashno-job-control-in-this-shell-mean
c - What are the differences between (and reasons to choose) tcmalloc/jemalloc and memory pools?,"tcmalloc/jemalloc are improved memory allocators, and memory pool is also introduced for better memory allocation. So what are the differences between them and how to choose them in my application?","
It depends upon requirement of your program. If your program has more dynamic memory allocations, then you 
need to choose a memory allocator, from available allocators, which would generate most optimal performance 
out of your program.
For good memory management you need to meet the following requirements at minimum:

Check if your system has enough memory to process data.
Are you albe to allocate from the available memory ?
Returning the used memory / deallocated memory to the pool (program or operating system)

The ability of a good memory manager can be tested on basis of (at the bare minimum) its efficiency in retriving / allocating and 
returning / dellaocating memory. (There are many more conditions like cache locality, managing overhead, VM environments, small or large
environments, threaded environment etc..)
With respect to tcmalloc and jemalloc there are many people who have done comparisions. With reference to one of the 
comparisions:
http://ithare.com/testing-memory-allocators-ptmalloc2-tcmalloc-hoard-jemalloc-while-trying-to-simulate-real-world-loads/
tcmalloc scores over all other in terms of CPU cycles per allocation if the number of threads are less.
jemalloc is very close to tcmalloc but better than ptmalloc (std glibc implementation). 
In terms of memory overhead jemalloc is the best, seconded by ptmalloc, followed by tcmalloc.
Overall it can be said that jemalloc scores over others.  You can also read more about jemalloc here:
https://www.facebook.com/notes/facebook-engineering/scalable-memory-allocation-using-jemalloc/480222803919
I have just quoted from tests done and published by other people and have not tested it myself. I hope 
this could be a good starting point for you and use it to test and select the most optimal for 
your application.
",https://stackoverflow.com//questions/9866145/what-are-the-differences-between-and-reasons-to-choose-tcmalloc-jemalloc-and-m
python - docker.errors.DockerException: Error while fetching server API version,I want to install this module but there is something wrong when I try the step docker-compose build ...,"
Are you sure docker is running on your system? You can get that error when compose is not able to connect to docker via docker socket (if any other way for connection is not defined).
If you are running on linux, usually you can run systemctl status docker to check if docker daemon is running and systemctl start docker to start it.
It would help to tell what OS and docker version are you using.
",https://stackoverflow.com//questions/64952238/docker-errors-dockerexception-error-while-fetching-server-api-version
math - How do I divide in the Linux console?,I have to variables and I want to find the value of one divided by the other. What commands should I use to do this?,"
In the bash shell, surround arithmetic expressions with $(( ... )) 
$ echo $(( 7 / 3 ))
2

Although I think you are limited to integers.
",https://stackoverflow.com//questions/1088098/how-do-i-divide-in-the-linux-console
c++ - How do you find what version of libstdc++ library is installed on your linux machine?,I found the following command: strings /usr/lib/libstdc++.so.6 | grep GLIBC from here. It seems to work but this is an ad-hoc/heuristic method.,"
To find which library is being used you could run
 $ /sbin/ldconfig -p | grep stdc++
    libstdc++.so.6 (libc6) => /usr/lib/libstdc++.so.6

The list of compatible versions for libstdc++ version 3.4.0 and above is provided by
 $ strings /usr/lib/libstdc++.so.6 | grep LIBCXX
 GLIBCXX_3.4
 GLIBCXX_3.4.1
 GLIBCXX_3.4.2
 ...

For earlier versions the symbol GLIBCPP is defined.
The date stamp of the library is defined in a macro __GLIBCXX__ or __GLIBCPP__ depending on the version:
// libdatestamp.cxx
#include <cstdio>

int main(int argc, char* argv[]){
#ifdef __GLIBCPP__
    std::printf(""GLIBCPP: %d\n"",__GLIBCPP__);
#endif
#ifdef __GLIBCXX__
    std::printf(""GLIBCXX: %d\n"",__GLIBCXX__);
#endif
   return 0;
}

$ g++ libdatestamp.cxx -o libdatestamp
$ ./libdatestamp
GLIBCXX: 20101208

The table of datestamps of libstdc++ versions is listed in the documentation:
",https://stackoverflow.com//questions/10354636/how-do-you-find-what-version-of-libstdc-library-is-installed-on-your-linux-mac
logging - View a log file in Linux dynamically," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
tail -f yourlog.csv
Newly appended lines will continuously show.
",https://stackoverflow.com//questions/2099149/view-a-log-file-in-linux-dynamically
How can I identify the request queue for a linux block device,"I am working on this driver that connects the hard disk over the network. There is a bug that if I enable two or more hard disks on the computer, only the first one gets the partitions looked over and identified. The result is, if I have 1 partition on hda and 1 partitions on hdb, as soon as I connect hda there is a partition that can be mounted. So hda1 gets a blkid xyz123 as soon as it mounts. But when I go ahead and mount hdb1 it also comes up with the same blkid and in fact, the driver is reading it from hda, not hdb. ","
Queue = blk_init_queue(sbd_request, &Device.lock);

",https://stackoverflow.com//questions/6785651/how-can-i-identify-the-request-queue-for-a-linux-block-device
linux - Is Mac OS X a POSIX OS?,"What is it that makes an OS a POSIX system? All versions of Linux are POSIX, right? What about Mac OS X?","

Is Mac OS X a POSIX OS?

Yes.
POSIX is a group of standards that determine a portable API for Unix-like operating systems. Mac OS X is Unix-based (and has been certified as such), and in accordance with this is POSIX compliant. POSIX guarantees that certain system calls will be available.
Essentially, Mac satisfies the API required to be POSIX compliant, which makes it a POSIX OS.
All versions of Linux are not POSIX-compliant. Kernel versions prior to 2.6 were not compliant, and today Linux isn't officially POSIX-compliant because they haven't gone out of their way to get certified (which will likely never happen). Regardless, Linux can be treated as a POSIX system for almost all intents and purposes.
",https://stackoverflow.com//questions/5785516/is-mac-os-x-a-posix-os
linux - How to perform grep operation on all files in a directory?,"Working with xenserver, and I want to perform a command on each file that is in a directory, grepping some stuff out of the output of the command and appending it in a file.","
In Linux, I normally use this command to recursively grep for a particular text within a directory:
grep -rni ""string"" *

where

r = recursive i.e, search subdirectories within the current directory
n = to print the line numbers to stdout
i = case insensitive search

",https://stackoverflow.com//questions/15286947/how-to-perform-grep-operation-on-all-files-in-a-directory
linux - 64 bit ntohl() in C++?,"The man pages for htonl() seem to suggest that you can only use it for up to 32 bit values. (In reality, ntohl() is defined for unsigned long, which on my platform is 32 bits. I suppose if the unsigned long were 8 bytes, it would work for 64 bit ints).","
Documentation: man htobe64 on Linux (glibc >= 2.9) or FreeBSD.
Unfortunately OpenBSD, FreeBSD and glibc (Linux) did not quite work together smoothly to create one (non-kernel-API) libc standard for this, during an attempt in 2009.
Currently, this short bit of preprocessor code:
#if defined(__linux__)
#  include <endian.h>
#elif defined(__FreeBSD__) || defined(__NetBSD__)
#  include <sys/endian.h>
#elif defined(__OpenBSD__)
#  include <sys/types.h>
#  define be16toh(x) betoh16(x)
#  define be32toh(x) betoh32(x)
#  define be64toh(x) betoh64(x)
#endif

(tested on Linux and OpenBSD) should hide the differences. It gives you the Linux/FreeBSD-style macros on those 4 platforms.
Use example:
  #include <stdint.h>    // For 'uint64_t'

  uint64_t  host_int = 123;
  uint64_t  big_endian;

  big_endian = htobe64( host_int );
  host_int = be64toh( big_endian );

It's the most ""standard C library""-ish approach available at the moment.
",https://stackoverflow.com//questions/809902/64-bit-ntohl-in-c
c++ - Easily measure elapsed time,I am trying to use time() to measure various points of my program.,"
//***C++11 Style:***
#include <chrono>

std::chrono::steady_clock::time_point begin = std::chrono::steady_clock::now();
std::chrono::steady_clock::time_point end = std::chrono::steady_clock::now();

std::cout << ""Time difference = "" << std::chrono::duration_cast<std::chrono::microseconds>(end - begin).count() << ""[µs]"" << std::endl;
std::cout << ""Time difference = "" << std::chrono::duration_cast<std::chrono::nanoseconds> (end - begin).count() << ""[ns]"" << std::endl;

",https://stackoverflow.com//questions/2808398/easily-measure-elapsed-time
Randomly shuffling lines in Linux / Bash,I have some files in linux. For example 2 and i need shuffling the files in one file.,"
You should use shuf command =)
cat file1 file2 | shuf

Or with Perl :
cat file1 file2 | perl -MList::Util=shuffle -wne 'print shuffle <>;'

",https://stackoverflow.com//questions/17578873/randomly-shuffling-lines-in-linux-bash
linux - How to convert Windows end of line in Unix end of line (CR/LF to LF),I'm a Java developer and I'm using Ubuntu to develop. The project was created in Windows with Eclipse and it's using the Windows-1252 encoding.,"
There should be a program called dos2unix that will fix line endings for you.  If it's not already on your Linux box, it should be available via the package manager.
",https://stackoverflow.com//questions/3891076/how-to-convert-windows-end-of-line-in-unix-end-of-line-cr-lf-to-lf
linux - Get Docker Container Names,This command gives me a list of running container IDs:,"
docker ps --format ""{{.Names}}""

",https://stackoverflow.com//questions/31887258/get-docker-container-names
"linux - shell init issue when click tab, what's wrong with getcwd?","once i click Tab on bash, the error message will appear, what's wrong?","
This usually occurs when your current directory does not exist anymore. Most likely, from another terminal you remove that directory (from within a script or whatever). To get rid of this, in case your current directory was recreated in the meantime, just cd to another (existing) directory and then cd back; the simplest would be: cd; cd -.
",https://stackoverflow.com//questions/12338233/shell-init-issue-when-click-tab-whats-wrong-with-getcwd
linux - C: Run a System Command and Get Output?,"I want to run a command in linux and get the text returned of what it outputs, but I do not want this text printed to screen. Is there a more elegant way than making a temporary file?","
You want the ""popen"" function.  Here's an example of running the command ""ls /etc"" and outputing to the console.
#include <stdio.h>
#include <stdlib.h>


int main( int argc, char *argv[] )
{

  FILE *fp;
  char path[1035];

  /* Open the command for reading. */
  fp = popen(""/bin/ls /etc/"", ""r"");
  if (fp == NULL) {
    printf(""Failed to run command\n"" );
    exit(1);
  }

  /* Read the output a line at a time - output it. */
  while (fgets(path, sizeof(path), fp) != NULL) {
    printf(""%s"", path);
  }

  /* close */
  pclose(fp);

  return 0;
}

",https://stackoverflow.com//questions/646241/c-run-a-system-command-and-get-output
centos - How do I download a file from the internet to my linux server with Bash,Want to improve this question? Update the question so it's on-topic for Stack Overflow.,"
Using wget
wget -O /tmp/myfile 'http://www.google.com/logo.jpg'

or curl:
curl -o /tmp/myfile 'http://www.google.com/logo.jpg'

",https://stackoverflow.com//questions/14300794/how-do-i-download-a-file-from-the-internet-to-my-linux-server-with-bash
node.js - How can I update NodeJS and NPM to their latest versions?,I installed NPM for access to additional Node.js Modules.,"
Use:
npm update -g npm

See the docs for the update command:

npm update [-g] [<pkg>...]
This command will update all the packages listed to the latest version (specified by the tag config), respecting semver.

Additionally, see the documentation on Node.js and NPM installation and Upgrading NPM.
The following original answer is from the old FAQ that no longer exists, but should work for Linux and Mac:

How do I update npm?
npm install -g npm

Please note that this command will remove your current version of npm. Make sure to use sudo npm install -g npm if on a Mac.
You can also update all outdated local packages by doing npm update without any arguments, or global packages by doing npm update -g.
Occasionally, the version of npm will progress such that the current version cannot be properly installed with the version that you have installed already. (Consider, if there is ever a bug in the update command.) In those cases, you can do this:
curl https://www.npmjs.com/install.sh | sh


To update Node.js itself, I recommend you use nvm, the Node Version Manager.
",https://stackoverflow.com//questions/6237295/how-can-i-update-nodejs-and-npm-to-their-latest-versions
Pip is not working for Python 3.10 on Ubuntu,I am new to using Ubuntu and Linux in general. I just attempted to update Python by using sudo apt-get install python3.10. When I run python3.10 -m pip install <library name> I always receive the following error:,"
This is likely caused by a too old system pip version.
Install the latest with:
curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10
and test result
python3.10 -m pip --version

e.g.
pip 22.2.2 from <home>/.local/lib/python3.10/site-packages/pip (python 3.10)

and then test upgrade
python3.10 -m pip install --upgrade pip

e.g.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pip in <home>/.local/lib/python3.10/site-packages (22.2.2)

",https://stackoverflow.com//questions/69503329/pip-is-not-working-for-python-3-10-on-ubuntu
linux - Skipping acquire of configured file 'main/binary-i386/Packages',"Good afternoon, please tell me what I'm doing wrong. I just installed the Linux Ubuntu on my computer and still don’t understand anything about it. I tried to install PostreSQL and pgAdmin. I installed on this video tutorial https://www.youtube.com/watch?v=Vdzb7JTPnGk I get this error.","
You must change the line of /etc/apt/sources.list.d/pgdg.list to 
deb [arch=amd64] http://apt.postgresql.org/pub/repos/apt/ focal-pgdg main

",https://stackoverflow.com//questions/61523447/skipping-acquire-of-configured-file-main-binary-i386-packages
linux - docker networking namespace not visible in ip netns list,When I create a new docker container like with ,"
That's because docker is not creating the reqired symlink:
# (as root)
pid=$(docker inspect -f '{{.State.Pid}}' ${container_id})
mkdir -p /var/run/netns/
ln -sfT /proc/$pid/ns/net /var/run/netns/$container_id

Then, the container's netns namespace can be examined with ip netns ${container_id}, e.g.:
# e.g. show stats about eth0 inside the container 
ip netns exec ""${container_id}"" ip -s link show eth0

",https://stackoverflow.com//questions/31265993/docker-networking-namespace-not-visible-in-ip-netns-list
c - How to make a daemon process,"I am trying to understand how can I make my program a daemon. So some things which I came across are in general, a program performs the following steps to become a daemon:","
If you are looking for a clean approach please consider using standard api-       int daemon(int nochdir, int noclose);. Man page pretty simple and self explanatory. man page. A well tested api far outweigh our own implementation interms of portability and stability.
",https://stackoverflow.com//questions/5384168/how-to-make-a-daemon-process
environment variables - How does /usr/bin/env work in a Linux shebang line?,I know shebang line like this: ,"
env is the name of a Unix program. If you read the manual (man env) you can see that one way to use it is env COMMAND, where in your case, COMMAND is python3.
According to the manual, this will

Set each NAME to VALUE in the environment and run COMMAND.

Running env alone will show you what NAMEs and VALUEs are set:
$ env
TERM=xterm-256color
SHELL=/bin/bash
PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin
…

Therefore, /usr/bin/env python3 is an instruction to set the PATH (as well as all the other NAME+VALUE pairs), and then run python3, using the first directory in the PATH that contains the python3 executable.
",https://stackoverflow.com//questions/43793040/how-does-usr-bin-env-work-in-a-linux-shebang-line
c - unix domain socket VS named pipes?,After looking at a unix named socket and i thought they were named pipes. I looked at name pipes and didnt see much of a difference. I saw they were initialized differently but thats the only thing i notice. Both use the C write/read function and work alike AFAIK.,"
UNIX-domain sockets are generally more flexible than named pipes.  Some of their advantages are:

You can use them for more than two processes communicating (eg. a server process with potentially multiple client processes connecting);
They are bidirectional;
They support passing kernel-verified UID / GID credentials between processes;
They support passing file descriptors between processes;
They support packet and sequenced packet modes.

To use many of these features, you need to use the send() / recv() family of system calls rather than write() / read().
",https://stackoverflow.com//questions/9475442/unix-domain-socket-vs-named-pipes
linux - sudo: docker-compose: command not found,I am trying to run docker-compose using sudo.,"
On Ubuntu 16.04
Here's how I fixed this issue: Refer Docker Compose documentation 

sudo curl -L https://github.com/docker/compose/releases/download/1.21.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

After you do the curl command , it'll put docker-compose into the 

/usr/local/bin

which is not on the PATH.
To fix it, create a symbolic link:

sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose

And now if you do: 
docker-compose --version
You'll see that docker-compose is now on the PATH
",https://stackoverflow.com//questions/38775954/sudo-docker-compose-command-not-found
linux - Automatically enter SSH password with script,I need to create a script that automatically inputs a password to OpenSSH ssh client.,"
First you need to install sshpass.

Ubuntu/Debian: apt-get install sshpass 
Fedora/CentOS: yum install sshpass 
Arch: pacman -S sshpass


Example:
sshpass -p ""YOUR_PASSWORD"" ssh -o StrictHostKeyChecking=no YOUR_USERNAME@SOME_SITE.COM

Custom port example:
sshpass -p ""YOUR_PASSWORD"" ssh -o StrictHostKeyChecking=no YOUR_USERNAME@SOME_SITE.COM:2400

Notes:

sshpass can also read a password from a file when the -f flag is passed.


Using -f prevents the password from being visible if the ps command is executed.
The file that the password is stored in should have secure permissions.


",https://stackoverflow.com//questions/12202587/automatically-enter-ssh-password-with-script
linux - How do I find out what inotify watches have been registered?,"I have my inotify watch limit set to 1024 (I think the default is 128?).  Despite that, yeoman, Guard and Dropbox constantly fail, and tell me to up my inotify limit.  Before doing so, I'd like to know what's consuming all my watches (I have very few files in my Dropbox).","
Oct 31 2022 update

While my script below works fine as it is, Michael Sartain implemented a native executable that is much faster, along with additional functionality not present in my script (below). Worth checking out if you can spend a few seconds compiling it! I have also added contributed some PRs to align the functionality, so it should be pretty 1:1, just faster.

Upvote his answer on the Unix Stackexchange.


Original answer with script
I already answered this in the same thread on Unix Stackexchange as was mentioned by @cincodenada, but thought I could repost my ready-made answer here, seeing that no one really has something that works:

I have a premade script, inotify-consumers, that lists the top offenders for you:
   INOTIFY   INSTANCES
   WATCHES      PER   
    COUNT     PROCESS   PID USER         COMMAND
------------------------------------------------------------
   21270         1       11076 my-user    /snap/intellij-idea-ultimate/357/bin/fsnotifier
     201         6           1 root       /sbin/init splash
     115         5        1510 my-user    /lib/systemd/systemd --user
      85         1        3600 my-user    /usr/libexec/xdg-desktop-portal-gtk
      77         1        2580 my-user    /usr/libexec/gsd-xsettings
      35         1        2475 my-user    /usr/libexec/gvfsd-trash --spawner :1.5 /org/gtk/gvfs/exec_spaw/0
      32         1         570 root       /lib/systemd/systemd-udevd
      26         1        2665 my-user    /snap/snap-store/558/usr/bin/snap-store --gapplication-service
      18         2        1176 root       /usr/libexec/polkitd --no-debug
      14         1        1858 my-user    /usr/bin/gnome-shell
      13         1        3641 root       /usr/libexec/fwupd/fwupd
...

   21983  WATCHES TOTAL COUNT

INotify instances per user (e.g. limits specified by fs.inotify.max_user_instances): 

INSTANCES    USER
-----------  ------------------
41           my-user
23           root
1            whoopsie
1            systemd-ti+
...

Here you quickly see why the default limit of 8K watchers is too little on a development machine, as just WebStorm instance quickly maxes this when encountering a node_modules folder with thousands of folders. Add a webpack watcher to guarantee problems ...
Even though it was much faster than the other alternatives when I made it initially, Simon Matter added some speed enhancements for heavily loaded Big Iron Linux (hundreds of cores) that sped it up immensely, taking it down from ten minutes (!) to 15 seconds on his monster rig.
Later on, Brian Dowling contributed instance count per process, at the expense of relatively higher runtime. This is insignificant on normal machines with a runtime of about one second, but if you have Big Iron, you might want the earlier version with about 1/10 the amount of system time :)
How to use
inotify-consumers --help 😊 To get it on your machine, just copy the contents of the script and put it somewhere in your $PATH, like /usr/local/bin. Alternatively, if you trust this stranger on the net, you can avoid copying it and pipe it into bash over http:
$ curl -s https://raw.githubusercontent.com/fatso83/dotfiles/master/utils/scripts/inotify-consumers | bash 

       INOTIFY
       WATCHER
        COUNT     PID USER     COMMAND
    --------------------------------------
        3044   3933 myuser node /usr/local/bin/tsserver
        2965   3941 myuser /usr/local/bin/node /home/myuser/.config/coc/extensions/node_modules/coc-tsserver/bin/tsserverForkStart /hom...

        6990  WATCHES TOTAL COUNT

How does it work?
For reference, the main content of the script is simply this (inspired by this answer)
find /proc/*/fd \
    -lname anon_inode:inotify \
    -printf '%hinfo/%f\n' 2>/dev/null \
    \
    | xargs grep -c '^inotify'  \
    | sort -n -t: -k2 -r 

Changing the limits
In case you are wondering how to increase the limits
$ inotify-consumers --limits 

Current limits
-------------
fs.inotify.max_user_instances = 128
fs.inotify.max_user_watches = 524288


Changing settings permanently
-----------------------------
echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf
sudo sysctl -p # re-read config

",https://stackoverflow.com//questions/13758877/how-do-i-find-out-what-inotify-watches-have-been-registered
linux - fork: retry: Resource temporarily unavailable,Want to improve this question? Update the question so it's on-topic for Stack Overflow.,"
This is commonly caused by running out of file descriptors. 
There is the systems total file descriptor limit, what do you get from the command:
sysctl fs.file-nr

This returns counts of file descriptors:
<in_use> <unused_but_allocated> <maximum>

To find out what a users file descriptor limit is run the commands:
sudo su - <username>
ulimit -Hn

To find out how many file descriptors are in use by a user run the command:
sudo lsof -u <username> 2>/dev/null | wc -l

So now if you are having a system file descriptor limit issue you will need to edit your /etc/sysctl.conf file and add, or modify it it already exists, a line with fs.file-max and set it to a value large enough to deal with the number of file descriptors you need and reboot.
fs.file-max = 204708

",https://stackoverflow.com//questions/12079087/fork-retry-resource-temporarily-unavailable
"linux - Split files using tar, gz, zip, or bzip2",Want to improve this question? Update the question so it's on-topic for Stack Overflow.,"
You can use the split command with the -b option:
split -b 1024m file.tar.gz

It can be reassembled on a Windows machine using @Joshua's answer.
copy /b file1 + file2 + file3 + file4 filetogether


Edit: As @Charlie stated in the comment below, you might want to set a prefix explicitly because it will use x otherwise, which can be confusing.
split -b 1024m ""file.tar.gz"" ""file.tar.gz.part-""

// Creates files: file.tar.gz.part-aa, file.tar.gz.part-ab, file.tar.gz.part-ac, ...


Edit: Editing the post because question is closed and the most effective solution is very close to the content of this answer:
# create archives
$ tar cz my_large_file_1 my_large_file_2 | split -b 1024MiB - myfiles_split.tgz_
# uncompress
$ cat myfiles_split.tgz_* | tar xz

This solution avoids the need to use an intermediate large file when (de)compressing. Use the tar -C option to use a different directory for the resulting files. btw if the archive consists from only a single file, tar could be avoided and only gzip used:
# create archives
$ gzip -c my_large_file | split -b 1024MiB - myfile_split.gz_
# uncompress
$ cat myfile_split.gz_* | gunzip -c > my_large_file

For windows you can download ported versions of the same commands or use cygwin.
",https://stackoverflow.com//questions/1120095/split-files-using-tar-gz-zip-or-bzip2
c++ - How to fix: /usr/lib/libstdc++.so.6: version `GLIBCXX_3.4.15' not found,"So I'm now desperate in finding a fix for this. I'm compiling a shared library .so in Ubuntu 32 bit (Have tried doing it under Debian and Ubuntu 64 bit, but none worked either)","
Link statically to libstdc++ with -static-libstdc++ gcc option.
",https://stackoverflow.com//questions/19386651/how-to-fix-usr-lib-libstdc-so-6-version-glibcxx-3-4-15-not-found
shell - Fast Linux file count for a large number of files,"I'm trying to figure out the best way to find the number of files in a particular directory when there are a very large number of files (more than 100,000).","
By default ls sorts the names, which can take a while if there are a lot of them.  Also there will be no output until all of the names are read and sorted.  Use the ls -f option to turn off sorting.
ls -f | wc -l

Note: This will also enable -a, so ., .., and other files starting with . will be counted.
",https://stackoverflow.com//questions/1427032/fast-linux-file-count-for-a-large-number-of-files
Why is creating a new process more expensive on Windows than Linux?,I've heard that creating a new process on a Windows box is more expensive than on Linux.  Is this true?  Can somebody explain the technical reasons for why it's more expensive and provide any historical reasons for the design decisions behind those reasons?,"
mweerden: NT has been designed for multi-user from day one, so this is not really a reason. However, you are right about that process creation plays a less important role on NT than on Unix as NT, in contrast to Unix, favors multithreading over multiprocessing.
Rob, it is true that fork is relatively cheap when COW is used, but as a matter of fact, fork is mostly followed by an exec. And an exec has to load all images as well. Discussing the performance of fork therefore is only part of the truth.
When discussing the speed of process creation, it is probably a good idea to distinguish between NT and Windows/Win32. As far as NT (i.e. the kernel itself) goes, I do not think process creation (NtCreateProcess) and thread creation (NtCreateThread) is significantly slower as on the average Unix. There might be a little bit more going on, but I do not see the primary reason for the performance difference here. 
If you look at Win32, however, you'll notice that it adds quite a bit of overhead to process creation. For one, it requires the CSRSS to be notified about process creation, which involves LPC. It requires at least kernel32 to be loaded additionally, and it has to perform a number of additional bookkeeping work items to be done before the process is considered to be a full-fledged Win32 process. And let's not forget about all the additional overhead imposed by parsing manifests, checking if the image requires a compatbility shim, checking whether software restriction policies apply, yada yada.
That said, I see the overall slowdown in the sum of all those little things that have to be done in addition to the raw creation of a process, VA space, and initial thread. But as said in the beginning -- due to the favoring of multithreading over multitasking, the only software that is seriously affected by this additional expense is poorly ported Unix software. Although this sitatuion changes when software like Chrome and IE8 suddenly rediscover the benefits of multiprocessing and begin to frequently start up and teardown processes...
",https://stackoverflow.com//questions/47845/why-is-creating-a-new-process-more-expensive-on-windows-than-linux
"linux - ""Couldn't find a file descriptor referring to the console"" on Ubuntu bash on Windows","I have a problem with Bash on Ubuntu on Windows. If I type ""open (filename)"" on Mac terminal, it opens the file with the right program but if I try to use it on Windows bash, it says: ""Couldn't find a file descriptor referring to the console"".","
Instead of open u can use xdg-open which does the same thing, independently of application i.e. pdf, image, etc. It will open a new virtual terminal (I have tried this on Linux)
Example:
xdg-open ~/Pictures/Wallpapers/myPic.jpg
xdg-open ~/Docs/holidays.pdf
",https://stackoverflow.com//questions/42463929/couldnt-find-a-file-descriptor-referring-to-the-console-on-ubuntu-bash-on-win
c++ - How can I get the IP address of a (Linux) machine?,This Question is almost the same as the previously asked How can I get the IP Address of a local computer? -Question. However I need to find the IP address(es) of a Linux Machine.,"
I found the ioctl solution problematic on os x (which is POSIX compliant so should be similiar to linux). However getifaddress() will let you do the same thing easily, it works fine for me on os x 10.5 and should be the same below.
I've done a quick example below which will print all of the machine's IPv4 address, (you should also check the getifaddrs was successful ie returns 0).
I've updated it show IPv6 addresses too.
#include <stdio.h>      
#include <sys/types.h>
#include <ifaddrs.h>
#include <netinet/in.h> 
#include <string.h> 
#include <arpa/inet.h>

int main (int argc, const char * argv[]) {
    struct ifaddrs * ifAddrStruct=NULL;
    struct ifaddrs * ifa=NULL;
    void * tmpAddrPtr=NULL;

    getifaddrs(&ifAddrStruct);

    for (ifa = ifAddrStruct; ifa != NULL; ifa = ifa->ifa_next) {
        if (!ifa->ifa_addr) {
            continue;
        }
        if (ifa->ifa_addr->sa_family == AF_INET) { // check it is IP4
            // is a valid IP4 Address
            tmpAddrPtr=&((struct sockaddr_in *)ifa->ifa_addr)->sin_addr;
            char addressBuffer[INET_ADDRSTRLEN];
            inet_ntop(AF_INET, tmpAddrPtr, addressBuffer, INET_ADDRSTRLEN);
            printf(""%s IP Address %s\n"", ifa->ifa_name, addressBuffer); 
        } else if (ifa->ifa_addr->sa_family == AF_INET6) { // check it is IP6
            // is a valid IP6 Address
            tmpAddrPtr=&((struct sockaddr_in6 *)ifa->ifa_addr)->sin6_addr;
            char addressBuffer[INET6_ADDRSTRLEN];
            inet_ntop(AF_INET6, tmpAddrPtr, addressBuffer, INET6_ADDRSTRLEN);
            printf(""%s IP Address %s\n"", ifa->ifa_name, addressBuffer); 
        } 
    }
    if (ifAddrStruct!=NULL) freeifaddrs(ifAddrStruct);
    return 0;
}

",https://stackoverflow.com//questions/212528/how-can-i-get-the-ip-address-of-a-linux-machine
"linux - Explain the effects of export LANG, LC_CTYPE, and LC_ALL"," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
I'll explain with detail:
export LANG=ru_RU.UTF-8

That is a shell command that will export an environment variable named LANG with the given value ru_RU.UTF-8. That instructs internationalized programs to use the Russian language (ru), variant from Russia (RU), and the UTF-8 encoding for console output.
Generally this single line is enough.
This other one:
export LC_CTYPE=ru_RU.UTF-8

Does a similar thing, but it tells the program not to change the language, but only the CTYPE to Russian. If a program can change a text to uppercase, then it will use the Russian rules to do so, even though the text itself may be in English.
It is worth saying that mixing LANG and LC_CTYPE can give unexpected results, because few people do that, so it is quite untested, unless maybe:
export LANG=ru_RU.UTF-8
export LC_CTYPE=C

That will make the program output in Russian, but the CTYPE standard old C style.
The last line, LC_ALL is a last resort override, that will make the program ignore all the other LC_* variables and use this. I think that you should never write it in a profile line, but use it to run a program in a given language. For example, if you want to write a bug report, and you don't want any kind of localized output, and you don't know which LC_* variables are set:
LC_ALL=C program

About changing the language of all your programs or only the console, that depends on where you put these lines. I put mine in ~/.bashrc so they don't apply to the GUI, only to the bash consoles.
",https://stackoverflow.com//questions/30479607/explain-the-effects-of-export-lang-lc-ctype-and-lc-all
networking - Increasing the maximum number of TCP/IP connections in Linux,"I am programming a server and it seems like my number of connections is being limited since my bandwidth isn't being saturated even when I've set the number of connections to ""unlimited"".","
Maximum number of connections are impacted by certain limits on both client & server sides, albeit a little differently.
On the client side:
Increase the ephermal port range, and decrease the tcp_fin_timeout
To find out the default values:
sysctl net.ipv4.ip_local_port_range
sysctl net.ipv4.tcp_fin_timeout

The ephermal port range defines the maximum number of outbound sockets a host can create from a particular I.P. address. The fin_timeout defines the minimum time these sockets will stay in TIME_WAIT state (unusable after being used once).
Usual system defaults are:

net.ipv4.ip_local_port_range = 32768   61000
net.ipv4.tcp_fin_timeout = 60

This basically means your system cannot consistently guarantee more than (61000 - 32768) / 60 = 470 sockets per second. If you are not happy with that, you could begin with increasing the port_range. Setting the range to 15000 61000 is pretty common these days. You could further increase the availability by decreasing the fin_timeout. Suppose you do both, you should see over 1500 outbound connections per second, more readily.
To change the values:
sysctl net.ipv4.ip_local_port_range=""15000 61000""
sysctl net.ipv4.tcp_fin_timeout=30

The above should not be interpreted as the factors impacting system capability for making outbound connections per second. But rather these factors affect system's ability to handle concurrent connections in a sustainable manner for large periods of ""activity.""
Default Sysctl values on a typical Linux box for tcp_tw_recycle & tcp_tw_reuse would be
net.ipv4.tcp_tw_recycle=0
net.ipv4.tcp_tw_reuse=0

These do not allow a connection from a ""used"" socket (in wait state) and force the sockets to last the complete time_wait cycle. I recommend setting:
sysctl net.ipv4.tcp_tw_recycle=1
sysctl net.ipv4.tcp_tw_reuse=1 

This allows fast cycling of sockets in time_wait state and re-using them. But before you do this change make sure that this does not conflict with the protocols that you would use for the application that needs these sockets. Make sure to read post ""Coping with the TCP TIME-WAIT"" from Vincent Bernat to understand the implications. The net.ipv4.tcp_tw_recycle option is quite problematic for public-facing servers as it won’t handle connections from two different computers behind the same NAT device, which is a problem hard to detect and waiting to bite you. Note that net.ipv4.tcp_tw_recycle has been removed from Linux 4.12.
On the Server Side:
The net.core.somaxconn value has an important role. It limits the maximum number of requests queued to a listen socket. If you are sure of your server application's capability, bump it up from default 128 to something like 128 to 1024. Now you can take advantage of this increase by modifying the listen backlog variable in your application's listen call, to an equal or higher integer.
sysctl net.core.somaxconn=1024

txqueuelen parameter of your ethernet cards also have a role to play. Default values are 1000, so bump them up to 5000 or even more if your system can handle it.
ifconfig eth0 txqueuelen 5000
echo ""/sbin/ifconfig eth0 txqueuelen 5000"" >> /etc/rc.local

Similarly bump up the values for net.core.netdev_max_backlog and net.ipv4.tcp_max_syn_backlog. Their default values are 1000 and 1024 respectively.
sysctl net.core.netdev_max_backlog=2000
sysctl net.ipv4.tcp_max_syn_backlog=2048

Now remember to start both your client and server side applications by increasing the FD ulimts, in the shell.
Besides the above one more popular technique used by programmers is to reduce the number of tcp write calls. My own preference is to use a buffer wherein I push the data I wish to send to the client, and then at appropriate points I write out the buffered data into the actual socket. This technique allows me to use large data packets, reduce fragmentation, reduces my CPU utilization both in the user land and at kernel-level.
",https://stackoverflow.com//questions/410616/increasing-the-maximum-number-of-tcp-ip-connections-in-linux
How to get the current time in milliseconds from C in Linux?,How do I get the current time on Linux in milliseconds?,"
This can be achieved using the POSIX clock_gettime function.
In the current version of POSIX, gettimeofday is marked obsolete. This means it may be removed from a future version of the specification. Application writers are encouraged to use the clock_gettime function instead of gettimeofday.
Here is an example of how to use clock_gettime:
#define _POSIX_C_SOURCE 200809L

#include <inttypes.h>
#include <math.h>
#include <stdio.h>
#include <time.h>

void print_current_time_with_ms (void)
{
    long            ms; // Milliseconds
    time_t          s;  // Seconds
    struct timespec spec;

    clock_gettime(CLOCK_REALTIME, &spec);

    s  = spec.tv_sec;
    ms = round(spec.tv_nsec / 1.0e6); // Convert nanoseconds to milliseconds
    if (ms > 999) {
        s++;
        ms = 0;
    }

    printf(""Current time: %""PRIdMAX"".%03ld seconds since the Epoch\n"",
           (intmax_t)s, ms);
}

If your goal is to measure elapsed time, and your system supports the ""monotonic clock"" option, then you should consider using CLOCK_MONOTONIC instead of CLOCK_REALTIME.
",https://stackoverflow.com//questions/3756323/how-to-get-the-current-time-in-milliseconds-from-c-in-linux
linux - sed command with -i option (in-place editing) works fine on Ubuntu but not Mac,I know nothing about Sed but need this command (which works fine on Ubuntu) to work on a Mac OSX:,"
Ubuntu ships with GNU sed, where the suffix for the -i option is optional. OS X ships with BSD sed, where the suffix is mandatory. Try sed -i ''
",https://stackoverflow.com//questions/16745988/sed-command-with-i-option-in-place-editing-works-fine-on-ubuntu-but-not-mac
c - How to make a daemon process,"I am trying to understand how can I make my program a daemon. So some things which I came across are in general, a program performs the following steps to become a daemon:","
If you are looking for a clean approach please consider using standard api-       int daemon(int nochdir, int noclose);. Man page pretty simple and self explanatory. man page. A well tested api far outweigh our own implementation interms of portability and stability.
",https://stackoverflow.com//questions/5384168/how-to-make-a-daemon-process
linux - How to update-alternatives to Python 3 without breaking apt?,The other day I decided that I wanted the command python to default to firing up python3 instead of python2.,"
Per Debian policy, python refers to Python 2 and python3 refers to Python 3. Don't try to change this system-wide or you are in for the sort of trouble you already discovered.
Virtual environments allow you to run an isolated Python installation with whatever version of Python and whatever libraries you need without messing with the system Python install.
With recent Python 3, venv is part of the standard library; with older versions, you might need to install python3-venv or a similar package.
$HOME~$ python --version
Python 2.7.11

$HOME~$ python3 -m venv myenv
... stuff happens ...

$HOME~$ . ./myenv/bin/activate

(myenv) $HOME~$ type python   # ""type"" is preferred over which; see POSIX
python is /home/you/myenv/bin/python

(myenv) $HOME~$ python --version
Python 3.5.1

A common practice is to have a separate environment for each project you work on, anyway; but if you want this to look like it's effectively system-wide for your own login, you could add the activation stanza to your .profile or similar.
",https://stackoverflow.com//questions/43062608/how-to-update-alternatives-to-python-3-without-breaking-apt
Git status ignore line endings / identical files / windows & linux environment / dropbox / meld,How do I make ,"
Try setting core.autocrlf value like this : 
git config --global core.autocrlf true

",https://stackoverflow.com//questions/20496084/git-status-ignore-line-endings-identical-files-windows-linux-environment
linux - Fast way to get image dimensions (not filesize),"I'm looking for a fast way to get the height and width of an image in pixels. It should handle at least JPG, PNG and TIFF, but the more the better. I emphasize fast because my images are quite big (up to 250 MB) and it takes soooo long to get the size with ImageMagick's identify because it obviously reads the images as a whole first.","

The file command prints the dimensions for several image formats (e.g. PNG, GIF, JPEG; recent versions also PPM, WEBP), and does only read the header.

The identify command (from ImageMagick) prints lots of image information for a wide variety of images. It seems to restrain itself to reading the header portion (see comments). It also uses a unified format which file sadly lacks.

exiv2 gives you dimensions for many formats, including JPEG, TIFF, PNG, GIF, WEBP, even if no EXIF header present. It is unclear if it reads the whole data for that though. See the manpage of exiv2 for all supported image formats.

head -n1 will give you the dimensions for PPM, PGM formats.


For formats popular on the web, both exiv2 and identify will do the job.
Depending on the use-case you may need to write your own script that combines/parses outputs of several tools.
",https://stackoverflow.com//questions/4670013/fast-way-to-get-image-dimensions-not-filesize
linux - How to remove non UTF-8 characters from text file,"I have a bunch of Arabic, English, Russian files which are encoded in utf-8. Trying to process these files using a Perl script, I get this error:","
This command:
iconv -f utf-8 -t utf-8 -c file.txt

will clean up your UTF-8 file, skipping all the invalid characters.
-f is the source format
-t the target format
-c skips any invalid sequence

",https://stackoverflow.com//questions/12999651/how-to-remove-non-utf-8-characters-from-text-file
java - How do I run a spring boot executable jar in a Production environment?,Spring boot's preferred deployment method is via a executable jar file which contains tomcat inside.,"
Please note that since Spring Boot 1.3.0.M1, you are able to build fully executable jars using Maven and Gradle. 
For Maven, just include the following in your pom.xml:
<plugin>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-maven-plugin</artifactId>
    <configuration>
        <executable>true</executable>
    </configuration>
</plugin>

For Gradle add the following snippet to your build.gradle:
springBoot {
    executable = true
}

The fully executable jar contains an extra script at the front of the file, which allows you to just symlink your Spring Boot jar to init.d or use a systemd script.
init.d example:
$ln -s /var/yourapp/yourapp.jar /etc/init.d/yourapp

This allows you to start, stop and restart your application like:
$/etc/init.d/yourapp start|stop|restart

Or use a systemd script:
[Unit]
Description=yourapp
After=syslog.target

[Service]
ExecStart=/var/yourapp/yourapp.jar
User=yourapp
WorkingDirectory=/var/yourapp
SuccessExitStatus=143

[Install]
WantedBy=multi-user.target

More information at the following links:

Installation as an init.d service
Installation as a systemd service

",https://stackoverflow.com//questions/22886083/how-do-i-run-a-spring-boot-executable-jar-in-a-production-environment
linux - Freeing up a TCP/IP port?," This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.","
As the others have said, you'll have to kill all processes that are listening on that port.  The easiest way to do that would be to use the fuser(1) command.  For example, to see all of the processes listening for HTTP requests on port 80 (run as root or use sudo):
# fuser 80/tcp

If you want to kill them, then just add the -k option.
",https://stackoverflow.com//questions/750604/freeing-up-a-tcp-ip-port
